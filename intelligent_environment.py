# intelligent_environment.py
"""
æ™ºèƒ½MLç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿ
åŸºäºgemini-cliçš„TypeScriptæ¶æ„ç§»æ¤åˆ°Python
æä¾›å¼ºå¤§çš„ç¯å¢ƒåˆ†æå’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
"""

import asyncio
import json
import platform
import subprocess
import sys
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import importlib.util
import pkg_resources
import psutil
import pandas as pd
import hashlib
import os
from datetime import datetime

@dataclass
class DatasetInfo:
    path: str
    size_bytes: int
    type: str
    rows: Optional[int] = None
    columns: Optional[int] = None
    schema: Optional[Dict[str, str]] = None
    quality_score: Optional[float] = None
    last_modified: Optional[str] = None

@dataclass
class ModelInfo:
    path: str
    type: str
    framework: str
    size_bytes: int
    created: str
    metadata: Optional[Dict[str, Any]] = None

@dataclass
class ComputeResource:
    type: str
    name: str
    memory_total: int
    memory_available: int
    utilization: float
    temperature: Optional[float] = None

@dataclass
class MLEnvironmentContext:
    """å®Œæ•´çš„MLç¯å¢ƒä¸Šä¸‹æ–‡"""
    # åŸºç¡€ç¯å¢ƒä¿¡æ¯
    working_dir: str
    platform: str
    python_version: str
    timestamp: str
    
    # é¡¹ç›®ç»“æ„
    project_type: str
    project_patterns: List[str]
    quality_indicators: Dict[str, bool]
    
    # æ•°æ®èµ„æº
    available_datasets: List[DatasetInfo]
    data_volume_gb: float
    
    # è®¡ç®—èµ„æº
    compute_resources: List[ComputeResource]
    gpu_available: bool
    
    # è½¯ä»¶ç¯å¢ƒ
    installed_packages: Dict[str, str]
    ml_frameworks: List[str]
    
    # ç°æœ‰èµ„äº§
    existing_models: List[ModelInfo]
    notebook_files: List[str]
    script_files: List[str]
    
    # é…ç½®æ–‡ä»¶
    config_files: List[str]
    environment_files: List[str]

class IntelligentMLEnvironmentAnalyzer:
    """æ™ºèƒ½MLç¯å¢ƒåˆ†æå™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root).resolve()
        self.analysis_cache = {}
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜
        
        # MLç›¸å…³æ–‡ä»¶æ‰©å±•å
        self.dataset_extensions = {
            '.csv', '.parquet', '.json', '.jsonl', '.xlsx', '.xls',
            '.h5', '.hdf5', '.npz', '.npy', '.pkl', '.pickle',
            '.tsv', '.txt', '.data'
        }
        
        self.model_extensions = {
            '.pkl', '.pickle', '.joblib', '.h5', '.hdf5', '.pb',
            '.pth', '.pt', '.onnx', '.tflite', '.model'
        }
        
        self.notebook_extensions = {'.ipynb'}
        self.script_extensions = {'.py', '.R', '.sql'}
        
    async def analyze_environment(self) -> MLEnvironmentContext:
        """ä¸»è¦åˆ†æå…¥å£ï¼šæ™ºèƒ½åˆ†ææ•´ä¸ªMLç¯å¢ƒ"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key()
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.analysis_cache:
            cached_result, timestamp = self.analysis_cache[cache_key]
            if (datetime.now().timestamp() - timestamp) < self.cache_ttl:
                print(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„ç¯å¢ƒåˆ†æç»“æœ")
                return cached_result
        
        print(f"ğŸ” å¼€å§‹æ™ºèƒ½ç¯å¢ƒåˆ†æ: {self.project_root}")
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
        tasks = [
            self._analyze_project_structure(),
            self._discover_datasets(),
            self._analyze_compute_resources(),
            self._analyze_software_environment(),
            self._discover_existing_models(),
            self._analyze_code_files(),
            self._discover_config_files()
        ]
        
        try:
            results = await asyncio.gather(*tasks)
            
            # æ„å»ºå®Œæ•´çš„ç¯å¢ƒä¸Šä¸‹æ–‡
            context = MLEnvironmentContext(
                # åŸºç¡€ä¿¡æ¯
                working_dir=str(self.project_root),
                platform=platform.platform(),
                python_version=platform.python_version(),
                timestamp=datetime.now().isoformat(),
                
                # é¡¹ç›®ç»“æ„åˆ†æ
                project_type=results[0]['type'],
                project_patterns=results[0]['patterns'],
                quality_indicators=results[0]['quality_indicators'],
                
                # æ•°æ®èµ„æº
                available_datasets=results[1]['datasets'],
                data_volume_gb=results[1]['total_size_gb'],
                
                # è®¡ç®—èµ„æº
                compute_resources=results[2]['resources'],
                gpu_available=results[2]['gpu_available'],
                
                # è½¯ä»¶ç¯å¢ƒ
                installed_packages=results[3]['packages'],
                ml_frameworks=results[3]['ml_frameworks'],
                
                # ç°æœ‰èµ„äº§
                existing_models=results[4]['models'],
                notebook_files=results[5]['notebooks'],
                script_files=results[5]['scripts'],
                
                # é…ç½®
                config_files=results[6]['config_files'],
                environment_files=results[6]['env_files']
            )
            
            # ç¼“å­˜ç»“æœ
            self.analysis_cache[cache_key] = (context, datetime.now().timestamp())
            
            print(f"âœ… ç¯å¢ƒåˆ†æå®Œæˆï¼šå‘ç° {len(context.available_datasets)} ä¸ªæ•°æ®é›†ï¼Œ{len(context.existing_models)} ä¸ªæ¨¡å‹")
            return context
            
        except Exception as e:
            print(f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {str(e)}")
            raise
    
    async def _analyze_project_structure(self) -> Dict[str, Any]:
        """åˆ†æé¡¹ç›®ç»“æ„å’Œç±»å‹"""
        structure = {
            'type': 'unknown',
            'patterns': [],
            'quality_indicators': {},
            'framework_indicators': {}
        }
        
        # æ£€æµ‹é¡¹ç›®ç±»å‹ç‰¹å¾
        type_indicators = {
            'research': ['notebooks/', 'experiments/', 'research/', '*.ipynb'],
            'production': ['src/', 'app/', 'api/', 'main.py', 'app.py'],
            'ml_pipeline': ['pipelines/', 'dags/', 'workflows/', 'pipeline.py'],
            'data_science': ['data/', 'datasets/', 'models/', 'analysis/'],
            'experiment': ['experiments/', 'trials/', 'runs/', 'mlruns/']
        }
        
        detected_types = []
        for proj_type, indicators in type_indicators.items():
            score = 0
            for indicator in indicators:
                if '*' in indicator:
                    # Glob pattern
                    if list(self.project_root.glob(indicator)):
                        score += 2
                else:
                    # Directory or file
                    if (self.project_root / indicator).exists():
                        score += 1
            
            if score > 0:
                detected_types.append((proj_type, score))
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if detected_types:
            detected_types.sort(key=lambda x: x[1], reverse=True)
            structure['type'] = detected_types[0][0]
            structure['patterns'] = [t[0] for t in detected_types if t[1] > 0]
        
        # è´¨é‡æŒ‡æ ‡æ£€æµ‹
        structure['quality_indicators'] = {
            'has_tests': any([
                (self.project_root / d).exists() 
                for d in ['tests/', 'test/', '__tests__']
            ]),
            'has_docs': any([
                (self.project_root / d).exists() 
                for d in ['docs/', 'documentation/', 'README.md']
            ]),
            'has_config': any([
                (self.project_root / f).exists() 
                for f in ['config.yml', 'config.yaml', 'config.json', 'settings.py']
            ]),
            'has_requirements': any([
                (self.project_root / f).exists() 
                for f in ['requirements.txt', 'environment.yml', 'pyproject.toml', 'Pipfile']
            ]),
            'has_version_control': (self.project_root / '.git').exists(),
            'has_ci_cd': any([
                (self.project_root / d).exists() 
                for d in ['.github/', '.gitlab-ci.yml', 'Jenkinsfile']
            ]),
            'has_docker': any([
                (self.project_root / f).exists() 
                for f in ['Dockerfile', 'docker-compose.yml']
            ])
        }
        
        return structure
    
    async def _discover_datasets(self) -> Dict[str, Any]:
        """æ™ºèƒ½å‘ç°å’Œåˆ†ææ•°æ®é›†"""
        datasets = []
        total_size = 0
        
        # æœç´¢æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ–‡ä»¶
        for ext in self.dataset_extensions:
            pattern = f"**/*{ext}"
            for file_path in self.project_root.glob(pattern):
                if file_path.is_file():
                    try:
                        dataset_info = await self._analyze_dataset_file(file_path)
                        datasets.append(dataset_info)
                        total_size += dataset_info.size_bytes
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•åˆ†ææ•°æ®é›† {file_path}: {str(e)}")
        
        return {
            'datasets': datasets,
            'total_size_gb': total_size / (1024**3),
            'count': len(datasets)
        }
    
    async def _analyze_dataset_file(self, file_path: Path) -> DatasetInfo:
        """åˆ†æå•ä¸ªæ•°æ®é›†æ–‡ä»¶"""
        stat = file_path.stat()
        file_ext = file_path.suffix.lower()
        
        dataset_info = DatasetInfo(
            path=str(file_path.relative_to(self.project_root)),
            size_bytes=stat.st_size,
            type=self._infer_dataset_type(file_ext),
            last_modified=datetime.fromtimestamp(stat.st_mtime).isoformat()
        )
        
        # å°è¯•è¯»å–æ•°æ®é›†å…ƒä¿¡æ¯ï¼ˆä»…å¯¹è¾ƒå°çš„æ–‡ä»¶ï¼‰
        if stat.st_size < 100 * 1024 * 1024:  # å°äº100MB
            try:
                if file_ext == '.csv':
                    df = pd.read_csv(file_path, nrows=1000)  # åªè¯»å‰1000è¡Œ
                    dataset_info.rows = len(df)
                    dataset_info.columns = len(df.columns)
                    dataset_info.schema = {col: str(dtype) for col, dtype in df.dtypes.items()}
                    dataset_info.quality_score = self._calculate_data_quality(df)
                    
                elif file_ext in ['.parquet']:
                    df = pd.read_parquet(file_path)
                    dataset_info.rows = len(df)
                    dataset_info.columns = len(df.columns)
                    dataset_info.schema = {col: str(dtype) for col, dtype in df.dtypes.items()}
                    dataset_info.quality_score = self._calculate_data_quality(df)
                    
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è¯»å–æ•°æ®é›†è¯¦æƒ… {file_path}: {str(e)}")
        
        return dataset_info
    
    def _calculate_data_quality(self, df: pd.DataFrame) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•° (0-1)"""
        quality_score = 1.0
        
        # ç¼ºå¤±å€¼æƒ©ç½š
        missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
        quality_score -= missing_ratio * 0.3
        
        # é‡å¤å€¼æƒ©ç½š
        if len(df) > 1:
            duplicate_ratio = df.duplicated().sum() / len(df)
            quality_score -= duplicate_ratio * 0.2
        
        # æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
        inconsistent_types = 0
        for col in df.columns:
            if df[col].dtype == 'object':
                # æ£€æŸ¥æ•°å€¼åˆ—æ˜¯å¦è¢«é”™è¯¯è¯†åˆ«ä¸ºå¯¹è±¡ç±»å‹
                try:
                    pd.to_numeric(df[col].dropna(), errors='raise')
                    inconsistent_types += 1
                except:
                    pass
        
        if len(df.columns) > 0:
            type_inconsistency = inconsistent_types / len(df.columns)
            quality_score -= type_inconsistency * 0.1
        
        return max(0.0, min(1.0, quality_score))
    
    def _infer_dataset_type(self, file_ext: str) -> str:
        """æ¨æ–­æ•°æ®é›†ç±»å‹"""
        type_mapping = {
            '.csv': 'structured_text',
            '.parquet': 'structured_binary',
            '.json': 'semi_structured',
            '.jsonl': 'semi_structured',
            '.xlsx': 'structured_spreadsheet',
            '.h5': 'scientific_binary',
            '.npz': 'numpy_array',
            '.pkl': 'python_object',
            '.txt': 'unstructured_text'
        }
        return type_mapping.get(file_ext, 'unknown')
    
    async def _analyze_compute_resources(self) -> Dict[str, Any]:
        """åˆ†æè®¡ç®—èµ„æº"""
        resources = []
        
        # CPUä¿¡æ¯
        cpu_info = ComputeResource(
            type='CPU',
            name=platform.processor() or 'Unknown CPU',
            memory_total=psutil.virtual_memory().total,
            memory_available=psutil.virtual_memory().available,
            utilization=psutil.cpu_percent(interval=1)
        )
        resources.append(cpu_info)
        
        # GPUæ£€æµ‹
        gpu_available = False
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            for gpu in gpus:
                gpu_info = ComputeResource(
                    type='GPU',
                    name=gpu.name,
                    memory_total=int(gpu.memoryTotal * 1024 * 1024),  # MB to bytes
                    memory_available=int(gpu.memoryFree * 1024 * 1024),
                    utilization=gpu.load * 100,
                    temperature=gpu.temperature
                )
                resources.append(gpu_info)
                gpu_available = True
        except ImportError:
            # å°è¯•nvidia-smi
            try:
                result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free,utilization.gpu', 
                                       '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    for line in result.stdout.strip().split('\n'):
                        if line:
                            parts = line.split(', ')
                            if len(parts) >= 4:
                                gpu_info = ComputeResource(
                                    type='GPU',
                                    name=parts[0],
                                    memory_total=int(parts[1]) * 1024 * 1024,  # MB to bytes
                                    memory_available=int(parts[2]) * 1024 * 1024,
                                    utilization=float(parts[3])
                                )
                                resources.append(gpu_info)
                                gpu_available = True
            except Exception:
                pass
        
        return {
            'resources': resources,
            'gpu_available': gpu_available,
            'total_memory_gb': psutil.virtual_memory().total / (1024**3)
        }
    
    async def _analyze_software_environment(self) -> Dict[str, Any]:
        """åˆ†æè½¯ä»¶ç¯å¢ƒ"""
        packages = {}
        ml_frameworks = []
        
        # è·å–å·²å®‰è£…çš„åŒ…
        try:
            installed_packages = [d for d in pkg_resources.working_set]
            for pkg in installed_packages:
                packages[pkg.project_name] = pkg.version
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è·å–åŒ…ä¿¡æ¯: {str(e)}")
        
        # æ£€æµ‹MLæ¡†æ¶
        ml_framework_indicators = {
            'tensorflow': ['tensorflow', 'tensorflow-gpu'],
            'pytorch': ['torch', 'pytorch'],
            'scikit-learn': ['scikit-learn', 'sklearn'],
            'xgboost': ['xgboost'],
            'lightgbm': ['lightgbm'],
            'catboost': ['catboost'],
            'keras': ['keras'],
            'transformers': ['transformers'],
            'opencv': ['opencv-python', 'cv2'],
            'pandas': ['pandas'],
            'numpy': ['numpy'],
            'matplotlib': ['matplotlib'],
            'seaborn': ['seaborn'],
            'plotly': ['plotly'],
            'jupyter': ['jupyter', 'jupyterlab']
        }
        
        for framework, package_names in ml_framework_indicators.items():
            for pkg_name in package_names:
                if pkg_name in packages:
                    ml_frameworks.append(framework)
                    break
        
        return {
            'packages': packages,
            'ml_frameworks': list(set(ml_frameworks)),
            'package_count': len(packages)
        }
    
    async def _discover_existing_models(self) -> Dict[str, Any]:
        """å‘ç°ç°æœ‰çš„æ¨¡å‹æ–‡ä»¶"""
        models = []
        
        for ext in self.model_extensions:
            pattern = f"**/*{ext}"
            for file_path in self.project_root.glob(pattern):
                if file_path.is_file():
                    try:
                        model_info = await self._analyze_model_file(file_path)
                        models.append(model_info)
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•åˆ†ææ¨¡å‹æ–‡ä»¶ {file_path}: {str(e)}")
        
        return {
            'models': models,
            'count': len(models)
        }
    
    async def _analyze_model_file(self, file_path: Path) -> ModelInfo:
        """åˆ†ææ¨¡å‹æ–‡ä»¶"""
        stat = file_path.stat()
        file_ext = file_path.suffix.lower()
        
        # æ¨æ–­æ¡†æ¶
        framework = 'unknown'
        if file_ext in ['.pkl', '.pickle', '.joblib']:
            framework = 'scikit-learn/pickle'
        elif file_ext in ['.h5', '.hdf5']:
            framework = 'tensorflow/keras'
        elif file_ext in ['.pth', '.pt']:
            framework = 'pytorch'
        elif file_ext == '.pb':
            framework = 'tensorflow'
        elif file_ext == '.onnx':
            framework = 'onnx'
        
        return ModelInfo(
            path=str(file_path.relative_to(self.project_root)),
            type=self._infer_model_type(file_ext),
            framework=framework,
            size_bytes=stat.st_size,
            created=datetime.fromtimestamp(stat.st_ctime).isoformat()
        )
    
    def _infer_model_type(self, file_ext: str) -> str:
        """æ¨æ–­æ¨¡å‹ç±»å‹"""
        type_mapping = {
            '.pkl': 'machine_learning_model',
            '.h5': 'deep_learning_model',
            '.pth': 'deep_learning_model',
            '.pb': 'tensorflow_graph',
            '.onnx': 'onnx_model',
            '.tflite': 'mobile_optimized_model'
        }
        return type_mapping.get(file_ext, 'unknown_model')
    
    async def _analyze_code_files(self) -> Dict[str, Any]:
        """åˆ†æä»£ç æ–‡ä»¶"""
        notebooks = []
        scripts = []
        
        # æŸ¥æ‰¾Jupyter notebooks
        for notebook_file in self.project_root.glob("**/*.ipynb"):
            notebooks.append(str(notebook_file.relative_to(self.project_root)))
        
        # æŸ¥æ‰¾Pythonè„šæœ¬
        for script_file in self.project_root.glob("**/*.py"):
            scripts.append(str(script_file.relative_to(self.project_root)))
        
        return {
            'notebooks': notebooks,
            'scripts': scripts,
            'notebook_count': len(notebooks),
            'script_count': len(scripts)
        }
    
    async def _discover_config_files(self) -> Dict[str, Any]:
        """å‘ç°é…ç½®æ–‡ä»¶"""
        config_patterns = [
            'config.yml', 'config.yaml', 'config.json',
            'settings.py', 'settings.yml', 'settings.yaml',
            '*.conf', '*.ini', '*.toml'
        ]
        
        env_patterns = [
            '.env', '.env.*', 'environment.yml', 
            'requirements.txt', 'pyproject.toml', 'Pipfile'
        ]
        
        config_files = []
        env_files = []
        
        for pattern in config_patterns:
            for file_path in self.project_root.glob(pattern):
                if file_path.is_file():
                    config_files.append(str(file_path.relative_to(self.project_root)))
        
        for pattern in env_patterns:
            for file_path in self.project_root.glob(pattern):
                if file_path.is_file():
                    env_files.append(str(file_path.relative_to(self.project_root)))
        
        return {
            'config_files': config_files,
            'env_files': env_files
        }
    
    def _generate_cache_key(self) -> str:
        """ç”Ÿæˆåˆ†æç¼“å­˜é”®"""
        # åŸºäºé¡¹ç›®è·¯å¾„å’Œæœ€è¿‘ä¿®æ”¹æ—¶é—´ç”Ÿæˆé”®
        key_data = f"{self.project_root}_{os.path.getmtime(self.project_root)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def export_analysis(self, context: MLEnvironmentContext, output_path: str = None) -> str:
        """å¯¼å‡ºç¯å¢ƒåˆ†æç»“æœ"""
        if output_path is None:
            output_path = f"ml_environment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        data = asdict(context)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ç¯å¢ƒåˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
        return output_path
    
    def generate_summary_report(self, context: MLEnvironmentContext) -> str:
        """ç”Ÿæˆç¯å¢ƒåˆ†ææ‘˜è¦æŠ¥å‘Š"""
        report = f"""
ğŸ¤– MLç¯å¢ƒæ™ºèƒ½åˆ†ææŠ¥å‘Š
{'='*50}

ğŸ“‚ é¡¹ç›®æ¦‚è§ˆ:
  ç±»å‹: {context.project_type}
  è·¯å¾„: {context.working_dir}
  å¹³å°: {context.platform}
  Pythonç‰ˆæœ¬: {context.python_version}

ğŸ“Š æ•°æ®èµ„æº:
  æ•°æ®é›†æ•°é‡: {len(context.available_datasets)}
  æ€»æ•°æ®é‡: {context.data_volume_gb:.2f} GB
  æ•°æ®ç±»å‹: {len(set(d.type for d in context.available_datasets))} ç§

ğŸ§  æ¨¡å‹èµ„äº§:
  ç°æœ‰æ¨¡å‹: {len(context.existing_models)} ä¸ª
  æ¡†æ¶åˆ†å¸ƒ: {len(set(m.framework for m in context.existing_models))} ç§

ğŸ’» è®¡ç®—èµ„æº:
  CPUæ ¸å¿ƒ: {psutil.cpu_count()}
  å†…å­˜æ€»é‡: {psutil.virtual_memory().total / (1024**3):.1f} GB
  GPUå¯ç”¨: {'æ˜¯' if context.gpu_available else 'å¦'}

ğŸ”§ è½¯ä»¶ç¯å¢ƒ:
  MLæ¡†æ¶: {len(context.ml_frameworks)} ç§ ({', '.join(context.ml_frameworks[:5])})
  PythonåŒ…: {len(context.installed_packages)} ä¸ª

ğŸ“ ä»£ç èµ„äº§:
  Jupyter Notebooks: {len(context.notebook_files)} ä¸ª
  Pythonè„šæœ¬: {len(context.script_files)} ä¸ª

âœ… é¡¹ç›®è´¨é‡:
  æµ‹è¯•: {'æœ‰' if context.quality_indicators.get('has_tests') else 'æ— '}
  æ–‡æ¡£: {'æœ‰' if context.quality_indicators.get('has_docs') else 'æ— '}
  ç‰ˆæœ¬æ§åˆ¶: {'æœ‰' if context.quality_indicators.get('has_version_control') else 'æ— '}
  CI/CD: {'æœ‰' if context.quality_indicators.get('has_ci_cd') else 'æ— '}

ğŸ¯ æ™ºèƒ½å»ºè®®:
{self._generate_intelligent_recommendations(context)}
"""
        return report
    
    def _generate_intelligent_recommendations(self, context: MLEnvironmentContext) -> str:
        """ç”Ÿæˆæ™ºèƒ½å»ºè®®"""
        recommendations = []
        
        # æ•°æ®ç›¸å…³å»ºè®®
        if len(context.available_datasets) == 0:
            recommendations.append("  â€¢ å»ºè®®æ·»åŠ æ•°æ®é›†åˆ°é¡¹ç›®ä¸­ä»¥å¼€å§‹MLå·¥ä½œæµ")
        elif context.data_volume_gb > 10:
            recommendations.append("  â€¢ æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®è€ƒè™‘ä½¿ç”¨åˆ†å¸ƒå¼å¤„ç†æ¡†æ¶")
        
        # è®¡ç®—èµ„æºå»ºè®®
        if not context.gpu_available and 'tensorflow' in context.ml_frameworks:
            recommendations.append("  â€¢ æ£€æµ‹åˆ°æ·±åº¦å­¦ä¹ æ¡†æ¶ä½†æ— GPUï¼Œå»ºè®®é…ç½®GPUåŠ é€Ÿ")
        
        # é¡¹ç›®ç»“æ„å»ºè®®
        if not context.quality_indicators.get('has_tests'):
            recommendations.append("  â€¢ å»ºè®®æ·»åŠ å•å…ƒæµ‹è¯•ä»¥æé«˜ä»£ç è´¨é‡")
        
        if not context.quality_indicators.get('has_docs'):
            recommendations.append("  â€¢ å»ºè®®æ·»åŠ é¡¹ç›®æ–‡æ¡£ä»¥ä¾¿å›¢é˜Ÿåä½œ")
        
        # ç¯å¢ƒç®¡ç†å»ºè®®
        if not any('requirements' in f for f in context.environment_files):
            recommendations.append("  â€¢ å»ºè®®åˆ›å»ºrequirements.txtç®¡ç†ä¾èµ–")
        
        if len(recommendations) == 0:
            recommendations.append("  â€¢ é¡¹ç›®ç»“æ„è‰¯å¥½ï¼Œç¯å¢ƒé…ç½®å®Œå–„ï¼")
        
        return '\n'.join(recommendations)

# å¼‚æ­¥æµ‹è¯•å‡½æ•°
async def test_environment_analyzer():
    """æµ‹è¯•ç¯å¢ƒåˆ†æå™¨"""
    analyzer = IntelligentMLEnvironmentAnalyzer(".")
    
    print("ğŸš€ å¼€å§‹æ™ºèƒ½ç¯å¢ƒåˆ†ææµ‹è¯•...")
    context = await analyzer.analyze_environment()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_summary_report(context)
    print(report)
    
    # å¯¼å‡ºè¯¦ç»†åˆ†æ
    export_path = analyzer.export_analysis(context)
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†æå·²å¯¼å‡º: {export_path}")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_environment_analyzer())