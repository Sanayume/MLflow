# app.py

import streamlit as st
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import Tool
from langchain_core.tools import StructuredTool
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver
import os
import datetime
import json
import logging
import asyncio

# å¯¼å…¥å¢å¼ºé…ç½®ç®¡ç†å™¨
from enhanced_config import config_manager, render_provider_config_ui, create_llm_from_config

# å¯¼å…¥å¢å¼ºæ¨¡å—
from enhanced_agent import EnhancedAIAgent, MemoryType, TaskType
from enhanced_history import (
    EnhancedHistoryManager, HistoryHelpers, HistoryType, 
    EventSeverity, SearchFilters
)
from enhanced_ml_tools import EnhancedMLTools, MLTaskType, VisualizationType
from resource_monitor import SystemResourceMonitor, ResourceAlert, ResourceType

# å¯¼å…¥æ–°çš„æ™ºèƒ½ç³»ç»Ÿ
from intelligent_environment import IntelligentMLEnvironmentAnalyzer
from intelligent_scheduler import IntelligentMLToolScheduler
from intelligent_memory import IntelligentMLMemoryManager
from intelligent_prompts import (
    IntelligentMLPromptGenerator, 
    PromptGenerationContext,
    IntentType,
    PromptComplexity
)

# å¯¼å…¥å¢å¼ºé…ç½®ç®¡ç†å™¨
from enhanced_config import config_manager, render_provider_config_ui, create_llm_from_config

# å¯¼å…¥å¢å¼ºæ¨¡å—
from enhanced_agent import EnhancedAIAgent, MemoryType, TaskType
from enhanced_history import (
    EnhancedHistoryManager, HistoryHelpers, HistoryType, 
    EventSeverity, SearchFilters
)
from enhanced_ml_tools import EnhancedMLTools, MLTaskType, VisualizationType
from resource_monitor import SystemResourceMonitor, ResourceAlert, ResourceType
from local_tools import (
    safe_python_executor, 
    read_local_file, 
    write_local_file, 
    # list_directory_contents, # å¯ä»¥æ›¿æ¢æ‰æ—§çš„
    list_directory_items_with_paths, # æ–°çš„å·¥å…·
    make_web_request 
)
from sandbox_main import (
    execute_ml_code_in_docker,
    CONTAINER_CODE_EXECUTION_BASE_TARGET,
    CONTAINER_OUTPUTS_MOUNT_TARGET,
    CONTAINER_DATASETS_MOUNT_TARGET,
    HOST_EXECUTION_LOGS_ROOT,
    HOST_OUTPUTS_MOUNT_SOURCE,
    HOST_DATASETS_MOUNT_SOURCE,
    HOST_ROOT_WORKSPACE,
    DOCKER_IMAGE_NAME,
    SandboxExecutionInput
)
from langchain_community.tools.file_management import (
    CopyFileTool,
    DeleteFileTool,
    FileSearchTool,
    ListDirectoryTool,
    MoveFileTool,
    ReadFileTool,
    WriteFileTool,
)
from db_tools import (
    SaveMLResultInput,
    DatabaseQueryInput,
    QueryMLResultsInput,
    save_ml_result_to_db,
    query_execution_history,
    query_ml_results_from_db,
)
from config import (
        SYSTEM_PROMPT, 
        EXECUTE_CODE_TOOL_DESCRIPTION,
        SYSTEM_PROMPT_2,
        QUERY_EXEC_LOGS_TOOL_DESCRIPTION,
        QUERY_ML_RESULTS_TOOL_DESCRIPTION,
        SAVE_ML_RESULT_TOOL_DESCRIPTION
)

# è·å–åº”ç”¨è®¾ç½®
app_settings = config_manager.get_app_settings()

# é…ç½®é¡µé¢
st.set_page_config(
    page_title=app_settings.get("title", "AutoML Workflow Agent"),
    page_icon=app_settings.get("page_icon", "ğŸ¤–"),
    layout=app_settings.get("layout", "wide"),
    initial_sidebar_state=app_settings.get("sidebar_state", "expanded")
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è·å–å¯ç”¨çš„ä¾›åº”å•†
available_providers = config_manager.get_enabled_providers()
if not available_providers:
    st.error("âŒ æ²¡æœ‰é…ç½®å¯ç”¨çš„APIä¾›åº”å•†ï¼è¯·å…ˆé…ç½®è‡³å°‘ä¸€ä¸ªä¾›åº”å•†ã€‚")
    st.stop()

# é€‰æ‹©å½“å‰ä½¿ç”¨çš„ä¾›åº”å•†
if "current_provider_id" not in st.session_state:
    default_provider = config_manager.get_default_provider()
    st.session_state.current_provider_id = list(available_providers.keys())[0] if available_providers else None

current_provider = config_manager.get_provider(st.session_state.current_provider_id)
if not current_provider or not current_provider.enabled or not current_provider.api_key:
    st.error(f"âŒ å½“å‰é€‰æ‹©çš„ä¾›åº”å•† {st.session_state.current_provider_id} æœªæ­£ç¡®é…ç½®ï¼")
    st.stop()

# 1. åˆå§‹åŒ–LLM
llm = None
try:
    llm = create_llm_from_config(current_provider)
    st.success(f"âœ… å·²è¿æ¥åˆ° {current_provider.name} - {current_provider.default_model}")
except Exception as e:
    st.error(f"âŒ åˆå§‹åŒ–LLMå¤±è´¥: {str(e)}")
    st.info("ğŸ’¡ è¯·åœ¨ä¾§è¾¹æ é…ç½®APIä¾›åº”å•†ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½")
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„LLMå¯¹è±¡ä»¥é¿å…é”™è¯¯
    class MockLLM:
        def invoke(self, *args, **kwargs):
            return "è¯·é…ç½®æœ‰æ•ˆçš„APIä¾›åº”å•†ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½"
        
        def bind(self, **kwargs):
            return self
        
        def with_structured_output(self, *args, **kwargs):
            return self
    llm = MockLLM()

# 2. åˆå§‹åŒ–å¢å¼ºç»„ä»¶å’Œæ™ºèƒ½ç³»ç»Ÿ
@st.cache_resource
def initialize_enhanced_components():
    """åˆå§‹åŒ–å¢å¼ºç»„ä»¶å’Œæ™ºèƒ½ç³»ç»Ÿï¼ˆä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åˆå§‹åŒ–ï¼‰"""
    history_manager = EnhancedHistoryManager()
    ml_tools = EnhancedMLTools()
    resource_monitor = SystemResourceMonitor()
    
    # åˆå§‹åŒ–æ™ºèƒ½ç³»ç»Ÿï¼ˆä¸ä½¿ç”¨å¼‚æ­¥ä»»åŠ¡ï¼‰
    environment_analyzer = IntelligentMLEnvironmentAnalyzer(".")
    tool_scheduler = IntelligentMLToolScheduler()
    memory_manager = IntelligentMLMemoryManager(".")
    prompt_generator = IntelligentMLPromptGenerator(".")
    
    return (
        history_manager, ml_tools, resource_monitor,
        environment_analyzer, tool_scheduler, memory_manager, prompt_generator
    )

(history_manager, ml_tools, resource_monitor, 
 environment_analyzer, tool_scheduler, memory_manager, prompt_generator) = initialize_enhanced_components()

# ä¸ºæ™ºèƒ½ç³»ç»Ÿåˆ›å»ºå¼‚æ­¥æ‰§è¡Œç¯å¢ƒ
@st.cache_resource
def create_async_runner():
    """åˆ›å»ºå¼‚æ­¥è¿è¡Œå™¨"""
    import asyncio
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop

async_loop = create_async_runner()

execute_in_ml_sandbox_tool = StructuredTool.from_function(
    func=execute_ml_code_in_docker, # æŒ‡å‘æˆ‘ä»¬å¥å£®çš„åç«¯å‡½æ•°
    name="ExecutePythonInMLSandbox", # å·¥å…·çš„è°ƒç”¨åç§°
    description=EXECUTE_CODE_TOOL_DESCRIPTION, # ä¸Šé¢å®šä¹‰çš„è¯¦ç»†æè¿°
    args_schema=SandboxExecutionInput # Pydanticæ¨¡å‹å®šä¹‰è¾“å…¥å‚æ•°
)
query_system_execution_logs_tool = StructuredTool.from_function(
    func=query_execution_history, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="QuerySystemExecutionLogs",
    description=QUERY_EXEC_LOGS_TOOL_DESCRIPTION,
    args_schema=DatabaseQueryInput # ä½¿ç”¨ DatabaseQueryInput
)
save_ml_result_tool = StructuredTool.from_function(
    func=save_ml_result_to_db, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="SaveMachineLearningResult",
    description=SAVE_ML_RESULT_TOOL_DESCRIPTION,
    args_schema=SaveMLResultInput # ä½¿ç”¨ SaveMLResultInput
)
query_ml_results_tool = StructuredTool.from_function(
    func=query_ml_results_from_db, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="QueryMachineLearningResults",
    description=QUERY_ML_RESULTS_TOOL_DESCRIPTION,
    args_schema=QueryMLResultsInput # ä½¿ç”¨ QueryMLResultsInput
)

tools = [
    execute_in_ml_sandbox_tool,
    query_system_execution_logs_tool,
    save_ml_result_tool,
    query_ml_results_tool
]

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_PROMPT_2), # ä½¿ç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„è¯¦ç»†ç³»ç»Ÿæç¤º
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

# --- Agentå’ŒAgentExecutorçš„åˆ›å»º (ä¸æ‚¨ä¹‹å‰çš„ä»£ç ç±»ä¼¼) ---
agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,  # å¤„ç†è§£æé”™è¯¯
)

# ------------- Streamlit UIéƒ¨åˆ† -------------
# ==============================================================================

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "session_id" not in st.session_state:
    st.session_state.session_id = f"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
if "enhanced_agent" not in st.session_state:
    st.session_state.enhanced_agent = EnhancedAIAgent(llm, tools, current_provider.api_key)
if "resource_monitoring" not in st.session_state:
    st.session_state.resource_monitoring = False

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("ğŸ›ï¸ æ§åˆ¶é¢æ¿")
    
    # æ·»åŠ ä¾›åº”å•†é…ç½®å’Œæ™ºèƒ½ç³»ç»Ÿé€‰é¡¹å¡
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”§ ä¾›åº”å•†é…ç½®", "ğŸ“Š ä¼šè¯æ§åˆ¶", "ğŸ§  æ™ºèƒ½ç³»ç»Ÿ", "ğŸ“ˆ ç³»ç»Ÿç›‘æ§"])
    
    with tab1:
        # å½“å‰ä¾›åº”å•†æ˜¾ç¤º
        st.subheader("ğŸ¤– å½“å‰ä¾›åº”å•†")
        provider_options = {k: f"{v.name}" for k, v in available_providers.items()}
        
        selected_provider_id = st.selectbox(
            "é€‰æ‹©APIä¾›åº”å•†",
            options=list(provider_options.keys()),
            format_func=lambda x: provider_options[x],
            index=list(provider_options.keys()).index(st.session_state.current_provider_id),
            key="provider_selector"
        )
        
        if selected_provider_id != st.session_state.current_provider_id:
            st.session_state.current_provider_id = selected_provider_id
            st.rerun()
        
        current_provider_display = config_manager.get_provider(st.session_state.current_provider_id)
        st.info(f"**æ¨¡å‹**: {current_provider_display.default_model}\n**ç±»å‹**: {current_provider_display.type.value}")
        
        # ä¾›åº”å•†é…ç½®ç•Œé¢
        render_provider_config_ui(config_manager)
    
    with tab2:
        # ä¼šè¯ä¿¡æ¯
        st.subheader("ğŸ“Š ä¼šè¯ä¿¡æ¯")
        st.info(f"**ä¼šè¯ID:** {st.session_state.session_id}")
        
        # AgentçŠ¶æ€
        agent_status = st.session_state.enhanced_agent.get_agent_status()
        st.json(agent_status)
    
    with tab3:
        # æ™ºèƒ½ç³»ç»Ÿæ§åˆ¶é¢æ¿
        st.subheader("ğŸ§  æ™ºèƒ½ç³»ç»ŸçŠ¶æ€")
        
        # ç¯å¢ƒåˆ†æå™¨
        with st.expander("ğŸŒ æ™ºèƒ½ç¯å¢ƒåˆ†æå™¨", expanded=False):
            if st.button("ğŸ” åˆ†æå½“å‰ç¯å¢ƒ", key="analyze_env"):
                with st.spinner("åˆ†æç¯å¢ƒä¸­..."):
                    try:
                        # ä½¿ç”¨å¼‚æ­¥è¿è¡Œç¯å¢ƒåˆ†æ
                        def run_env_analysis():
                            import asyncio
                            try:
                                # è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯
                                try:
                                    loop = asyncio.get_event_loop()
                                except RuntimeError:
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                
                                # è¿è¡Œç¯å¢ƒåˆ†æ
                                context = loop.run_until_complete(
                                    environment_analyzer.analyze_environment()
                                )
                                return context
                            except Exception as e:
                                st.error(f"ç¯å¢ƒåˆ†æå¤±è´¥: {str(e)}")
                                return None
                        
                        env_context = run_env_analysis()
                        
                        if env_context:
                            st.success(f"âœ… ç¯å¢ƒåˆ†æå®Œæˆ")
                            
                            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("æ•°æ®é›†", len(env_context.available_datasets))
                            with col2:
                                st.metric("æ¨¡å‹", len(env_context.existing_models))
                            with col3:
                                st.metric("MLæ¡†æ¶", len(env_context.ml_frameworks))
                            with col4:
                                st.metric("æ•°æ®é‡", f"{env_context.data_volume_gb:.1f}GB")
                            
                            # é¡¹ç›®ç±»å‹å’Œè´¨é‡æŒ‡æ ‡
                            st.write(f"**é¡¹ç›®ç±»å‹**: {env_context.project_type}")
                            st.write(f"**Pythonç‰ˆæœ¬**: {env_context.python_version}")
                            st.write(f"**GPUå¯ç”¨**: {'æ˜¯' if env_context.gpu_available else 'å¦'}")
                            
                            # è´¨é‡æŒ‡æ ‡
                            quality = env_context.quality_indicators
                            quality_items = []
                            for key, value in quality.items():
                                status = "âœ…" if value else "âŒ"
                                quality_items.append(f"{status} {key}")
                            st.write("**è´¨é‡æŒ‡æ ‡**:")
                            st.write(" | ".join(quality_items))
                            
                            # å¯¼å‡ºç¯å¢ƒåˆ†æ
                            if st.button("ğŸ“„ å¯¼å‡ºç¯å¢ƒåˆ†æ", key="export_env"):
                                export_path = environment_analyzer.export_analysis(env_context)
                                st.success(f"ç¯å¢ƒåˆ†æå·²å¯¼å‡º: {export_path}")
                    
                    except Exception as e:
                        st.error(f"ç¯å¢ƒåˆ†æå‡ºé”™: {str(e)}")
        
        # æ™ºèƒ½è°ƒåº¦å™¨
        with st.expander("âš™ï¸ æ™ºèƒ½å·¥å…·è°ƒåº¦å™¨", expanded=False):
            st.write("**è°ƒåº¦å™¨çŠ¶æ€**: å°±ç»ª")
            
            # æ˜¾ç¤ºè°ƒåº¦ç»Ÿè®¡
            status = tool_scheduler.get_execution_status()
            if status.get('total_tools', 0) > 0:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»å·¥å…·", status['total_tools'])
                with col2:
                    completed = status['status_breakdown'].get('completed', 0)
                    st.metric("å·²å®Œæˆ", completed)
                with col3:
                    cpu_usage = status['resource_utilization'].get('cpu', '0/0')
                    st.metric("CPUä½¿ç”¨", cpu_usage)
                
                if st.button("ğŸ“Š å¯¼å‡ºè°ƒåº¦æŠ¥å‘Š", key="export_scheduler"):
                    report_path = tool_scheduler.export_execution_report()
                    st.success(f"è°ƒåº¦æŠ¥å‘Šå·²å¯¼å‡º: {report_path}")
            else:
                st.info("æš‚æ— è°ƒåº¦ä»»åŠ¡")
        
        # æ™ºèƒ½è®°å¿†ç®¡ç†å™¨
        with st.expander("ğŸ’¾ æ™ºèƒ½è®°å¿†ç®¡ç†å™¨", expanded=False):
            # è·å–è®°å¿†ç»Ÿè®¡
            memory_stats = memory_manager.get_memory_statistics()
            
            if memory_stats.get('total_memories', 0) > 0:
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æ€»è®°å¿†", memory_stats['total_memories'])
                with col2:
                    st.metric("æ´å¯Ÿ", memory_stats['total_insights'])
                with col3:
                    st.metric("çŸ¥è¯†å›¾è°±èŠ‚ç‚¹", memory_stats['knowledge_graph_nodes'])
                with col4:
                    avg_access = memory_stats.get('average_access_count', 0)
                    st.metric("å¹³å‡è®¿é—®", f"{avg_access:.1f}")
                
                # è®°å¿†ç±»å‹åˆ†å¸ƒ
                st.write("**è®°å¿†ç±»å‹åˆ†å¸ƒ**:")
                memory_types = memory_stats.get('memory_types', {})
                for mem_type, count in memory_types.items():
                    st.write(f"- {mem_type}: {count}")
                
                # è®°å¿†æœç´¢
                search_query = st.text_input("ğŸ” æœç´¢è®°å¿†", key="memory_search")
                if search_query and st.button("æœç´¢", key="search_memory"):
                    with st.spinner("æœç´¢è®°å¿†ä¸­..."):
                        try:
                            def run_memory_search():
                                import asyncio
                                try:
                                    loop = asyncio.get_event_loop()
                                except RuntimeError:
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                
                                return loop.run_until_complete(
                                    memory_manager.smart_search(search_query, limit=5)
                                )
                            
                            search_results = run_memory_search()
                            
                            if search_results:
                                st.write(f"æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³è®°å¿†:")
                                for result in search_results:
                                    st.write(f"**ç›¸ä¼¼åº¦**: {result.similarity:.3f}")
                                    st.write(f"**å†…å®¹**: {result.node.content[:100]}...")
                                    st.write(f"**ç±»å‹**: {result.node.memory_type}")
                                    st.write("---")
                            else:
                                st.info("æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
                        except Exception as e:
                            st.error(f"è®°å¿†æœç´¢å¤±è´¥: {str(e)}")
                
                if st.button("ğŸ“„ å¯¼å‡ºè®°å¿†æ•°æ®", key="export_memory"):
                    export_path = memory_manager.export_memories()
                    st.success(f"è®°å¿†æ•°æ®å·²å¯¼å‡º: {export_path}")
            else:
                st.info("æš‚æ— è®°å¿†æ•°æ®")
        
        # æ™ºèƒ½æç¤ºç”Ÿæˆå™¨
        with st.expander("ğŸ¯ æ™ºèƒ½æç¤ºç”Ÿæˆå™¨", expanded=False):
            # è·å–æç¤ºç”Ÿæˆç»Ÿè®¡
            prompt_stats = prompt_generator.get_statistics()
            
            if prompt_stats.get('total_prompts', 0) > 0:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»æç¤º", prompt_stats['total_prompts'])
                with col2:
                    st.metric("æ¨¡æ¿æ•°", prompt_stats['total_templates'])
                with col3:
                    avg_confidence = prompt_stats.get('average_confidence', 0)
                    st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{avg_confidence:.2f}")
                
                # æ„å›¾åˆ†å¸ƒ
                intent_dist = prompt_stats.get('intent_distribution', {})
                if intent_dist:
                    st.write("**æ„å›¾åˆ†å¸ƒ**:")
                    for intent, count in intent_dist.items():
                        st.write(f"- {intent}: {count}")
                
                # æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
                template_usage = prompt_stats.get('template_usage', {})
                if template_usage:
                    st.write("**æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡**:")
                    for template, count in list(template_usage.items())[:5]:
                        st.write(f"- {template}: {count}æ¬¡")
                
                if st.button("ğŸ“„ å¯¼å‡ºæç¤ºæ•°æ®", key="export_prompts"):
                    export_path = prompt_generator.export_data()
                    st.success(f"æç¤ºæ•°æ®å·²å¯¼å‡º: {export_path}")
            else:
                st.info("æš‚æ— æç¤ºç”Ÿæˆè®°å½•")
    
    with tab4:
        # ç³»ç»Ÿç›‘æ§é€‰é¡¹å¡
        st.subheader("ğŸ“ˆ ç³»ç»Ÿèµ„æºç›‘æ§")
        
        # èµ„æºç›‘æ§æ§åˆ¶
        monitoring_col1, monitoring_col2 = st.columns(2)
        
        with monitoring_col1:
            if st.button("ğŸ” æŸ¥çœ‹å½“å‰èµ„æºçŠ¶æ€", key="check_resources"):
                current_usage = resource_monitor.get_current_usage()
                if current_usage:
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("CPUä½¿ç”¨ç‡", f"{current_usage.cpu_percent:.1f}%")
                    with col2:
                        st.metric("å†…å­˜ä½¿ç”¨ç‡", f"{current_usage.memory_percent:.1f}%")
                    with col3:
                        st.metric("ç£ç›˜ä½¿ç”¨ç‡", f"{current_usage.disk_percent:.1f}%")
        
        with monitoring_col2:
            if st.button("â–¶ï¸ å¯åŠ¨èµ„æºç›‘æ§" if not st.session_state.resource_monitoring else "â¹ï¸ åœæ­¢èµ„æºç›‘æ§", key="toggle_monitoring"):
                if not st.session_state.resource_monitoring:
                    resource_monitor.start_monitoring()
                    st.session_state.resource_monitoring = True
                    st.success("èµ„æºç›‘æ§å·²å¯åŠ¨")
                else:
                    resource_monitor.stop_monitoring()
                    st.session_state.resource_monitoring = False
                    st.success("èµ„æºç›‘æ§å·²åœæ­¢")
        
        # å†å²è®°å½•æœç´¢
        st.subheader("ğŸ” å†å²è®°å½•æœç´¢")
        search_text = st.text_input("æœç´¢å…³é”®è¯")
        history_type_filter = st.selectbox(
            "è®°å½•ç±»å‹",
            ["å…¨éƒ¨"] + [ht.value for ht in HistoryType],
            index=0
        )
    
        # æ—¶é—´èŒƒå›´
        time_range = st.selectbox(
            "æ—¶é—´èŒƒå›´",
            ["æœ€è¿‘1å°æ—¶", "æœ€è¿‘24å°æ—¶", "æœ€è¿‘7å¤©", "æœ€è¿‘30å¤©", "å…¨éƒ¨"],
            index=1
        )
        
        if st.button("ğŸ” æœç´¢å†å²"):
            # æ„å»ºæœç´¢è¿‡æ»¤å™¨
            filters = SearchFilters()
            if search_text:
                filters.search_text = search_text
            if history_type_filter != "å…¨éƒ¨":
                filters.history_types = [HistoryType(history_type_filter)]
            
            # è®¾ç½®æ—¶é—´èŒƒå›´
            if time_range != "å…¨éƒ¨":
                hours_map = {
                    "æœ€è¿‘1å°æ—¶": 1,
                    "æœ€è¿‘24å°æ—¶": 24,
                    "æœ€è¿‘7å¤©": 24 * 7,
                    "æœ€è¿‘30å¤©": 24 * 30
                }
                hours = hours_map[time_range]
                filters.end_time = datetime.datetime.now()
                filters.start_time = filters.end_time - datetime.timedelta(hours=hours)
            
            # æ‰§è¡Œæœç´¢
            entries, total_count = history_manager.search_entries(filters, limit=20)
            
            st.write(f"æ‰¾åˆ° {total_count} æ¡è®°å½•")
            for entry in entries:
                with st.expander(f"{entry.title} - {entry.timestamp.strftime('%Y-%m-%d %H:%M')}"):
                    st.write(f"**ç±»å‹:** {entry.history_type.value}")
                    st.write(f"**ä¸¥é‡ç¨‹åº¦:** {entry.severity.value}")
                    st.write(f"**æè¿°:** {entry.description}")
                    if entry.tags:
                        st.write(f"**æ ‡ç­¾:** {', '.join(entry.tags)}")
        
        # æ˜¾ç¤ºæœ€è¿‘å‘Šè­¦
        recent_alerts = resource_monitor.get_alerts(resolved=False, hours=1)
        if recent_alerts:
            st.warning(f"âš ï¸ å‘ç° {len(recent_alerts)} ä¸ªæœªè§£å†³çš„èµ„æºå‘Šè­¦")
            for alert in recent_alerts[:3]:  # æ˜¾ç¤ºæœ€è¿‘3ä¸ª
                st.error(f"{alert['resource_type'].upper()}: {alert['message']}")
        
        # å¯¼å‡ºåŠŸèƒ½
        st.subheader("ğŸ“¤ å¯¼å‡ºæ•°æ®")
        if st.button("å¯¼å‡ºä¼šè¯æ•°æ®"):
            session_data = st.session_state.enhanced_agent.export_session_data()
            st.download_button(
                label="ä¸‹è½½JSON",
                data=json.dumps(session_data, ensure_ascii=False, indent=2),
                file_name=f"session_{st.session_state.session_id}.json",
                mime="application/json"
            )
        
        # æ¸…ç†åŠŸèƒ½
        st.subheader("ğŸ§¹ ç»´æŠ¤")
        if st.button("æ¸…ç†æ—§è®°å½•"):
            cleaned_count = history_manager.cleanup_old_entries(days=30)
            st.success(f"æ¸…ç†äº† {cleaned_count} æ¡æ—§è®°å½•")
        
        if st.button("æ¸…ç†èµ„æºç›‘æ§æ•°æ®"):
            resource_monitor.cleanup_old_data(days=7)
            st.success("æ¸…ç†äº†7å¤©å‰çš„èµ„æºç›‘æ§æ•°æ®")

# MLå·¥å…·æ§åˆ¶ - æ·»åŠ åˆ°ä¾§è¾¹æ ä¸»åŒºåŸŸ
with st.sidebar:
    st.subheader("ğŸ§  å¢å¼ºMLå·¥å…·")
    auto_analysis = st.checkbox("è‡ªåŠ¨æ•°æ®åˆ†æ", value=True)
    smart_preprocessing = st.checkbox("æ™ºèƒ½é¢„å¤„ç†", value=True)
    auto_model_selection = st.checkbox("è‡ªåŠ¨æ¨¡å‹é€‰æ‹©", value=False)
    advanced_viz = st.checkbox("é«˜çº§å¯è§†åŒ–", value=True)

# ä¸»ç•Œé¢
app_title = app_settings.get("title", "ğŸ¤– Enhanced AutoML Workflow Agent")
app_description = app_settings.get("description", "å…·å¤‡æ™ºèƒ½è®°å¿†ã€ä»»åŠ¡è§„åˆ’å’Œå¢å¼ºæ¨ç†èƒ½åŠ›çš„AIåŠ©æ‰‹")

st.title(app_title)
st.markdown(f"### {app_description}")
st.markdown(f"**å½“å‰ä½¿ç”¨**: {current_provider.name} - {current_provider.default_model}")

# æ˜¾ç¤ºAgentçŠ¶æ€æ¦‚è¦
try:
    agent_status = st.session_state.enhanced_agent.get_agent_status()
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("è®°å¿†æ¡ç›®", agent_status.get("memory_summary", {}).get("total_memories", 0))
    with col2:
        st.metric("æ€»ä»»åŠ¡", agent_status.get("task_summary", {}).get("total_tasks", 0))
    with col3:
        st.metric("å¹³å‡å®Œæˆåº¦", f"{agent_status.get('task_summary', {}).get('avg_completion', 0):.1f}%")
    with col4:
        st.metric("ä¸Šä¸‹æ–‡çª—å£", agent_status.get("context_window_size", "N/A"))
except Exception as e:
    st.warning("âš ï¸ æ— æ³•è·å–AgentçŠ¶æ€ä¿¡æ¯")

# æ˜¾ç¤ºç³»ç»Ÿèµ„æºçŠ¶æ€
if st.session_state.resource_monitoring:
    st.subheader("ğŸ“Š ç³»ç»Ÿèµ„æºçŠ¶æ€")
    current_usage = resource_monitor.get_current_usage()
    if current_usage:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            cpu_color = "red" if current_usage.cpu_percent > 80 else "orange" if current_usage.cpu_percent > 60 else "green"
            st.metric("CPUä½¿ç”¨ç‡", f"{current_usage.cpu_percent:.1f}%", 
                     delta=None, delta_color=cpu_color)
        with col2:
            mem_color = "red" if current_usage.memory_percent > 80 else "orange" if current_usage.memory_percent > 60 else "green"
            st.metric("å†…å­˜ä½¿ç”¨ç‡", f"{current_usage.memory_percent:.1f}%", 
                     delta=f"{current_usage.memory_used_gb:.1f}GB", delta_color=mem_color)
        with col3:
            disk_color = "red" if current_usage.disk_percent > 80 else "orange" if current_usage.disk_percent > 60 else "green"
            st.metric("ç£ç›˜ä½¿ç”¨ç‡", f"{current_usage.disk_percent:.1f}%", 
                     delta=f"{current_usage.disk_used_gb:.1f}GB", delta_color=disk_color)
        with col4:
            net_mb = (current_usage.network_io_bytes.get('bytes_recv', 0) + 
                     current_usage.network_io_bytes.get('bytes_sent', 0)) / (1024*1024)
            st.metric("ç½‘ç»œIO", f"{net_mb:.1f}MB", delta="æ€»è®¡")
        
        # æ˜¾ç¤ºGPUä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if current_usage.gpu_usage and current_usage.gpu_usage.get('gpus'):
            st.subheader("ğŸ® GPUçŠ¶æ€")
            for gpu in current_usage.gpu_usage['gpus']:
                st.write(f"**{gpu['name']}**: {gpu['gpu_utilization']}% ä½¿ç”¨ç‡, "
                        f"{gpu['memory_percent']:.1f}% æ˜¾å­˜, {gpu['temperature_c']}Â°C")

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.chat_history:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage) and message.content:
        with st.chat_message("assistant"):
            st.markdown(message.content)

# è·å–ç”¨æˆ·è¾“å…¥
initial_user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„æŒ‡ä»¤...")

if initial_user_input:
    # ç«‹å³æ˜¾ç¤ºå¹¶è®°å½•ç”¨æˆ·çš„åˆæ¬¡è¾“å…¥
    with st.chat_message("user"):
        st.markdown(initial_user_input)
    st.session_state.chat_history.append(HumanMessage(content=initial_user_input))

    # ä½¿ç”¨å¢å¼ºçš„Agentå¤„ç†ç”¨æˆ·è¾“å…¥
    with st.spinner("ğŸ§  æ™ºèƒ½åˆ†æä¸­..."):
        try:
            # æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
            enhanced_context = {
                "chat_history_length": len(st.session_state.chat_history),
                "session_id": st.session_state.session_id,
                "ml_tools_available": {
                    "auto_analysis": auto_analysis,
                    "smart_preprocessing": smart_preprocessing,
                    "auto_model_selection": auto_model_selection,
                    "advanced_viz": advanced_viz
                },
                "resource_monitoring": st.session_state.resource_monitoring
            }
            
            # ä½¿ç”¨å¢å¼ºçš„Agentå¤„ç†è¾“å…¥
            enhanced_response = st.session_state.enhanced_agent.process_user_input(
                user_input=initial_user_input,
                context=enhanced_context
            )
            
            # è®°å½•å¯¹è¯åˆ°å†å²
            conversation_entry = HistoryHelpers.create_conversation_entry(
                session_id=st.session_state.session_id,
                user_input=initial_user_input,
                agent_response=enhanced_response["response"]["content"]
            )
            history_manager.add_entry(conversation_entry)
            
            # æ˜¾ç¤ºå¢å¼ºçš„å“åº”
            with st.chat_message("assistant"):
                st.markdown(enhanced_response["response"]["content"])
                
                # æ˜¾ç¤ºæ„å›¾åˆ†æï¼ˆå¯æŠ˜å ï¼‰
                if enhanced_response.get("intent_analysis"):
                    with st.expander("ğŸ¯ æ„å›¾åˆ†æ"):
                        st.json(enhanced_response["intent_analysis"])
                
                # æ˜¾ç¤ºä»»åŠ¡è®¡åˆ’ï¼ˆå¦‚æœæœ‰ï¼‰
                if enhanced_response.get("task_plan"):
                    with st.expander("ğŸ“‹ ä»»åŠ¡è®¡åˆ’"):
                        task_plan = enhanced_response["task_plan"]
                        st.write(f"**ä»»åŠ¡ID:** {task_plan['task_id']}")
                        st.write(f"**ç±»å‹:** {task_plan['task_type']}")
                        st.write(f"**ä¼˜å…ˆçº§:** {task_plan['priority']}")
                        st.write(f"**é¢„ä¼°æ—¶é•¿:** {task_plan['estimated_duration']}åˆ†é’Ÿ")
                        
                        if task_plan['subtasks']:
                            st.write("**å­ä»»åŠ¡:**")
                            for subtask in task_plan['subtasks']:
                                st.write(f"- {subtask['description']}")
                
                # æ˜¾ç¤ºç›¸å…³è®°å¿†ï¼ˆå¦‚æœæœ‰ï¼‰
                if enhanced_response.get("relevant_memories"):
                    with st.expander("ğŸ§  ç›¸å…³è®°å¿†"):
                        for memory in enhanced_response["relevant_memories"]:
                            st.write(f"**{memory['timestamp'][:19]}:** {memory['content']}")
                            if memory['tags']:
                                st.caption(f"æ ‡ç­¾: {', '.join(memory['tags'])}")
            
            # å°†å¢å¼ºå“åº”æ·»åŠ åˆ°å†å²
            st.session_state.chat_history.append(AIMessage(content=enhanced_response["response"]["content"]))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦MLå·¥å…·å¢å¼ºå¤„ç†
            should_use_ml_tools = any(keyword in initial_user_input.lower() for keyword in 
                ["åˆ†æ", "æ•°æ®", "æ¨¡å‹", "è®­ç»ƒ", "é¢„æµ‹", "å¯è§†åŒ–", "ç‰¹å¾"])
            
            if should_use_ml_tools:
                with st.spinner("ğŸ§  ä½¿ç”¨å¢å¼ºMLå·¥å…·å¤„ç†..."):
                    try:
                        # è¿™é‡Œå¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥è°ƒç”¨ç›¸åº”çš„MLå·¥å…·
                        ml_context = {
                            "auto_analysis_enabled": auto_analysis,
                            "smart_preprocessing_enabled": smart_preprocessing,
                            "auto_model_selection_enabled": auto_model_selection,
                            "advanced_viz_enabled": advanced_viz
                        }
                        
                        # æ˜¾ç¤ºMLå·¥å…·å»ºè®®
                        with st.chat_message("assistant"):
                            st.markdown("### ğŸ§  MLå·¥å…·å»ºè®®")
                            if auto_analysis and "åˆ†æ" in initial_user_input:
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨è‡ªåŠ¨æ•°æ®åˆ†æå·¥å…·æ¥å¿«é€Ÿäº†è§£æ•°æ®ç‰¹å¾")
                            if smart_preprocessing and ("é¢„å¤„ç†" in initial_user_input or "æ¸…æ´—" in initial_user_input):
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨æ™ºèƒ½é¢„å¤„ç†å·¥å…·æ¥è‡ªåŠ¨å¤„ç†æ•°æ®è´¨é‡é—®é¢˜")
                            if auto_model_selection and ("æ¨¡å‹" in initial_user_input or "è®­ç»ƒ" in initial_user_input):
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨è‡ªåŠ¨æ¨¡å‹é€‰æ‹©å·¥å…·æ¥æ‰¾åˆ°æœ€é€‚åˆçš„ç®—æ³•")
                            if advanced_viz and "å¯è§†åŒ–" in initial_user_input:
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨é«˜çº§å¯è§†åŒ–å·¥å…·æ¥åˆ›å»ºäº¤äº’å¼å›¾è¡¨")
                        
                    except Exception as ml_e:
                        logger.warning(f"ML tools enhancement failed: {str(ml_e)}")
            
            # å¦‚æœéœ€è¦æ‰§è¡Œä»£ç ï¼Œè°ƒç”¨åŸæœ‰çš„agent_executor
            if "ä»£ç " in initial_user_input or "æ‰§è¡Œ" in initial_user_input or "è¿è¡Œ" in initial_user_input:
                with st.spinner("ğŸ”§ æ‰§è¡Œä»£ç ä¸­..."):
                    try:
                        # è°ƒç”¨åŸæœ‰çš„agent_executoræ‰§è¡Œå…·ä½“ä»»åŠ¡
                        execution_response = agent_executor.invoke({
                            "input": initial_user_input,
                            "chat_history": st.session_state.chat_history[:-2]
                        })
                        
                        execution_content = execution_response["output"]
                        
                        # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
                        with st.chat_message("assistant"):
                            st.markdown("### ğŸ”§ æ‰§è¡Œç»“æœ")
                            st.markdown(execution_content)
                        
                        # è®°å½•æ‰§è¡Œç»“æœ
                        st.session_state.chat_history.append(AIMessage(content=f"æ‰§è¡Œç»“æœ: {execution_content}"))
                        
                        # è®°å½•æ‰§è¡Œåˆ°å†å²
                        execution_entry = HistoryHelpers.create_execution_entry(
                            session_id=st.session_state.session_id,
                            code=initial_user_input,
                            result=execution_content,
                            execution_time=2.0,  # ç®€åŒ–å¤„ç†
                            success=True
                        )
                        history_manager.add_entry(execution_entry)
                        
                    except Exception as exec_e:
                        error_message = f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(exec_e)}"
                        with st.chat_message("assistant"):
                            st.error(error_message)
                        
                        # è®°å½•é”™è¯¯åˆ°å†å²
                        error_entry = HistoryHelpers.create_error_entry(
                            session_id=st.session_state.session_id,
                            error_message=str(exec_e),
                            error_type="execution_error",
                            context={"user_input": initial_user_input}
                        )
                        history_manager.add_entry(error_entry)

        except Exception as e:
            # å¦‚æœinvokeåœ¨ä»»ä½•æ­¥éª¤å¤±è´¥äº†ï¼ˆåŒ…æ‹¬æˆ‘ä»¬å…³å¿ƒçš„ValidationErrorï¼‰
            print(f"--- [Agent Error Caught] æ•è·åˆ°å¼‚å¸¸: {e} ---")
            
            # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ï¼Œå‡†å¤‡ä½œä¸ºæ–°çš„è¾“å…¥åé¦ˆç»™Agent
            error_feedback_prompt = f"""
            æˆ‘åœ¨å°è¯•æ‰§è¡Œä½ ä¸Šä¸€æ­¥çš„è®¡åˆ’æ—¶é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ã€‚è¯·åˆ†æä¸‹é¢çš„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶ä¿®æ­£ä½ ä¹‹å‰çš„å·¥å…·è°ƒç”¨æˆ–è®¡åˆ’ã€‚

            **é”™è¯¯ä¿¡æ¯:**
            ```
            {str(e)}
            ```

            è¯·æ ¹æ®è¿™ä¸ªé”™è¯¯ï¼Œé‡æ–°ç”Ÿæˆä¸€ä¸ªæ­£ç¡®çš„å·¥å…·è°ƒç”¨ã€‚
            """
            
            # å°†è¿™ä¸ªé”™è¯¯åé¦ˆä½œä¸ºæ–°çš„è¾“å…¥ï¼Œå†æ¬¡è°ƒç”¨Agent
            with st.spinner("æ£€æµ‹åˆ°é”™è¯¯ï¼Œå°è¯•è‡ªæˆ‘ä¿®æ­£..."):
                try:
                    # æˆ‘ä»¬å°†é”™è¯¯åé¦ˆä¹Ÿä½œä¸ºHumanMessageæ·»åŠ åˆ°å†å²ä¸­ï¼Œè®©ä¸Šä¸‹æ–‡æ›´æ¸…æ™°
                    st.session_state.chat_history.append(HumanMessage(content=error_feedback_prompt))

                    # å†æ¬¡è°ƒç”¨ï¼Œä½†è¿™æ¬¡çš„inputæ˜¯æˆ‘ä»¬çš„é”™è¯¯åé¦ˆ
                    # å†å²è®°å½•ç°åœ¨åŒ…å«äº†åŸå§‹è¾“å…¥å’Œæˆ‘ä»¬çš„é”™è¯¯åé¦ˆ
                    corrected_response = agent_executor.invoke({
                        "input": error_feedback_prompt, 
                        "chat_history": st.session_state.chat_history[:-1] # ä¼ é€’åŒ…å«åŸå§‹è¾“å…¥ä½†ä¸åŒ…å«æˆ‘ä»¬åˆšå‘çš„é”™è¯¯åé¦ˆçš„å†å²
                    })

                    ai_response_content = corrected_response["output"]

                    # æ˜¾ç¤ºä¿®æ­£åçš„æœ€ç»ˆå›å¤
                    with st.chat_message("assistant"):
                        st.markdown(ai_response_content)
                    
                    # å°†æœ€ç»ˆçš„æˆåŠŸå›å¤æ·»åŠ åˆ°å†å²
                    st.session_state.chat_history.append(AIMessage(content=ai_response_content))

                except Exception as final_e:
                    # å¦‚æœåœ¨ä¿®æ­£åå†æ¬¡è°ƒç”¨ä»ç„¶å¤±è´¥
                    final_error_message = f"æŠ±æ­‰ï¼Œåœ¨å°è¯•è‡ªæˆ‘ä¿®æ­£åï¼Œæˆ‘ä»ç„¶é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼š\n\n```\n{final_e}\n```"
                    with st.chat_message("assistant"):
                        st.error(final_error_message)
                    st.session_state.chat_history.append(AIMessage(content=final_error_message))