# app.py

import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import Tool
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import StructuredTool
import os
from local_tools import (
    safe_python_executor, 
    read_local_file, 
    write_local_file, 
    # list_directory_contents, # å¯ä»¥æ›¿æ¢æ‰æ—§çš„
    list_directory_items_with_paths, # æ–°çš„å·¥å…·
    make_web_request 
)
from sanbox_main import (
    execute_ml_code_in_docker,
    CONTAINER_CODE_EXECUTION_BASE_TARGET,
    CONTAINER_OUTPUTS_MOUNT_TARGET,
    CONTAINER_DATASETS_MOUNT_TARGET,
    HOST_EXECUTION_LOGS_ROOT,
    HOST_OUTPUTS_MOUNT_SOURCE,
    HOST_DATASETS_MOUNT_SOURCE,
    HOST_ROOT_WORKSPACE,
    DOCKER_IMAGE_NAME,
    SandboxExecutionInput
)
from langchain_community.tools.file_management import (
    CopyFileTool,
    DeleteFileTool,
    FileSearchTool,
    ListDirectoryTool,
    MoveFileTool,
    ReadFileTool,
    WriteFileTool,
)
from db_tools import (
    SaveMLResultInput,
    DatabaseQueryInput,
    QueryMLResultsInput,
    save_ml_result_to_db,
    query_execution_history,
    query_ml_results_from_db,
)
from config import (SYSTEM_PROMPT, 
        EXECUTE_CODE_TOOL_DESCRIPTION,
        SYSTEM_PROMPT_2,
        QUERY_EXEC_LOGS_TOOL_DESCRIPTION,
        QUERY_ML_RESULTS_TOOL_DESCRIPTION,
        SAVE_ML_RESULT_TOOL_DESCRIPTION
)

st.set_page_config(page_title="AutoML Workflow Agent", layout="wide")

# åŠ è½½ç¯å¢ƒå˜é‡ (OPENAI_API_KEY)

# å¯¼å…¥æˆ‘ä»¬æœ¬åœ°å®šä¹‰çš„å·¥å…· 
from local_tools import safe_python_executor

# 1. åˆå§‹åŒ–LLM
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-preview-05-20", google_api_key=) # æˆ–è€… gpt-4-turbogem

execute_in_ml_sandbox_tool = StructuredTool.from_function(
    func=execute_ml_code_in_docker, # æŒ‡å‘æˆ‘ä»¬å¥å£®çš„åç«¯å‡½æ•°
    name="ExecutePythonInMLSandbox", # å·¥å…·çš„è°ƒç”¨åç§°
    description=EXECUTE_CODE_TOOL_DESCRIPTION, # ä¸Šé¢å®šä¹‰çš„è¯¦ç»†æè¿°
    args_schema=SandboxExecutionInput # Pydanticæ¨¡å‹å®šä¹‰è¾“å…¥å‚æ•°
)
query_system_execution_logs_tool = StructuredTool.from_function(
    func=query_execution_history, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="QuerySystemExecutionLogs",
    description=QUERY_EXEC_LOGS_TOOL_DESCRIPTION,
    args_schema=DatabaseQueryInput # ä½¿ç”¨ DatabaseQueryInput
)
save_ml_result_tool = StructuredTool.from_function(
    func=save_ml_result_to_db, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="SaveMachineLearningResult",
    description=SAVE_ML_RESULT_TOOL_DESCRIPTION,
    args_schema=SaveMLResultInput # ä½¿ç”¨ SaveMLResultInput
)
query_ml_results_tool = StructuredTool.from_function(
    func=query_ml_results_from_db, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="QueryMachineLearningResults",
    description=QUERY_ML_RESULTS_TOOL_DESCRIPTION,
    args_schema=QueryMLResultsInput # ä½¿ç”¨ QueryMLResultsInput
)

tools = [
    execute_in_ml_sandbox_tool,
    query_system_execution_logs_tool,
    save_ml_result_tool,
    query_ml_results_tool
]

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_PROMPT_2), # ä½¿ç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„è¯¦ç»†ç³»ç»Ÿæç¤º
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

# --- Agentå’ŒAgentExecutorçš„åˆ›å»º (ä¸æ‚¨ä¹‹å‰çš„ä»£ç ç±»ä¼¼) ---
agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,  # å¤„ç†è§£æé”™è¯¯
)

# ------------- Streamlit UIéƒ¨åˆ† -------------
# ==============================================================================
#st.set_page_config(page_title="AutoML Workflow Agent", layout="wide")
st.title("ğŸ¤– AutoML Workflow Agent")

# åˆå§‹åŒ–å¯¹è¯å†å² (ä»ç„¶ä½¿ç”¨Messageå¯¹è±¡)
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.chat_history:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage) and message.content:
        with st.chat_message("assistant"):
            st.markdown(message.content)

# è·å–ç”¨æˆ·è¾“å…¥
initial_user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„æŒ‡ä»¤...")

if initial_user_input:
    # ç«‹å³æ˜¾ç¤ºå¹¶è®°å½•ç”¨æˆ·çš„åˆæ¬¡è¾“å…¥
    with st.chat_message("user"):
        st.markdown(initial_user_input)
    st.session_state.chat_history.append(HumanMessage(content=initial_user_input))

    # --- æœ€ç»ˆä¼˜åŒ–çš„é”™è¯¯åé¦ˆé€»è¾‘ ---
    
    # å‡†å¤‡ç¬¬ä¸€æ¬¡è°ƒç”¨çš„è¾“å…¥
    current_input = initial_user_input
    
    with st.spinner("æ€è€ƒä¸­..."):
        try:
            # ç›´æ¥è°ƒç”¨agent_executor
            response = agent_executor.invoke({
                "input": current_input,
                "chat_history": st.session_state.chat_history[:-1] # ä¼ é€’åˆ°å½“å‰ç”¨æˆ·è¾“å…¥ä¹‹å‰çš„æ‰€æœ‰å†å²
            })
            
            # å¦‚æœinvokeæˆåŠŸï¼Œæ²¡æœ‰æŠ›å‡ºä»»ä½•å¼‚å¸¸
            ai_response_content = response["output"]

            # æ˜¾ç¤ºAIçš„æœ€ç»ˆå›å¤
            with st.chat_message("assistant"):
                st.markdown(ai_response_content)
                
            # å°†AIçš„æœ€ç»ˆæˆåŠŸå›å¤æ·»åŠ åˆ°å†å²
            st.session_state.chat_history.append(AIMessage(content=ai_response_content))

        except Exception as e:
            # å¦‚æœinvokeåœ¨ä»»ä½•æ­¥éª¤å¤±è´¥äº†ï¼ˆåŒ…æ‹¬æˆ‘ä»¬å…³å¿ƒçš„ValidationErrorï¼‰
            print(f"--- [Agent Error Caught] æ•è·åˆ°å¼‚å¸¸: {e} ---")
            
            # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ï¼Œå‡†å¤‡ä½œä¸ºæ–°çš„è¾“å…¥åé¦ˆç»™Agent
            error_feedback_prompt = f"""
            æˆ‘åœ¨å°è¯•æ‰§è¡Œä½ ä¸Šä¸€æ­¥çš„è®¡åˆ’æ—¶é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ã€‚è¯·åˆ†æä¸‹é¢çš„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶ä¿®æ­£ä½ ä¹‹å‰çš„å·¥å…·è°ƒç”¨æˆ–è®¡åˆ’ã€‚

            **é”™è¯¯ä¿¡æ¯:**
            ```
            {str(e)}
            ```

            è¯·æ ¹æ®è¿™ä¸ªé”™è¯¯ï¼Œé‡æ–°ç”Ÿæˆä¸€ä¸ªæ­£ç¡®çš„å·¥å…·è°ƒç”¨ã€‚
            """
            
            # å°†è¿™ä¸ªé”™è¯¯åé¦ˆä½œä¸ºæ–°çš„è¾“å…¥ï¼Œå†æ¬¡è°ƒç”¨Agent
            with st.spinner("æ£€æµ‹åˆ°é”™è¯¯ï¼Œå°è¯•è‡ªæˆ‘ä¿®æ­£..."):
                try:
                    # æˆ‘ä»¬å°†é”™è¯¯åé¦ˆä¹Ÿä½œä¸ºHumanMessageæ·»åŠ åˆ°å†å²ä¸­ï¼Œè®©ä¸Šä¸‹æ–‡æ›´æ¸…æ™°
                    st.session_state.chat_history.append(HumanMessage(content=error_feedback_prompt))

                    # å†æ¬¡è°ƒç”¨ï¼Œä½†è¿™æ¬¡çš„inputæ˜¯æˆ‘ä»¬çš„é”™è¯¯åé¦ˆ
                    # å†å²è®°å½•ç°åœ¨åŒ…å«äº†åŸå§‹è¾“å…¥å’Œæˆ‘ä»¬çš„é”™è¯¯åé¦ˆ
                    corrected_response = agent_executor.invoke({
                        "input": error_feedback_prompt, 
                        "chat_history": st.session_state.chat_history[:-1] # ä¼ é€’åŒ…å«åŸå§‹è¾“å…¥ä½†ä¸åŒ…å«æˆ‘ä»¬åˆšå‘çš„é”™è¯¯åé¦ˆçš„å†å²
                    })

                    ai_response_content = corrected_response["output"]

                    # æ˜¾ç¤ºä¿®æ­£åçš„æœ€ç»ˆå›å¤
                    with st.chat_message("assistant"):
                        st.markdown(ai_response_content)
                    
                    # å°†æœ€ç»ˆçš„æˆåŠŸå›å¤æ·»åŠ åˆ°å†å²
                    st.session_state.chat_history.append(AIMessage(content=ai_response_content))

                except Exception as final_e:
                    # å¦‚æœåœ¨ä¿®æ­£åå†æ¬¡è°ƒç”¨ä»ç„¶å¤±è´¥
                    final_error_message = f"æŠ±æ­‰ï¼Œåœ¨å°è¯•è‡ªæˆ‘ä¿®æ­£åï¼Œæˆ‘ä»ç„¶é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼š\n\n```\n{final_e}\n```"
                    with st.chat_message("assistant"):
                        st.error(final_error_message)
                    st.session_state.chat_history.append(AIMessage(content=final_error_message))