# intelligent_memory.py
"""
æ™ºèƒ½MLè®°å¿†ç®¡ç†ç³»ç»Ÿ
åŸºäºgemini-cliçš„è®°å¿†æ¶æ„ï¼Œç»“åˆå‘é‡æœç´¢å’ŒçŸ¥è¯†å›¾è°±
æä¾›é¡¹ç›®çº§è®°å¿†ã€ä¸Šä¸‹æ–‡æ£€ç´¢å’Œæ™ºèƒ½çŸ¥è¯†ç®¡ç†
"""

import asyncio
import json
import sqlite3
import hashlib
import pickle
import ast
import re
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple, Union
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

@dataclass
class MemoryNode:
    """è®°å¿†èŠ‚ç‚¹"""
    id: str
    content: str
    memory_type: str  # conversation, execution, insight, knowledge, pattern
    tags: Set[str] = field(default_factory=set)
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    importance: float = 0.5  # é‡è¦æ€§è¯„åˆ† 0-1
    embedding: Optional[np.ndarray] = None
    
    # å…³ç³»ä¿¡æ¯
    related_nodes: Set[str] = field(default_factory=set)
    parent_nodes: Set[str] = field(default_factory=set)
    child_nodes: Set[str] = field(default_factory=set)
    
    # è®¿é—®ç»Ÿè®¡
    access_count: int = 0
    last_accessed: datetime = field(default_factory=datetime.now)

@dataclass
class MLInsight:
    """MLæ´å¯Ÿ"""
    id: str
    title: str
    description: str
    insight_type: str  # pattern, performance, data_quality, model_behavior
    confidence: float
    supporting_evidence: List[str]
    applicable_contexts: List[str]
    discovered_at: datetime = field(default_factory=datetime.now)

@dataclass
class DataLineage:
    """æ•°æ®è¡€ç¼˜"""
    source_id: str
    target_id: str
    transformation: str
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    node: MemoryNode
    similarity: float
    relevance_explanation: str

class IntelligentMLMemoryManager:
    """æ™ºèƒ½MLè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = ".", memory_db_path: str = None):
        self.project_root = Path(project_root).resolve()
        self.memory_db_path = memory_db_path or self.project_root / ".mlagent" / "memory.db"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.memory_db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å†…å­˜ä¸­çš„æ•°æ®ç»“æ„
        self.memory_nodes: Dict[str, MemoryNode] = {}
        self.knowledge_graph = nx.DiGraph()
        self.data_lineage_graph = nx.DiGraph()
        self.insights: Dict[str, MLInsight] = {}
        
        # å‘é‡åŒ–å’Œæœç´¢
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.document_vectors = None
        self.document_ids = []
        
        # ç¼“å­˜
        self.search_cache = {}
        self.cache_ttl = 3600  # 1å°æ—¶
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
        
        # åŠ è½½ç°æœ‰è®°å¿†
        asyncio.create_task(self._load_memories())
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        self.conn = sqlite3.connect(str(self.memory_db_path), check_same_thread=False)
        
        # åˆ›å»ºè¡¨
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS memory_nodes (
                id TEXT PRIMARY KEY,
                content TEXT NOT NULL,
                memory_type TEXT NOT NULL,
                tags TEXT,
                metadata TEXT,
                timestamp TEXT,
                importance REAL,
                embedding BLOB,
                related_nodes TEXT,
                parent_nodes TEXT,
                child_nodes TEXT,
                access_count INTEGER DEFAULT 0,
                last_accessed TEXT
            );
            
            CREATE TABLE IF NOT EXISTS ml_insights (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                description TEXT NOT NULL,
                insight_type TEXT NOT NULL,
                confidence REAL,
                supporting_evidence TEXT,
                applicable_contexts TEXT,
                discovered_at TEXT
            );
            
            CREATE TABLE IF NOT EXISTS data_lineage (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id TEXT NOT NULL,
                target_id TEXT NOT NULL,
                transformation TEXT NOT NULL,
                metadata TEXT,
                UNIQUE(source_id, target_id, transformation)
            );
            
            CREATE INDEX IF NOT EXISTS idx_memory_type ON memory_nodes(memory_type);
            CREATE INDEX IF NOT EXISTS idx_timestamp ON memory_nodes(timestamp);
            CREATE INDEX IF NOT EXISTS idx_importance ON memory_nodes(importance);
            CREATE INDEX IF NOT EXISTS idx_insight_type ON ml_insights(insight_type);
        """)
        
        self.conn.commit()
    
    async def _load_memories(self):
        """ä»æ•°æ®åº“åŠ è½½è®°å¿†"""
        print("ğŸ§  åŠ è½½ç°æœ‰è®°å¿†...")
        
        cursor = self.conn.cursor()
        
        # åŠ è½½è®°å¿†èŠ‚ç‚¹
        cursor.execute("SELECT * FROM memory_nodes")
        for row in cursor.fetchall():
            node = self._row_to_memory_node(row)
            self.memory_nodes[node.id] = node
            
            # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
            self.knowledge_graph.add_node(node.id, node=node)
            
            # æ·»åŠ å…³ç³»è¾¹
            for related_id in node.related_nodes:
                if related_id in self.memory_nodes:
                    self.knowledge_graph.add_edge(node.id, related_id, relation='related')
        
        # åŠ è½½æ´å¯Ÿ
        cursor.execute("SELECT * FROM ml_insights")
        for row in cursor.fetchall():
            insight = self._row_to_insight(row)
            self.insights[insight.id] = insight
        
        # åŠ è½½æ•°æ®è¡€ç¼˜
        cursor.execute("SELECT * FROM data_lineage")
        for row in cursor.fetchall():
            lineage = self._row_to_lineage(row)
            self.data_lineage_graph.add_edge(
                lineage.source_id, 
                lineage.target_id, 
                transformation=lineage.transformation,
                metadata=lineage.metadata
            )
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.memory_nodes)} ä¸ªè®°å¿†èŠ‚ç‚¹, {len(self.insights)} ä¸ªæ´å¯Ÿ")
        
        # é‡å»ºå‘é‡ç´¢å¼•
        if self.memory_nodes:
            await self._rebuild_vector_index()
    
    def _row_to_memory_node(self, row) -> MemoryNode:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºè®°å¿†èŠ‚ç‚¹"""
        return MemoryNode(
            id=row[0],
            content=row[1],
            memory_type=row[2],
            tags=set(json.loads(row[3]) if row[3] else []),
            metadata=json.loads(row[4]) if row[4] else {},
            timestamp=datetime.fromisoformat(row[5]),
            importance=row[6],
            embedding=pickle.loads(row[7]) if row[7] else None,
            related_nodes=set(json.loads(row[8]) if row[8] else []),
            parent_nodes=set(json.loads(row[9]) if row[9] else []),
            child_nodes=set(json.loads(row[10]) if row[10] else []),
            access_count=row[11],
            last_accessed=datetime.fromisoformat(row[12])
        )
    
    def _row_to_insight(self, row) -> MLInsight:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºæ´å¯Ÿ"""
        return MLInsight(
            id=row[0],
            title=row[1],
            description=row[2],
            insight_type=row[3],
            confidence=row[4],
            supporting_evidence=json.loads(row[5]) if row[5] else [],
            applicable_contexts=json.loads(row[6]) if row[6] else [],
            discovered_at=datetime.fromisoformat(row[7])
        )
    
    def _row_to_lineage(self, row) -> DataLineage:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºæ•°æ®è¡€ç¼˜"""
        return DataLineage(
            source_id=row[1],
            target_id=row[2],
            transformation=row[3],
            metadata=json.loads(row[4]) if row[4] else {}
        )
    
    async def add_memory(self, 
                        content: str, 
                        memory_type: str,
                        tags: Set[str] = None,
                        metadata: Dict[str, Any] = None,
                        importance: float = 0.5) -> str:
        """æ·»åŠ æ–°è®°å¿†"""
        
        # ç”Ÿæˆå”¯ä¸€ID
        memory_id = hashlib.md5(f"{content}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
        
        # åˆ›å»ºè®°å¿†èŠ‚ç‚¹
        node = MemoryNode(
            id=memory_id,
            content=content,
            memory_type=memory_type,
            tags=tags or set(),
            metadata=metadata or {},
            importance=importance
        )
        
        # ç”Ÿæˆå†…å®¹å‘é‡
        node.embedding = await self._generate_embedding(content)
        
        # å­˜å‚¨åˆ°å†…å­˜
        self.memory_nodes[memory_id] = node
        
        # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
        self.knowledge_graph.add_node(memory_id, node=node)
        
        # è‡ªåŠ¨å‘ç°ç›¸å…³è®°å¿†
        related_memories = await self._find_related_memories(node)
        for related_id, similarity in related_memories[:5]:  # å–å‰5ä¸ªæœ€ç›¸å…³çš„
            if similarity > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                node.related_nodes.add(related_id)
                self.memory_nodes[related_id].related_nodes.add(memory_id)
                self.knowledge_graph.add_edge(memory_id, related_id, relation='related', weight=similarity)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        await self._save_memory_node(node)
        
        # é‡å»ºå‘é‡ç´¢å¼•
        await self._rebuild_vector_index()
        
        print(f"ğŸ’­ æ–°å¢è®°å¿†: {memory_type} - {content[:50]}...")
        return memory_id
    
    async def add_conversation_memory(self, 
                                   user_input: str, 
                                   agent_response: str,
                                   context: Dict[str, Any] = None) -> str:
        """æ·»åŠ å¯¹è¯è®°å¿†"""
        
        conversation_content = f"User: {user_input}\nAgent: {agent_response}"
        
        metadata = {
            'user_input': user_input,
            'agent_response': agent_response,
            'context': context or {},
            'conversation_length': len(user_input) + len(agent_response)
        }
        
        # è‡ªåŠ¨æå–æ ‡ç­¾
        tags = self._extract_tags_from_text(conversation_content)
        
        # è®¡ç®—é‡è¦æ€§
        importance = self._calculate_conversation_importance(user_input, agent_response, context)
        
        return await self.add_memory(
            content=conversation_content,
            memory_type='conversation',
            tags=tags,
            metadata=metadata,
            importance=importance
        )
    
    async def add_execution_memory(self, 
                                 tool_name: str,
                                 parameters: Dict[str, Any],
                                 result: Any,
                                 success: bool,
                                 execution_time: float) -> str:
        """æ·»åŠ æ‰§è¡Œè®°å¿†"""
        
        content = f"Tool: {tool_name}\nParameters: {json.dumps(parameters, indent=2)}\nResult: {str(result)[:200]}"
        
        metadata = {
            'tool_name': tool_name,
            'parameters': parameters,
            'result': result,
            'success': success,
            'execution_time': execution_time
        }
        
        tags = {tool_name, 'execution'}
        if success:
            tags.add('success')
        else:
            tags.add('failure')
        
        importance = 0.7 if success else 0.9  # å¤±è´¥çš„æ‰§è¡Œæ›´é‡è¦ï¼Œéœ€è¦å­¦ä¹ 
        
        return await self.add_memory(
            content=content,
            memory_type='execution',
            tags=tags,
            metadata=metadata,
            importance=importance
        )
    
    async def add_insight(self, 
                         title: str,
                         description: str,
                         insight_type: str,
                         confidence: float = 0.8,
                         supporting_evidence: List[str] = None,
                         applicable_contexts: List[str] = None) -> str:
        """æ·»åŠ MLæ´å¯Ÿ"""
        
        insight_id = hashlib.md5(f"{title}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
        
        insight = MLInsight(
            id=insight_id,
            title=title,
            description=description,
            insight_type=insight_type,
            confidence=confidence,
            supporting_evidence=supporting_evidence or [],
            applicable_contexts=applicable_contexts or []
        )
        
        self.insights[insight_id] = insight
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        await self._save_insight(insight)
        
        # åˆ›å»ºå¯¹åº”çš„è®°å¿†èŠ‚ç‚¹
        memory_content = f"Insight: {title}\n{description}"
        await self.add_memory(
            content=memory_content,
            memory_type='insight',
            tags={'insight', insight_type},
            metadata={'insight_id': insight_id, 'confidence': confidence},
            importance=confidence
        )
        
        print(f"ğŸ’¡ æ–°å¢æ´å¯Ÿ: {title}")
        return insight_id
    
    async def smart_search(self, 
                          query: str,
                          memory_types: List[str] = None,
                          tags: Set[str] = None,
                          limit: int = 10,
                          min_similarity: float = 0.1) -> List[SearchResult]:
        """æ™ºèƒ½æœç´¢è®°å¿†"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = hashlib.md5(f"{query}_{memory_types}_{tags}_{limit}".encode()).hexdigest()
        if cache_key in self.search_cache:
            cached_result, timestamp = self.search_cache[cache_key]
            if (datetime.now().timestamp() - timestamp) < self.cache_ttl:
                return cached_result
        
        print(f"ğŸ” æ™ºèƒ½æœç´¢: {query}")
        
        results = []
        
        if not self.memory_nodes:
            return results
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = await self._generate_embedding(query)
        
        # è®¡ç®—ä¸æ‰€æœ‰è®°å¿†çš„ç›¸ä¼¼åº¦
        for node_id, node in self.memory_nodes.items():
            # ç±»å‹è¿‡æ»¤
            if memory_types and node.memory_type not in memory_types:
                continue
            
            # æ ‡ç­¾è¿‡æ»¤
            if tags and not tags.intersection(node.tags):
                continue
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            if node.embedding is not None:
                similarity = self._calculate_similarity(query_embedding, node.embedding)
            else:
                # å›é€€åˆ°æ–‡æœ¬ç›¸ä¼¼åº¦
                similarity = self._calculate_text_similarity(query, node.content)
            
            if similarity >= min_similarity:
                # ç”Ÿæˆç›¸å…³æ€§è§£é‡Š
                explanation = self._generate_relevance_explanation(query, node, similarity)
                
                results.append(SearchResult(
                    node=node,
                    similarity=similarity,
                    relevance_explanation=explanation
                ))
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                node.access_count += 1
                node.last_accessed = datetime.now()
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x.similarity, reverse=True)
        results = results[:limit]
        
        # ç¼“å­˜ç»“æœ
        self.search_cache[cache_key] = (results, datetime.now().timestamp())
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³è®°å¿†")
        return results
    
    async def get_contextual_memories(self, 
                                    current_context: Dict[str, Any],
                                    limit: int = 5) -> List[MemoryNode]:
        """è·å–ä¸Šä¸‹æ–‡ç›¸å…³çš„è®°å¿†"""
        
        contextual_memories = []
        
        # æå–ä¸Šä¸‹æ–‡å…³é”®ä¿¡æ¯
        context_keys = []
        if 'current_task' in current_context:
            context_keys.append(current_context['current_task'])
        if 'data_files' in current_context:
            context_keys.extend(current_context['data_files'])
        if 'model_type' in current_context:
            context_keys.append(current_context['model_type'])
        
        # ä¸ºæ¯ä¸ªä¸Šä¸‹æ–‡å…³é”®ä¿¡æ¯æœç´¢ç›¸å…³è®°å¿†
        for key in context_keys:
            search_results = await self.smart_search(str(key), limit=3)
            for result in search_results:
                if result.node not in [m.node for m in contextual_memories]:
                    contextual_memories.append(result)
        
        # æŒ‰é‡è¦æ€§å’Œç›¸ä¼¼åº¦æ’åº
        contextual_memories.sort(key=lambda x: x.node.importance * x.similarity, reverse=True)
        
        return [result.node for result in contextual_memories[:limit]]
    
    async def discover_patterns(self) -> List[MLInsight]:
        """è‡ªåŠ¨å‘ç°æ¨¡å¼å’Œæ´å¯Ÿ"""
        
        print("ğŸ” è‡ªåŠ¨å‘ç°æ¨¡å¼...")
        new_insights = []
        
        # åˆ†ææ‰§è¡Œæ¨¡å¼
        execution_patterns = await self._analyze_execution_patterns()
        new_insights.extend(execution_patterns)
        
        # åˆ†ææ•°æ®ä½¿ç”¨æ¨¡å¼
        data_patterns = await self._analyze_data_usage_patterns()
        new_insights.extend(data_patterns)
        
        # åˆ†ææ€§èƒ½æ¨¡å¼
        performance_patterns = await self._analyze_performance_patterns()
        new_insights.extend(performance_patterns)
        
        # ä¿å­˜æ–°å‘ç°çš„æ´å¯Ÿ
        for insight in new_insights:
            await self.add_insight(
                title=insight['title'],
                description=insight['description'],
                insight_type=insight['type'],
                confidence=insight['confidence'],
                supporting_evidence=insight['evidence']
            )
        
        print(f"ğŸ’¡ å‘ç° {len(new_insights)} ä¸ªæ–°æ¨¡å¼")
        return new_insights
    
    async def _analyze_execution_patterns(self) -> List[Dict[str, Any]]:
        """åˆ†ææ‰§è¡Œæ¨¡å¼"""
        patterns = []
        
        # è·å–æ‰€æœ‰æ‰§è¡Œè®°å¿†
        execution_memories = [
            node for node in self.memory_nodes.values() 
            if node.memory_type == 'execution'
        ]
        
        if len(execution_memories) < 5:
            return patterns
        
        # åˆ†æå·¥å…·ä½¿ç”¨é¢‘ç‡
        tool_usage = {}
        for memory in execution_memories:
            tool_name = memory.metadata.get('tool_name')
            if tool_name:
                tool_usage[tool_name] = tool_usage.get(tool_name, 0) + 1
        
        # æ‰¾å‡ºæœ€å¸¸ç”¨çš„å·¥å…·
        if tool_usage:
            most_used_tool = max(tool_usage, key=tool_usage.get)
            usage_count = tool_usage[most_used_tool]
            
            if usage_count >= 3:
                patterns.append({
                    'title': f'é«˜é¢‘ä½¿ç”¨å·¥å…·: {most_used_tool}',
                    'description': f'å·¥å…· {most_used_tool} è¢«ä½¿ç”¨äº† {usage_count} æ¬¡ï¼Œæ˜¯æœ€å¸¸ç”¨çš„å·¥å…·',
                    'type': 'tool_usage',
                    'confidence': min(0.9, usage_count / len(execution_memories)),
                    'evidence': [f'å·¥å…·ä½¿ç”¨æ¬¡æ•°: {usage_count}']
                })
        
        # åˆ†æå¤±è´¥æ¨¡å¼
        failed_executions = [
            memory for memory in execution_memories 
            if not memory.metadata.get('success', True)
        ]
        
        if len(failed_executions) >= 2:
            failure_rate = len(failed_executions) / len(execution_memories)
            patterns.append({
                'title': 'æ‰§è¡Œå¤±è´¥æ¨¡å¼',
                'description': f'å‘ç° {len(failed_executions)} æ¬¡æ‰§è¡Œå¤±è´¥ï¼Œå¤±è´¥ç‡ä¸º {failure_rate:.1%}',
                'type': 'failure_pattern',
                'confidence': min(0.8, failure_rate * 2),
                'evidence': [f'å¤±è´¥æ¬¡æ•°: {len(failed_executions)}', f'æ€»æ‰§è¡Œæ¬¡æ•°: {len(execution_memories)}']
            })
        
        return patterns
    
    async def _analyze_data_usage_patterns(self) -> List[Dict[str, Any]]:
        """åˆ†ææ•°æ®ä½¿ç”¨æ¨¡å¼"""
        patterns = []
        
        # åˆ†ææ•°æ®è¡€ç¼˜
        if len(self.data_lineage_graph.edges()) >= 3:
            # æ‰¾å‡ºæ•°æ®å¤„ç†é“¾
            longest_chain = []
            for node in self.data_lineage_graph.nodes():
                if self.data_lineage_graph.in_degree(node) == 0:  # æºèŠ‚ç‚¹
                    chain = self._find_longest_path_from_node(node)
                    if len(chain) > len(longest_chain):
                        longest_chain = chain
            
            if len(longest_chain) >= 3:
                patterns.append({
                    'title': 'å¤æ‚æ•°æ®å¤„ç†é“¾',
                    'description': f'å‘ç°é•¿åº¦ä¸º {len(longest_chain)} çš„æ•°æ®å¤„ç†é“¾',
                    'type': 'data_pipeline',
                    'confidence': 0.7,
                    'evidence': [f'å¤„ç†é“¾: {" -> ".join(longest_chain)}']
                })
        
        return patterns
    
    async def _analyze_performance_patterns(self) -> List[Dict[str, Any]]:
        """åˆ†ææ€§èƒ½æ¨¡å¼"""
        patterns = []
        
        # è·å–æ‰§è¡Œæ—¶é—´æ•°æ®
        execution_times = []
        for memory in self.memory_nodes.values():
            if memory.memory_type == 'execution':
                exec_time = memory.metadata.get('execution_time')
                if exec_time:
                    execution_times.append((memory.metadata.get('tool_name'), exec_time))
        
        if len(execution_times) >= 5:
            # åˆ†æå¹³å‡æ‰§è¡Œæ—¶é—´
            avg_time = sum(time for _, time in execution_times) / len(execution_times)
            slow_executions = [item for item in execution_times if item[1] > avg_time * 2]
            
            if slow_executions:
                patterns.append({
                    'title': 'æ€§èƒ½ç“¶é¢ˆè¯†åˆ«',
                    'description': f'å‘ç° {len(slow_executions)} ä¸ªæ‰§è¡Œæ—¶é—´å¼‚å¸¸çš„æ“ä½œ',
                    'type': 'performance',
                    'confidence': 0.6,
                    'evidence': [f'å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.2f}s', f'å¼‚å¸¸æ“ä½œæ•°: {len(slow_executions)}']
                })
        
        return patterns
    
    def _find_longest_path_from_node(self, start_node: str) -> List[str]:
        """ä»æŒ‡å®šèŠ‚ç‚¹å¼€å§‹æ‰¾æœ€é•¿è·¯å¾„"""
        longest_path = [start_node]
        
        def dfs(node, current_path):
            nonlocal longest_path
            if len(current_path) > len(longest_path):
                longest_path = current_path.copy()
            
            for successor in self.data_lineage_graph.successors(node):
                dfs(successor, current_path + [successor])
        
        dfs(start_node, [start_node])
        return longest_path
    
    async def _generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬å‘é‡"""
        # ç®€åŒ–çš„TF-IDFå‘é‡åŒ–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å…ˆè¿›çš„embeddingæ¨¡å‹ï¼‰
        try:
            if not hasattr(self, '_temp_vectorizer'):
                self._temp_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
                # ç”¨ç°æœ‰çš„æ‰€æœ‰æ–‡æœ¬æ¥è®­ç»ƒå‘é‡åŒ–å™¨
                all_texts = [node.content for node in self.memory_nodes.values()]
                all_texts.append(text)
                self._temp_vectorizer.fit(all_texts)
            
            vector = self._temp_vectorizer.transform([text]).toarray()[0]
            return vector
        except:
            # å¦‚æœå‘é‡åŒ–å¤±è´¥ï¼Œè¿”å›éšæœºå‘é‡
            return np.random.random(100)
    
    def _calculate_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—å‘é‡ç›¸ä¼¼åº¦"""
        try:
            return cosine_similarity([vec1], [vec2])[0][0]
        except:
            return 0.0
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        intersection = words1 & words2
        union = words1 | words2
        
        if not union:
            return 0.0
        
        return len(intersection) / len(union)
    
    def _extract_tags_from_text(self, text: str) -> Set[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ ‡ç­¾"""
        tags = set()
        
        # MLç›¸å…³å…³é”®è¯
        ml_keywords = {
            'train', 'training', 'model', 'dataset', 'data', 'predict', 'prediction',
            'accuracy', 'loss', 'validation', 'test', 'feature', 'engineering',
            'preprocessing', 'visualization', 'analysis', 'evaluation', 'metrics'
        }
        
        # æ•°æ®ç§‘å­¦å·¥å…·
        tools = {
            'pandas', 'numpy', 'sklearn', 'tensorflow', 'pytorch', 'matplotlib',
            'seaborn', 'plotly', 'xgboost', 'lightgbm', 'catboost'
        }
        
        text_lower = text.lower()
        
        # æå–MLå…³é”®è¯
        for keyword in ml_keywords:
            if keyword in text_lower:
                tags.add(keyword)
        
        # æå–å·¥å…·å
        for tool in tools:
            if tool in text_lower:
                tags.add(tool)
        
        # æå–æ–‡ä»¶æ‰©å±•åç›¸å…³çš„æ ‡ç­¾
        file_patterns = re.findall(r'\.(\w+)', text)
        for ext in file_patterns:
            if ext in ['csv', 'json', 'pkl', 'h5', 'parquet']:
                tags.add(f'data_{ext}')
            elif ext in ['py', 'ipynb', 'R']:
                tags.add(f'code_{ext}')
        
        return tags
    
    def _calculate_conversation_importance(self, 
                                         user_input: str, 
                                         agent_response: str,
                                         context: Dict[str, Any] = None) -> float:
        """è®¡ç®—å¯¹è¯é‡è¦æ€§"""
        importance = 0.5
        
        # é•¿åº¦å› å­
        total_length = len(user_input) + len(agent_response)
        if total_length > 500:
            importance += 0.1
        
        # å…³é”®è¯å› å­
        important_keywords = {
            'error', 'fail', 'problem', 'issue', 'bug', 'fix',
            'important', 'critical', 'urgent', 'remember',
            'model', 'train', 'deploy', 'production'
        }
        
        text = (user_input + ' ' + agent_response).lower()
        keyword_count = sum(1 for keyword in important_keywords if keyword in text)
        importance += keyword_count * 0.05
        
        # ä¸Šä¸‹æ–‡å› å­
        if context:
            if context.get('error_occurred'):
                importance += 0.2
            if context.get('model_training'):
                importance += 0.15
        
        return min(1.0, importance)
    
    def _generate_relevance_explanation(self, 
                                      query: str, 
                                      node: MemoryNode, 
                                      similarity: float) -> str:
        """ç”Ÿæˆç›¸å…³æ€§è§£é‡Š"""
        explanations = []
        
        # ç›¸ä¼¼åº¦è§£é‡Š
        if similarity > 0.7:
            explanations.append("é«˜åº¦ç›¸å…³")
        elif similarity > 0.4:
            explanations.append("ä¸­ç­‰ç›¸å…³")
        else:
            explanations.append("ä½åº¦ç›¸å…³")
        
        # ç±»å‹åŒ¹é…
        explanations.append(f"è®°å¿†ç±»å‹: {node.memory_type}")
        
        # æ ‡ç­¾åŒ¹é…
        query_words = set(query.lower().split())
        matching_tags = node.tags.intersection(query_words)
        if matching_tags:
            explanations.append(f"åŒ¹é…æ ‡ç­¾: {', '.join(matching_tags)}")
        
        # é‡è¦æ€§
        if node.importance > 0.7:
            explanations.append("é«˜é‡è¦æ€§")
        
        return " | ".join(explanations)
    
    async def _rebuild_vector_index(self):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        if not self.memory_nodes:
            return
        
        print("ğŸ”„ é‡å»ºå‘é‡ç´¢å¼•...")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬
        texts = []
        self.document_ids = []
        
        for node_id, node in self.memory_nodes.items():
            texts.append(node.content)
            self.document_ids.append(node_id)
        
        # è®­ç»ƒå‘é‡åŒ–å™¨
        try:
            self.document_vectors = self.vectorizer.fit_transform(texts)
            print(f"âœ… å‘é‡ç´¢å¼•é‡å»ºå®Œæˆ: {len(texts)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âš ï¸ å‘é‡ç´¢å¼•é‡å»ºå¤±è´¥: {str(e)}")
    
    async def _save_memory_node(self, node: MemoryNode):
        """ä¿å­˜è®°å¿†èŠ‚ç‚¹åˆ°æ•°æ®åº“"""
        cursor = self.conn.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO memory_nodes 
            (id, content, memory_type, tags, metadata, timestamp, importance, 
             embedding, related_nodes, parent_nodes, child_nodes, access_count, last_accessed)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            node.id,
            node.content,
            node.memory_type,
            json.dumps(list(node.tags)),
            json.dumps(node.metadata, default=str),
            node.timestamp.isoformat(),
            node.importance,
            pickle.dumps(node.embedding) if node.embedding is not None else None,
            json.dumps(list(node.related_nodes)),
            json.dumps(list(node.parent_nodes)),
            json.dumps(list(node.child_nodes)),
            node.access_count,
            node.last_accessed.isoformat()
        ))
        
        self.conn.commit()
    
    async def _save_insight(self, insight: MLInsight):
        """ä¿å­˜æ´å¯Ÿåˆ°æ•°æ®åº“"""
        cursor = self.conn.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO ml_insights 
            (id, title, description, insight_type, confidence, supporting_evidence, applicable_contexts, discovered_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            insight.id,
            insight.title,
            insight.description,
            insight.insight_type,
            insight.confidence,
            json.dumps(insight.supporting_evidence),
            json.dumps(insight.applicable_contexts),
            insight.discovered_at.isoformat()
        ))
        
        self.conn.commit()
    
    async def _find_related_memories(self, node: MemoryNode) -> List[Tuple[str, float]]:
        """æ‰¾åˆ°ç›¸å…³è®°å¿†"""
        related = []
        
        if not node.embedding is None:
            for other_id, other_node in self.memory_nodes.items():
                if other_id != node.id and other_node.embedding is not None:
                    similarity = self._calculate_similarity(node.embedding, other_node.embedding)
                    related.append((other_id, similarity))
        
        related.sort(key=lambda x: x[1], reverse=True)
        return related
    
    def get_memory_statistics(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.memory_nodes:
            return {'total_memories': 0}
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_counts = {}
        for node in self.memory_nodes.values():
            type_counts[node.memory_type] = type_counts.get(node.memory_type, 0) + 1
        
        # æŒ‰é‡è¦æ€§ç»Ÿè®¡
        importance_levels = {'low': 0, 'medium': 0, 'high': 0}
        for node in self.memory_nodes.values():
            if node.importance < 0.3:
                importance_levels['low'] += 1
            elif node.importance < 0.7:
                importance_levels['medium'] += 1
            else:
                importance_levels['high'] += 1
        
        # è®¡ç®—å¹³å‡è®¿é—®æ¬¡æ•°
        avg_access = sum(node.access_count for node in self.memory_nodes.values()) / len(self.memory_nodes)
        
        return {
            'total_memories': len(self.memory_nodes),
            'total_insights': len(self.insights),
            'memory_types': type_counts,
            'importance_distribution': importance_levels,
            'knowledge_graph_nodes': len(self.knowledge_graph.nodes()),
            'knowledge_graph_edges': len(self.knowledge_graph.edges()),
            'data_lineage_nodes': len(self.data_lineage_graph.nodes()),
            'data_lineage_edges': len(self.data_lineage_graph.edges()),
            'average_access_count': avg_access
        }
    
    def export_memories(self, output_path: str = None) -> str:
        """å¯¼å‡ºè®°å¿†æ•°æ®"""
        if output_path is None:
            output_path = f"ml_memories_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        export_data = {
            'memories': {
                node_id: {
                    'content': node.content,
                    'memory_type': node.memory_type,
                    'tags': list(node.tags),
                    'metadata': node.metadata,
                    'timestamp': node.timestamp.isoformat(),
                    'importance': node.importance,
                    'access_count': node.access_count
                }
                for node_id, node in self.memory_nodes.items()
            },
            'insights': {
                insight_id: asdict(insight)
                for insight_id, insight in self.insights.items()
            },
            'statistics': self.get_memory_statistics(),
            'export_timestamp': datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“„ è®°å¿†æ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")
        return output_path

# æµ‹è¯•å‡½æ•°
async def test_intelligent_memory():
    """æµ‹è¯•æ™ºèƒ½è®°å¿†ç®¡ç†å™¨"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½è®°å¿†ç®¡ç†å™¨æµ‹è¯•...")
    
    # åˆ›å»ºè®°å¿†ç®¡ç†å™¨
    memory_manager = IntelligentMLMemoryManager(".")
    
    # æ·»åŠ ä¸€äº›æµ‹è¯•è®°å¿†
    await memory_manager.add_conversation_memory(
        user_input="æˆ‘æƒ³è®­ç»ƒä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡å‹",
        agent_response="å¥½çš„ï¼Œæˆ‘å¯ä»¥å¸®ä½ è®­ç»ƒå›¾åƒåˆ†ç±»æ¨¡å‹ã€‚é¦–å…ˆéœ€è¦å‡†å¤‡æ•°æ®é›†ï¼Œç„¶åé€‰æ‹©åˆé€‚çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚",
        context={'task_type': 'image_classification', 'user_level': 'beginner'}
    )
    
    await memory_manager.add_execution_memory(
        tool_name="preprocess_data",
        parameters={"dataset_path": "images/", "target_size": [224, 224]},
        result={"processed_images": 1000, "validation_split": 0.2},
        success=True,
        execution_time=45.2
    )
    
    await memory_manager.add_execution_memory(
        tool_name="train_model",
        parameters={"model_type": "CNN", "epochs": 50, "batch_size": 32},
        result={"final_accuracy": 0.89, "loss": 0.25},
        success=True,
        execution_time=1800.5
    )
    
    # æœç´¢æµ‹è¯•
    search_results = await memory_manager.smart_search("å›¾åƒåˆ†ç±»æ¨¡å‹è®­ç»ƒ", limit=5)
    print(f"ğŸ” æœç´¢ç»“æœ: {len(search_results)} ä¸ªç›¸å…³è®°å¿†")
    
    for result in search_results:
        print(f"  - ç›¸ä¼¼åº¦: {result.similarity:.3f} | {result.relevance_explanation}")
        print(f"    å†…å®¹: {result.node.content[:100]}...")
    
    # å‘ç°æ¨¡å¼
    patterns = await memory_manager.discover_patterns()
    print(f"ğŸ’¡ å‘ç° {len(patterns)} ä¸ªæ¨¡å¼")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = memory_manager.get_memory_statistics()
    print(f"ğŸ“Š è®°å¿†ç»Ÿè®¡: {stats}")
    
    # å¯¼å‡ºè®°å¿†
    export_path = memory_manager.export_memories()
    print(f"ğŸ“‹ è®°å¿†å¯¼å‡º: {export_path}")

if __name__ == "__main__":
    asyncio.run(test_intelligent_memory())