"""
Intelligent Prompt Generation System
æ™ºèƒ½æç¤ºç”Ÿæˆç³»ç»Ÿ
"""

import json
import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class PromptTemplate(Enum):
    """æç¤ºæ¨¡æ¿ç±»å‹"""
    DATA_ANALYSIS = "data_analysis"
    MODEL_TRAINING = "model_training"
    VISUALIZATION = "visualization"
    DEBUGGING = "debugging"
    OPTIMIZATION = "optimization"
    EXPLANATION = "explanation"

class ContextType(Enum):
    """ä¸Šä¸‹æ–‡ç±»å‹"""
    TECHNICAL = "technical"
    BUSINESS = "business"
    EDUCATIONAL = "educational"
    RESEARCH = "research"

@dataclass
class IntelligentPrompt:
    """æ™ºèƒ½æç¤ºæ•°æ®ç»“æ„"""
    template_type: PromptTemplate
    context_type: ContextType
    base_prompt: str
    dynamic_sections: Dict[str, str]
    examples: List[str]
    best_practices: List[str]
    common_pitfalls: List[str]
    
    def generate(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆå®Œæ•´çš„æ™ºèƒ½æç¤º"""
        sections = [
            self._generate_system_identity(),
            self._generate_context_section(context),
            self._generate_capability_section(),
            self._generate_memory_section(context),
            self._generate_task_analysis_section(user_input, context),
            self._generate_best_practices_section(),
            self._generate_response_guidelines(),
            self._generate_user_input_section(user_input)
        ]
        
        return "\n\n".join(filter(None, sections))
    
    def _generate_system_identity(self) -> str:
        """ç”Ÿæˆç³»ç»Ÿèº«ä»½éƒ¨åˆ†"""
        return """# AutoML Workflow Agent - å¢å¼ºå‹AIåŠ©æ‰‹

ä½ æ˜¯ä¸€ä¸ªå…·å¤‡é«˜çº§æœºå™¨å­¦ä¹ çŸ¥è¯†å’Œå®è·µç»éªŒçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºï¼š
- ğŸ§  æ™ºèƒ½åˆ†æç”¨æˆ·éœ€æ±‚å¹¶åˆ¶å®šæœ€ä¼˜è§£å†³æ–¹æ¡ˆ
- ğŸ”§ åœ¨å®‰å…¨çš„Dockerç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚çš„MLå·¥ä½œæµ
- ğŸ“Š æä¾›æ•°æ®é©±åŠ¨çš„æ´å¯Ÿå’Œå»ºè®®
- ğŸš€ ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡
- ğŸ“š ä¼ æˆæœ€ä½³å®è·µå’Œè¡Œä¸šæ ‡å‡†

## æ ¸å¿ƒèƒ½åŠ›
- **æ™ºèƒ½æ¨ç†**: åŸºäºä¸Šä¸‹æ–‡å’Œå†å²ç»éªŒè¿›è¡Œæ·±åº¦åˆ†æ
- **è®°å¿†ç³»ç»Ÿ**: å­¦ä¹ å’Œè®°ä½ç”¨æˆ·åå¥½ä¸é¡¹ç›®ç‰¹ç‚¹
- **ä»»åŠ¡è§„åˆ’**: è‡ªåŠ¨åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºå¯æ‰§è¡Œæ­¥éª¤
- **é”™è¯¯é¢„é˜²**: ä¸»åŠ¨è¯†åˆ«æ½œåœ¨é—®é¢˜å¹¶æä¾›è§£å†³æ–¹æ¡ˆ"""
    
    def _generate_context_section(self, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡éƒ¨åˆ†"""
        if not context:
            return ""
        
        sections = ["## å½“å‰ä¸Šä¸‹æ–‡"]
        
        if context.get("session_info"):
            sections.append(f"**ä¼šè¯ä¿¡æ¯**: {context['session_info']}")
        
        if context.get("data_info"):
            sections.append(f"**æ•°æ®ä¿¡æ¯**: {context['data_info']}")
        
        if context.get("previous_tasks"):
            sections.append("**å†å²ä»»åŠ¡**:")
            for task in context["previous_tasks"]:
                sections.append(f"- {task}")
        
        if context.get("user_preferences"):
            sections.append("**ç”¨æˆ·åå¥½**:")
            for pref, value in context["user_preferences"].items():
                sections.append(f"- {pref}: {value}")
        
        return "\n".join(sections)
    
    def _generate_capability_section(self) -> str:
        """ç”Ÿæˆèƒ½åŠ›è¯´æ˜éƒ¨åˆ†"""
        return f"""## å¯ç”¨å·¥å…·å’Œèƒ½åŠ›

### ğŸ”§ ä»£ç æ‰§è¡Œç¯å¢ƒ
- **å®‰å…¨æ²™ç®±**: Dockeréš”ç¦»ç¯å¢ƒï¼Œæ”¯æŒGPUåŠ é€Ÿ
- **MLåº“æ”¯æŒ**: scikit-learn, pandas, numpy, matplotlib, seaborn
- **æ·±åº¦å­¦ä¹ **: PyTorch, TensorFlow (CPU/GPU)
- **æ•°æ®å¤„ç†**: å¤§è§„æ¨¡æ•°æ®é›†å¤„ç†å’Œåˆ†æ

### ğŸ“Š æ•°æ®åˆ†æ
- **æ¢ç´¢æ€§åˆ†æ**: è‡ªåŠ¨ç”Ÿæˆæ•°æ®åˆ†å¸ƒã€ç›¸å…³æ€§åˆ†æ
- **è´¨é‡è¯„ä¼°**: ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€æ•°æ®åæ–œæ£€æµ‹
- **å¯è§†åŒ–**: äº¤äº’å¼å›¾è¡¨å’Œç»Ÿè®¡å›¾å½¢

### ğŸ¤– æœºå™¨å­¦ä¹ 
- **è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹**: ç‰¹å¾é€‰æ‹©ã€åˆ›å»ºã€è½¬æ¢
- **æ¨¡å‹é€‰æ‹©**: åŸºäºæ•°æ®ç‰¹ç‚¹æ¨èæœ€é€‚åˆçš„ç®—æ³•
- **è¶…å‚æ•°ä¼˜åŒ–**: è‡ªåŠ¨è°ƒå‚å’Œäº¤å‰éªŒè¯
- **æ¨¡å‹è¯„ä¼°**: å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡å’Œè§£é‡Šæ€§åˆ†æ

### ğŸ’¾ æ•°æ®ç®¡ç†
- **å†å²è®°å½•**: å®Œæ•´çš„å®éªŒè¿½è¸ªå’Œç‰ˆæœ¬æ§åˆ¶
- **ç»“æœä¿å­˜**: ç»“æ„åŒ–å­˜å‚¨MLæˆæœå’Œæ´å¯Ÿ
- **æ•°æ®è¡€ç¼˜**: è·Ÿè¸ªæ•°æ®å¤„ç†å’Œè½¬æ¢è¿‡ç¨‹"""
    
    def _generate_memory_section(self, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆè®°å¿†éƒ¨åˆ†"""
        if not context or not context.get("relevant_memories"):
            return ""
        
        sections = ["## ğŸ“š ç›¸å…³è®°å¿†å’Œç»éªŒ"]
        
        for memory in context["relevant_memories"]:
            importance_stars = "â­" * min(5, int(memory.get("importance", 0) * 5))
            sections.append(f"{importance_stars} {memory.get('content', '')}")
        
        return "\n".join(sections)
    
    def _generate_task_analysis_section(self, user_input: str, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆä»»åŠ¡åˆ†æéƒ¨åˆ†"""
        sections = ["## ğŸ¯ ä»»åŠ¡åˆ†ææ¡†æ¶"]
        
        # æ·»åŠ åŸºäºæ¨¡æ¿ç±»å‹çš„åˆ†ææ¡†æ¶
        if self.template_type == PromptTemplate.DATA_ANALYSIS:
            sections.extend([
                "### æ•°æ®åˆ†ææ£€æŸ¥æ¸…å•",
                "1. **æ•°æ®ç†è§£**: å½¢çŠ¶ã€ç±»å‹ã€åˆ†å¸ƒã€è´¨é‡",
                "2. **ä¸šåŠ¡ç†è§£**: ç›®æ ‡ã€çº¦æŸã€æˆåŠŸæŒ‡æ ‡",
                "3. **æ¢ç´¢ç­–ç•¥**: å•å˜é‡ã€åŒå˜é‡ã€å¤šå˜é‡åˆ†æ",
                "4. **æ´å¯Ÿæå–**: æ¨¡å¼ã€å¼‚å¸¸ã€å…³ç³»ã€è¶‹åŠ¿"
            ])
        elif self.template_type == PromptTemplate.MODEL_TRAINING:
            sections.extend([
                "### å»ºæ¨¡æµç¨‹æ¡†æ¶",
                "1. **é—®é¢˜å®šä¹‰**: ç›‘ç£/æ— ç›‘ç£ã€åˆ†ç±»/å›å½’/èšç±»",
                "2. **æ•°æ®å‡†å¤‡**: æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€åˆ’åˆ†",
                "3. **æ¨¡å‹é€‰æ‹©**: ç®—æ³•æ¯”è¾ƒã€å¤æ‚åº¦æƒè¡¡",
                "4. **è¯„ä¼°ä¼˜åŒ–**: æŒ‡æ ‡é€‰æ‹©ã€è°ƒå‚ã€éªŒè¯"
            ])
        elif self.template_type == PromptTemplate.VISUALIZATION:
            sections.extend([
                "### å¯è§†åŒ–è®¾è®¡åŸåˆ™",
                "1. **ç›®æ ‡æ˜ç¡®**: æ¢ç´¢æ€§ vs è§£é‡Šæ€§å¯è§†åŒ–",
                "2. **å›¾è¡¨é€‰æ‹©**: åŸºäºæ•°æ®ç±»å‹å’Œå…³ç³»",
                "3. **è®¾è®¡ç¾å­¦**: è‰²å½©ã€å¸ƒå±€ã€æ ‡æ³¨",
                "4. **äº¤äº’æ€§**: åŠ¨æ€ç­›é€‰ã€ç¼©æ”¾ã€è¯¦æƒ…"
            ])
        
        return "\n".join(sections)
    
    def _generate_best_practices_section(self) -> str:
        """ç”Ÿæˆæœ€ä½³å®è·µéƒ¨åˆ†"""
        general_practices = [
            "ğŸ” **æ•°æ®ä¼˜å…ˆ**: å§‹ç»ˆä»ç†è§£æ•°æ®å¼€å§‹",
            "ğŸ“ **æ¸è¿›å¼å¼€å‘**: ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–",
            "ğŸ”’ **å®‰å…¨ç¬¬ä¸€**: æ‰€æœ‰æ“ä½œåœ¨æ²™ç®±ç¯å¢ƒä¸­è¿›è¡Œ",
            "ğŸ“Š **å¯è§£é‡Šæ€§**: ç¡®ä¿ç»“æœå¯ä»¥å‘ä¸šåŠ¡æ–¹è§£é‡Š",
            "ğŸ§ª **å®éªŒè¿½è¸ª**: è®°å½•æ‰€æœ‰å°è¯•å’Œç»“æœ",
            "â™»ï¸ **ä»£ç å¤ç”¨**: å°†æœ‰æ•ˆæ–¹æ³•ä¿å­˜ä¸ºæ¨¡æ¿"
        ]
        
        template_specific = []
        if self.template_type == PromptTemplate.DATA_ANALYSIS:
            template_specific = [
                "ğŸ“‹ **å®Œæ•´æ€§æ£€æŸ¥**: éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§",
                "ğŸ¯ **ç›®æ ‡å¯¼å‘**: åˆ†æè¦ä¸ä¸šåŠ¡ç›®æ ‡å¯¹é½",
                "ğŸ“ˆ **ç»Ÿè®¡æ˜¾è‘—æ€§**: é¿å…å¶ç„¶æ¨¡å¼çš„è¿‡åº¦è§£è¯»"
            ]
        elif self.template_type == PromptTemplate.MODEL_TRAINING:
            template_specific = [
                "âš–ï¸ **åŸºçº¿å¯¹æ¯”**: æ€»æ˜¯å»ºç«‹ç®€å•åŸºçº¿æ¨¡å‹",
                "ğŸ”„ **äº¤å‰éªŒè¯**: ä½¿ç”¨åˆé€‚çš„éªŒè¯ç­–ç•¥",
                "ğŸš« **é¿å…è¿‡æ‹Ÿåˆ**: ç›‘æ§è®­ç»ƒå’ŒéªŒè¯æ€§èƒ½å·®å¼‚"
            ]
        
        all_practices = general_practices + template_specific
        return "## ğŸ’¡ æœ€ä½³å®è·µæŒ‡å—\n\n" + "\n".join(all_practices)
    
    def _generate_response_guidelines(self) -> str:
        """ç”Ÿæˆå“åº”æŒ‡å¯¼åŸåˆ™"""
        return """## ğŸ“ å“åº”æŒ‡å¯¼åŸåˆ™

### ğŸ¨ å“åº”ç»“æ„
1. **ç®€æ˜æ‘˜è¦**: é¦–å…ˆæä¾›1-2å¥è¯çš„æ ¸å¿ƒå›ç­”
2. **è¯¦ç»†åˆ†æ**: æ·±å…¥è§£é‡Šæ–¹æ³•ã€åŸç†å’Œè€ƒè™‘å› ç´ 
3. **å…·ä½“æ­¥éª¤**: æä¾›å¯æ‰§è¡Œçš„æ“ä½œæ­¥éª¤
4. **ä»£ç ç¤ºä¾‹**: ç»™å‡ºå®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç 
5. **æœŸæœ›ç»“æœ**: è¯´æ˜é¢„æœŸè¾“å‡ºå’Œå¦‚ä½•è§£é‡Š

### ğŸ¯ è´¨é‡æ ‡å‡†
- **å‡†ç¡®æ€§**: ç¡®ä¿æŠ€æœ¯å†…å®¹æ­£ç¡®æ— è¯¯
- **å®Œæ•´æ€§**: è¦†ç›–é—®é¢˜çš„æ‰€æœ‰é‡è¦æ–¹é¢
- **å®ç”¨æ€§**: æä¾›ç«‹å³å¯ç”¨çš„è§£å†³æ–¹æ¡ˆ
- **æ•™è‚²æ€§**: è§£é‡ŠèƒŒåçš„åŸç†å’Œæœ€ä½³å®è·µ
- **å®‰å…¨æ€§**: æ‰€æœ‰æ“ä½œç¬¦åˆå®‰å…¨è§„èŒƒ

### ğŸš€ å¢å€¼æœåŠ¡
- **ä¸»åŠ¨å»ºè®®**: æä¾›ç›¸å…³çš„æ”¹è¿›å»ºè®®
- **é£é™©æé†’**: æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- **èµ„æºæ¨è**: æ¨èç›¸å…³å·¥å…·ã€åº“æˆ–å­¦ä¹ èµ„æº
- **åç»­è§„åˆ’**: å»ºè®®ä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨æ–¹å‘"""
    
    def _generate_user_input_section(self, user_input: str) -> str:
        """ç”Ÿæˆç”¨æˆ·è¾“å…¥éƒ¨åˆ†"""
        return f"""## ğŸ’¬ ç”¨æˆ·è¯·æ±‚

{user_input}

---

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šä¸”å®ç”¨çš„å“åº”ã€‚è®°ä½è¦ï¼š
1. å……åˆ†åˆ©ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œç»éªŒ
2. è€ƒè™‘ä¸Šä¸‹æ–‡å’Œå†å²ä¿¡æ¯
3. æä¾›å…·ä½“å¯æ‰§è¡Œçš„è§£å†³æ–¹æ¡ˆ
4. ä¸»åŠ¨é¢„é˜²å¯èƒ½çš„é—®é¢˜
5. ç¡®ä¿å“åº”çš„æ•™è‚²ä»·å€¼å’Œå®ç”¨æ€§"""

class IntelligentPromptGenerator:
    """æ™ºèƒ½æç¤ºç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.templates = self._load_templates()
        self.context_enhancers = self._load_context_enhancers()
        
    def _load_templates(self) -> Dict[PromptTemplate, IntelligentPrompt]:
        """åŠ è½½æç¤ºæ¨¡æ¿"""
        templates = {}
        
        # æ•°æ®åˆ†ææ¨¡æ¿
        templates[PromptTemplate.DATA_ANALYSIS] = IntelligentPrompt(
            template_type=PromptTemplate.DATA_ANALYSIS,
            context_type=ContextType.TECHNICAL,
            base_prompt="æ•°æ®åˆ†æä¸“å®¶æç¤º",
            dynamic_sections={},
            examples=[
                "df.describe() è·å–æè¿°æ€§ç»Ÿè®¡",
                "df.info() æŸ¥çœ‹æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼",
                "sns.pairplot(df) åˆ›å»ºé…å¯¹å›¾"
            ],
            best_practices=[
                "å§‹ç»ˆä»æ•°æ®è´¨é‡è¯„ä¼°å¼€å§‹",
                "ä½¿ç”¨å¤šç§å¯è§†åŒ–æ–¹æ³•æ¢ç´¢æ•°æ®",
                "éªŒè¯å‡è®¾å’Œå‘ç°"
            ],
            common_pitfalls=[
                "å¿½ç•¥ç¼ºå¤±å€¼çš„å¤„ç†",
                "è¿‡åº¦è§£è¯»ç›¸å…³æ€§",
                "å¿½ç•¥æ•°æ®åˆ†å¸ƒçš„åæ–œ"
            ]
        )
        
        # æ¨¡å‹è®­ç»ƒæ¨¡æ¿
        templates[PromptTemplate.MODEL_TRAINING] = IntelligentPrompt(
            template_type=PromptTemplate.MODEL_TRAINING,
            context_type=ContextType.TECHNICAL,
            base_prompt="æœºå™¨å­¦ä¹ å»ºæ¨¡ä¸“å®¶æç¤º",
            dynamic_sections={},
            examples=[
                "from sklearn.model_selection import train_test_split",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.metrics import classification_report"
            ],
            best_practices=[
                "å»ºç«‹åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒ",
                "ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹",
                "ç›‘æ§è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ"
            ],
            common_pitfalls=[
                "æ•°æ®æ³„éœ²é—®é¢˜",
                "ä¸å¹³è¡¡æ•°æ®é›†å¤„ç†ä¸å½“",
                "è¶…å‚æ•°è°ƒä¼˜è¿‡åº¦"
            ]
        )
        
        # å¯è§†åŒ–æ¨¡æ¿
        templates[PromptTemplate.VISUALIZATION] = IntelligentPrompt(
            template_type=PromptTemplate.VISUALIZATION,
            context_type=ContextType.TECHNICAL,
            base_prompt="æ•°æ®å¯è§†åŒ–ä¸“å®¶æç¤º",
            dynamic_sections={},
            examples=[
                "import matplotlib.pyplot as plt",
                "import seaborn as sns",
                "plt.figure(figsize=(12, 8))"
            ],
            best_practices=[
                "é€‰æ‹©åˆé€‚çš„å›¾è¡¨ç±»å‹",
                "æ³¨æ„é¢œè‰²å’Œæ ‡æ³¨",
                "ç¡®ä¿å¯è¯»æ€§å’Œç¾è§‚æ€§"
            ],
            common_pitfalls=[
                "å›¾è¡¨è¿‡äºå¤æ‚",
                "é¢œè‰²é€‰æ‹©ä¸å½“",
                "ç¼ºå°‘å¿…è¦çš„æ ‡æ³¨"
            ]
        )
        
        return templates
    
    def _load_context_enhancers(self) -> Dict[str, Any]:
        """åŠ è½½ä¸Šä¸‹æ–‡å¢å¼ºå™¨"""
        return {
            "data_quality_checklist": [
                "æ£€æŸ¥æ•°æ®å½¢çŠ¶å’ŒåŸºæœ¬ä¿¡æ¯",
                "è¯†åˆ«ç¼ºå¤±å€¼æ¨¡å¼",
                "æ£€æµ‹å¼‚å¸¸å€¼å’Œç¦»ç¾¤ç‚¹",
                "éªŒè¯æ•°æ®ç±»å‹ä¸€è‡´æ€§",
                "è¯„ä¼°æ•°æ®åˆ†å¸ƒç‰¹å¾"
            ],
            "ml_workflow_steps": [
                "é—®é¢˜å®šä¹‰å’Œç›®æ ‡è®¾å®š",
                "æ•°æ®æ”¶é›†å’Œç†è§£",
                "æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹",
                "æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒ",
                "æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–",
                "æ¨¡å‹éƒ¨ç½²å’Œç›‘æ§"
            ],
            "common_algorithms": {
                "åˆ†ç±»": ["RandomForest", "XGBoost", "SVM", "LogisticRegression"],
                "å›å½’": ["LinearRegression", "RandomForestRegressor", "XGBoostRegressor"],
                "èšç±»": ["KMeans", "DBSCAN", "HierarchicalClustering"],
                "é™ç»´": ["PCA", "t-SNE", "UMAP"]
            }
        }
    
    def infer_template_type(self, user_input: str, context: Dict[str, Any] = None) -> PromptTemplate:
        """æ¨æ–­æç¤ºæ¨¡æ¿ç±»å‹"""
        user_input_lower = user_input.lower()
        
        # æ•°æ®åˆ†æå…³é”®è¯
        analysis_keywords = ["åˆ†æ", "æ¢ç´¢", "ç»Ÿè®¡", "åˆ†å¸ƒ", "ç›¸å…³æ€§", "describe", "info", "explore"]
        if any(keyword in user_input_lower for keyword in analysis_keywords):
            return PromptTemplate.DATA_ANALYSIS
        
        # æ¨¡å‹è®­ç»ƒå…³é”®è¯
        training_keywords = ["è®­ç»ƒ", "æ¨¡å‹", "ç®—æ³•", "é¢„æµ‹", "åˆ†ç±»", "å›å½’", "train", "model", "predict"]
        if any(keyword in user_input_lower for keyword in training_keywords):
            return PromptTemplate.MODEL_TRAINING
        
        # å¯è§†åŒ–å…³é”®è¯
        viz_keywords = ["å›¾", "å¯è§†åŒ–", "ç”»", "chart", "plot", "visualization", "graph"]
        if any(keyword in user_input_lower for keyword in viz_keywords):
            return PromptTemplate.VISUALIZATION
        
        # è°ƒè¯•å…³é”®è¯
        debug_keywords = ["é”™è¯¯", "é—®é¢˜", "è°ƒè¯•", "bug", "error", "debug", "fix"]
        if any(keyword in user_input_lower for keyword in debug_keywords):
            return PromptTemplate.DEBUGGING
        
        # ä¼˜åŒ–å…³é”®è¯
        opt_keywords = ["ä¼˜åŒ–", "æ”¹è¿›", "æå‡", "optimize", "improve", "enhance"]
        if any(keyword in user_input_lower for keyword in opt_keywords):
            return PromptTemplate.OPTIMIZATION
        
        # è§£é‡Šå…³é”®è¯
        explain_keywords = ["è§£é‡Š", "è¯´æ˜", "åŸç†", "explain", "how", "why", "what"]
        if any(keyword in user_input_lower for keyword in explain_keywords):
            return PromptTemplate.EXPLANATION
        
        # é»˜è®¤è¿”å›æ•°æ®åˆ†æ
        return PromptTemplate.DATA_ANALYSIS
    
    def generate_intelligent_prompt(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆæ™ºèƒ½æç¤º"""
        try:
            # æ¨æ–­æ¨¡æ¿ç±»å‹
            template_type = self.infer_template_type(user_input, context)
            
            # è·å–å¯¹åº”æ¨¡æ¿
            template = self.templates.get(template_type)
            if not template:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šæ¨¡æ¿ï¼Œä½¿ç”¨æ•°æ®åˆ†ææ¨¡æ¿ä½œä¸ºé»˜è®¤
                template = self.templates[PromptTemplate.DATA_ANALYSIS]
            
            # å¢å¼ºä¸Šä¸‹æ–‡
            enhanced_context = self._enhance_context(context, template_type, user_input)
            
            # ç”Ÿæˆæ™ºèƒ½æç¤º
            intelligent_prompt = template.generate(user_input, enhanced_context)
            
            logger.info(f"Generated intelligent prompt using template: {template_type.value}")
            return intelligent_prompt
            
        except Exception as e:
            logger.error(f"Error generating intelligent prompt: {str(e)}", exc_info=True)
            # è¿”å›åŸºç¡€æç¤ºä½œä¸ºå¤‡ä»½
            return self._generate_fallback_prompt(user_input, context)
    
    def _enhance_context(self, context: Dict[str, Any], template_type: PromptTemplate, user_input: str) -> Dict[str, Any]:
        """å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        enhanced = context.copy() if context else {}
        
        # æ·»åŠ æ¨¡æ¿ç‰¹å®šçš„ä¸Šä¸‹æ–‡å¢å¼º
        if template_type == PromptTemplate.DATA_ANALYSIS:
            enhanced["data_quality_checklist"] = self.context_enhancers["data_quality_checklist"]
        elif template_type == PromptTemplate.MODEL_TRAINING:
            enhanced["ml_workflow"] = self.context_enhancers["ml_workflow_steps"]
            enhanced["algorithm_suggestions"] = self.context_enhancers["common_algorithms"]
        
        # æ·»åŠ æ—¶é—´ä¸Šä¸‹æ–‡
        enhanced["current_time"] = datetime.datetime.now().isoformat()
        
        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ†æ
        enhanced["input_analysis"] = {
            "length": len(user_input),
            "complexity": "high" if len(user_input) > 100 else "medium" if len(user_input) > 50 else "low",
            "contains_code": "```" in user_input or "def " in user_input or "import " in user_input
        }
        
        return enhanced
    
    def _generate_fallback_prompt(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆå¤‡ç”¨æç¤º"""
        return f"""# AutoML Workflow Agent

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨å­¦ä¹ åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ç”¨æˆ·è¯·æ±‚æä¾›ä¸“ä¸šã€è¯¦ç»†çš„å¸®åŠ©ï¼š

## ç”¨æˆ·è¯·æ±‚
{user_input}

## å“åº”è¦æ±‚
1. æä¾›å‡†ç¡®ã€å®ç”¨çš„è§£å†³æ–¹æ¡ˆ
2. åŒ…å«å…·ä½“çš„ä»£ç ç¤ºä¾‹
3. è§£é‡Šå…³é”®æ¦‚å¿µå’Œæœ€ä½³å®è·µ
4. è€ƒè™‘å®‰å…¨æ€§å’Œæ•ˆç‡
5. ä¸»åŠ¨æä¾›ç›¸å…³å»ºè®®

è¯·ç°åœ¨å¼€å§‹å“åº”ç”¨æˆ·çš„è¯·æ±‚ã€‚"""

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    generator = IntelligentPromptGenerator()
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„ç”¨æˆ·è¾“å…¥
    test_inputs = [
        "å¸®æˆ‘åˆ†æè¿™ä¸ªæ•°æ®é›†çš„è´¨é‡",
        "è®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹æ¥é¢„æµ‹å®¢æˆ·æµå¤±",
        "åˆ›å»ºä¸€ä¸ªå¯è§†åŒ–å±•ç¤ºé”€å”®è¶‹åŠ¿",
        "æˆ‘çš„æ¨¡å‹å‡ºç°äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³ï¼Ÿ"
    ]
    
    for user_input in test_inputs:
        print(f"\n{'='*50}")
        print(f"ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"{'='*50}")
        
        prompt = generator.generate_intelligent_prompt(
            user_input=user_input,
            context={
                "session_info": "æµ‹è¯•ä¼šè¯",
                "relevant_memories": [
                    {"content": "ç”¨æˆ·åå¥½ä½¿ç”¨scikit-learn", "importance": 0.8},
                    {"content": "ä¹‹å‰æˆåŠŸå¤„ç†è¿‡å®¢æˆ·æ•°æ®", "importance": 0.6}
                ]
            }
        )
        
        print(prompt[:500] + "...")  # æ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦