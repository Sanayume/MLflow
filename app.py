# app.py

import streamlit as st
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import Tool
from langchain_core.tools import StructuredTool
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver
import os
import datetime
import json
import logging

# å¯¼å…¥å¢å¼ºé…ç½®ç®¡ç†å™¨
from enhanced_config import config_manager, render_provider_config_ui, create_llm_from_config

# å¯¼å…¥å¢å¼ºæ¨¡å—
from enhanced_agent import EnhancedAIAgent, MemoryType, TaskType
from enhanced_history import (
    EnhancedHistoryManager, HistoryHelpers, HistoryType, 
    EventSeverity, SearchFilters
)
from enhanced_ml_tools import EnhancedMLTools, MLTaskType, VisualizationType
from resource_monitor import SystemResourceMonitor, ResourceAlert, ResourceType
from local_tools import (
    safe_python_executor, 
    read_local_file, 
    write_local_file, 
    # list_directory_contents, # å¯ä»¥æ›¿æ¢æ‰æ—§çš„
    list_directory_items_with_paths, # æ–°çš„å·¥å…·
    make_web_request 
)
from sandbox_main import (
    execute_ml_code_in_docker,
    CONTAINER_CODE_EXECUTION_BASE_TARGET,
    CONTAINER_OUTPUTS_MOUNT_TARGET,
    CONTAINER_DATASETS_MOUNT_TARGET,
    HOST_EXECUTION_LOGS_ROOT,
    HOST_OUTPUTS_MOUNT_SOURCE,
    HOST_DATASETS_MOUNT_SOURCE,
    HOST_ROOT_WORKSPACE,
    DOCKER_IMAGE_NAME,
    SandboxExecutionInput
)
from langchain_community.tools.file_management import (
    CopyFileTool,
    DeleteFileTool,
    FileSearchTool,
    ListDirectoryTool,
    MoveFileTool,
    ReadFileTool,
    WriteFileTool,
)
from db_tools import (
    SaveMLResultInput,
    DatabaseQueryInput,
    QueryMLResultsInput,
    save_ml_result_to_db,
    query_execution_history,
    query_ml_results_from_db,
)
from config import (
        SYSTEM_PROMPT, 
        EXECUTE_CODE_TOOL_DESCRIPTION,
        SYSTEM_PROMPT_2,
        QUERY_EXEC_LOGS_TOOL_DESCRIPTION,
        QUERY_ML_RESULTS_TOOL_DESCRIPTION,
        SAVE_ML_RESULT_TOOL_DESCRIPTION
)

# è·å–åº”ç”¨è®¾ç½®
app_settings = config_manager.get_app_settings()

# é…ç½®é¡µé¢
st.set_page_config(
    page_title=app_settings.get("title", "AutoML Workflow Agent"),
    page_icon=app_settings.get("page_icon", "ğŸ¤–"),
    layout=app_settings.get("layout", "wide"),
    initial_sidebar_state=app_settings.get("sidebar_state", "expanded")
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è·å–å¯ç”¨çš„ä¾›åº”å•†
available_providers = config_manager.get_enabled_providers()
if not available_providers:
    st.error("âŒ æ²¡æœ‰é…ç½®å¯ç”¨çš„APIä¾›åº”å•†ï¼è¯·å…ˆé…ç½®è‡³å°‘ä¸€ä¸ªä¾›åº”å•†ã€‚")
    st.stop()

# é€‰æ‹©å½“å‰ä½¿ç”¨çš„ä¾›åº”å•†
if "current_provider_id" not in st.session_state:
    default_provider = config_manager.get_default_provider()
    st.session_state.current_provider_id = list(available_providers.keys())[0] if available_providers else None

current_provider = config_manager.get_provider(st.session_state.current_provider_id)
if not current_provider or not current_provider.enabled or not current_provider.api_key:
    st.error(f"âŒ å½“å‰é€‰æ‹©çš„ä¾›åº”å•† {st.session_state.current_provider_id} æœªæ­£ç¡®é…ç½®ï¼")
    st.stop()

# 1. åˆå§‹åŒ–LLM
try:
    llm = create_llm_from_config(current_provider)
    st.success(f"âœ… å·²è¿æ¥åˆ° {current_provider.name} - {current_provider.default_model}")
except Exception as e:
    st.error(f"âŒ åˆå§‹åŒ–LLMå¤±è´¥: {str(e)}")
    st.stop()

# 2. åˆå§‹åŒ–å¢å¼ºç»„ä»¶
@st.cache_resource
def initialize_enhanced_components():
    """åˆå§‹åŒ–å¢å¼ºç»„ä»¶ï¼ˆä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åˆå§‹åŒ–ï¼‰"""
    history_manager = EnhancedHistoryManager()
    ml_tools = EnhancedMLTools()
    resource_monitor = SystemResourceMonitor()
    return history_manager, ml_tools, resource_monitor

history_manager, ml_tools, resource_monitor = initialize_enhanced_components()

execute_in_ml_sandbox_tool = StructuredTool.from_function(
    func=execute_ml_code_in_docker, # æŒ‡å‘æˆ‘ä»¬å¥å£®çš„åç«¯å‡½æ•°
    name="ExecutePythonInMLSandbox", # å·¥å…·çš„è°ƒç”¨åç§°
    description=EXECUTE_CODE_TOOL_DESCRIPTION, # ä¸Šé¢å®šä¹‰çš„è¯¦ç»†æè¿°
    args_schema=SandboxExecutionInput # Pydanticæ¨¡å‹å®šä¹‰è¾“å…¥å‚æ•°
)
query_system_execution_logs_tool = StructuredTool.from_function(
    func=query_execution_history, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="QuerySystemExecutionLogs",
    description=QUERY_EXEC_LOGS_TOOL_DESCRIPTION,
    args_schema=DatabaseQueryInput # ä½¿ç”¨ DatabaseQueryInput
)
save_ml_result_tool = StructuredTool.from_function(
    func=save_ml_result_to_db, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="SaveMachineLearningResult",
    description=SAVE_ML_RESULT_TOOL_DESCRIPTION,
    args_schema=SaveMLResultInput # ä½¿ç”¨ SaveMLResultInput
)
query_ml_results_tool = StructuredTool.from_function(
    func=query_ml_results_from_db, # æŒ‡å‘ db_utils.py ä¸­çš„å‡½æ•°
    name="QueryMachineLearningResults",
    description=QUERY_ML_RESULTS_TOOL_DESCRIPTION,
    args_schema=QueryMLResultsInput # ä½¿ç”¨ QueryMLResultsInput
)

tools = [
    execute_in_ml_sandbox_tool,
    query_system_execution_logs_tool,
    save_ml_result_tool,
    query_ml_results_tool
]

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_PROMPT_2), # ä½¿ç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„è¯¦ç»†ç³»ç»Ÿæç¤º
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

# --- Agentå’ŒAgentExecutorçš„åˆ›å»º (ä¸æ‚¨ä¹‹å‰çš„ä»£ç ç±»ä¼¼) ---
agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,  # å¤„ç†è§£æé”™è¯¯
)

# ------------- Streamlit UIéƒ¨åˆ† -------------
# ==============================================================================

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "session_id" not in st.session_state:
    st.session_state.session_id = f"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
if "enhanced_agent" not in st.session_state:
    st.session_state.enhanced_agent = EnhancedAIAgent(llm, tools, current_provider.api_key)
if "resource_monitoring" not in st.session_state:
    st.session_state.resource_monitoring = False

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("ğŸ›ï¸ æ§åˆ¶é¢æ¿")
    
    # æ·»åŠ ä¾›åº”å•†é…ç½®é€‰é¡¹å¡
    tab1, tab2 = st.tabs(["ğŸ”§ ä¾›åº”å•†é…ç½®", "ğŸ“Š ä¼šè¯æ§åˆ¶"])
    
    with tab1:
        # å½“å‰ä¾›åº”å•†æ˜¾ç¤º
        st.subheader("ğŸ¤– å½“å‰ä¾›åº”å•†")
        provider_options = {k: f"{v.name}" for k, v in available_providers.items()}
        
        selected_provider_id = st.selectbox(
            "é€‰æ‹©APIä¾›åº”å•†",
            options=list(provider_options.keys()),
            format_func=lambda x: provider_options[x],
            index=list(provider_options.keys()).index(st.session_state.current_provider_id),
            key="provider_selector"
        )
        
        if selected_provider_id != st.session_state.current_provider_id:
            st.session_state.current_provider_id = selected_provider_id
            st.rerun()
        
        current_provider_display = config_manager.get_provider(st.session_state.current_provider_id)
        st.info(f"**æ¨¡å‹**: {current_provider_display.default_model}\n**ç±»å‹**: {current_provider_display.type.value}")
        
        # ä¾›åº”å•†é…ç½®ç•Œé¢
        render_provider_config_ui(config_manager)
    
    with tab2:
        # ä¼šè¯ä¿¡æ¯
        st.subheader("ğŸ“Š ä¼šè¯ä¿¡æ¯")
        st.info(f"**ä¼šè¯ID:** {st.session_state.session_id}")
        
        # AgentçŠ¶æ€
        agent_status = st.session_state.enhanced_agent.get_agent_status()
        st.json(agent_status)
    
    # å†å²è®°å½•æœç´¢
    st.subheader("ğŸ” å†å²è®°å½•æœç´¢")
    search_text = st.text_input("æœç´¢å…³é”®è¯")
    history_type_filter = st.selectbox(
        "è®°å½•ç±»å‹",
        ["å…¨éƒ¨"] + [ht.value for ht in HistoryType],
        index=0
    )
    
    # æ—¶é—´èŒƒå›´
    time_range = st.selectbox(
        "æ—¶é—´èŒƒå›´",
        ["æœ€è¿‘1å°æ—¶", "æœ€è¿‘24å°æ—¶", "æœ€è¿‘7å¤©", "æœ€è¿‘30å¤©", "å…¨éƒ¨"],
        index=1
    )
    
    if st.button("ğŸ” æœç´¢å†å²"):
        # æ„å»ºæœç´¢è¿‡æ»¤å™¨
        filters = SearchFilters()
        if search_text:
            filters.search_text = search_text
        if history_type_filter != "å…¨éƒ¨":
            filters.history_types = [HistoryType(history_type_filter)]
        
        # è®¾ç½®æ—¶é—´èŒƒå›´
        if time_range != "å…¨éƒ¨":
            hours_map = {
                "æœ€è¿‘1å°æ—¶": 1,
                "æœ€è¿‘24å°æ—¶": 24,
                "æœ€è¿‘7å¤©": 24 * 7,
                "æœ€è¿‘30å¤©": 24 * 30
            }
            hours = hours_map[time_range]
            filters.end_time = datetime.datetime.now()
            filters.start_time = filters.end_time - datetime.timedelta(hours=hours)
        
        # æ‰§è¡Œæœç´¢
        entries, total_count = history_manager.search_entries(filters, limit=20)
        
        st.write(f"æ‰¾åˆ° {total_count} æ¡è®°å½•")
        for entry in entries:
            with st.expander(f"{entry.title} - {entry.timestamp.strftime('%Y-%m-%d %H:%M')}"):
                st.write(f"**ç±»å‹:** {entry.history_type.value}")
                st.write(f"**ä¸¥é‡ç¨‹åº¦:** {entry.severity.value}")
                st.write(f"**æè¿°:** {entry.description}")
                if entry.tags:
                    st.write(f"**æ ‡ç­¾:** {', '.join(entry.tags)}")
    
    # å¯¼å‡ºåŠŸèƒ½
    st.subheader("ğŸ“¤ å¯¼å‡ºæ•°æ®")
    if st.button("å¯¼å‡ºä¼šè¯æ•°æ®"):
        session_data = st.session_state.enhanced_agent.export_session_data()
        st.download_button(
            label="ä¸‹è½½JSON",
            data=json.dumps(session_data, ensure_ascii=False, indent=2),
            file_name=f"session_{st.session_state.session_id}.json",
            mime="application/json"
        )
    
    # MLå·¥å…·æ§åˆ¶
    st.subheader("ğŸ§  å¢å¼ºMLå·¥å…·")
    auto_analysis = st.checkbox("è‡ªåŠ¨æ•°æ®åˆ†æ", value=True)
    smart_preprocessing = st.checkbox("æ™ºèƒ½é¢„å¤„ç†", value=True)
    auto_model_selection = st.checkbox("è‡ªåŠ¨æ¨¡å‹é€‰æ‹©", value=False)
    advanced_viz = st.checkbox("é«˜çº§å¯è§†åŒ–", value=True)
    
    # èµ„æºç›‘æ§æ§åˆ¶
    st.subheader("ğŸ“Š èµ„æºç›‘æ§")
    if st.button("ğŸ” æŸ¥çœ‹å½“å‰èµ„æºçŠ¶æ€"):
        current_usage = resource_monitor.get_current_usage()
        if current_usage:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("CPUä½¿ç”¨ç‡", f"{current_usage.cpu_percent:.1f}%")
            with col2:
                st.metric("å†…å­˜ä½¿ç”¨ç‡", f"{current_usage.memory_percent:.1f}%")
            with col3:
                st.metric("ç£ç›˜ä½¿ç”¨ç‡", f"{current_usage.disk_percent:.1f}%")
    
    # å¯åŠ¨/åœæ­¢ç›‘æ§
    if st.button("â–¶ï¸ å¯åŠ¨èµ„æºç›‘æ§" if not st.session_state.resource_monitoring else "â¹ï¸ åœæ­¢èµ„æºç›‘æ§"):
        if not st.session_state.resource_monitoring:
            resource_monitor.start_monitoring()
            st.session_state.resource_monitoring = True
            st.success("èµ„æºç›‘æ§å·²å¯åŠ¨")
        else:
            resource_monitor.stop_monitoring()
            st.session_state.resource_monitoring = False
            st.success("èµ„æºç›‘æ§å·²åœæ­¢")
    
    # æ˜¾ç¤ºæœ€è¿‘å‘Šè­¦
    recent_alerts = resource_monitor.get_alerts(resolved=False, hours=1)
    if recent_alerts:
        st.warning(f"âš ï¸ å‘ç° {len(recent_alerts)} ä¸ªæœªè§£å†³çš„èµ„æºå‘Šè­¦")
        for alert in recent_alerts[:3]:  # æ˜¾ç¤ºæœ€è¿‘3ä¸ª
            st.error(f"{alert['resource_type'].upper()}: {alert['message']}")
    
    # æ¸…ç†åŠŸèƒ½
    st.subheader("ğŸ§¹ ç»´æŠ¤")
    if st.button("æ¸…ç†æ—§è®°å½•"):
        cleaned_count = history_manager.cleanup_old_entries(days=30)
        st.success(f"æ¸…ç†äº† {cleaned_count} æ¡æ—§è®°å½•")
    
    if st.button("æ¸…ç†èµ„æºç›‘æ§æ•°æ®"):
        resource_monitor.cleanup_old_data(days=7)
        st.success("æ¸…ç†äº†7å¤©å‰çš„èµ„æºç›‘æ§æ•°æ®")

# ä¸»ç•Œé¢
app_title = app_settings.get("title", "ğŸ¤– Enhanced AutoML Workflow Agent")
app_description = app_settings.get("description", "å…·å¤‡æ™ºèƒ½è®°å¿†ã€ä»»åŠ¡è§„åˆ’å’Œå¢å¼ºæ¨ç†èƒ½åŠ›çš„AIåŠ©æ‰‹")

st.title(app_title)
st.markdown(f"### {app_description}")
st.markdown(f"**å½“å‰ä½¿ç”¨**: {current_provider.name} - {current_provider.default_model}")

# æ˜¾ç¤ºAgentçŠ¶æ€æ¦‚è¦
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("è®°å¿†æ¡ç›®", agent_status["memory_summary"]["total_memories"])
with col2:
    st.metric("æ€»ä»»åŠ¡", agent_status["task_summary"]["total_tasks"])
with col3:
    st.metric("å¹³å‡å®Œæˆåº¦", f"{agent_status['task_summary']['avg_completion']:.1f}%")
with col4:
    st.metric("ä¸Šä¸‹æ–‡çª—å£", agent_status["context_window_size"])

# æ˜¾ç¤ºç³»ç»Ÿèµ„æºçŠ¶æ€
if st.session_state.resource_monitoring:
    st.subheader("ğŸ“Š ç³»ç»Ÿèµ„æºçŠ¶æ€")
    current_usage = resource_monitor.get_current_usage()
    if current_usage:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            cpu_color = "red" if current_usage.cpu_percent > 80 else "orange" if current_usage.cpu_percent > 60 else "green"
            st.metric("CPUä½¿ç”¨ç‡", f"{current_usage.cpu_percent:.1f}%", 
                     delta=None, delta_color=cpu_color)
        with col2:
            mem_color = "red" if current_usage.memory_percent > 80 else "orange" if current_usage.memory_percent > 60 else "green"
            st.metric("å†…å­˜ä½¿ç”¨ç‡", f"{current_usage.memory_percent:.1f}%", 
                     delta=f"{current_usage.memory_used_gb:.1f}GB", delta_color=mem_color)
        with col3:
            disk_color = "red" if current_usage.disk_percent > 80 else "orange" if current_usage.disk_percent > 60 else "green"
            st.metric("ç£ç›˜ä½¿ç”¨ç‡", f"{current_usage.disk_percent:.1f}%", 
                     delta=f"{current_usage.disk_used_gb:.1f}GB", delta_color=disk_color)
        with col4:
            net_mb = (current_usage.network_io_bytes.get('bytes_recv', 0) + 
                     current_usage.network_io_bytes.get('bytes_sent', 0)) / (1024*1024)
            st.metric("ç½‘ç»œIO", f"{net_mb:.1f}MB", delta="æ€»è®¡")
        
        # æ˜¾ç¤ºGPUä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if current_usage.gpu_usage and current_usage.gpu_usage.get('gpus'):
            st.subheader("ğŸ® GPUçŠ¶æ€")
            for gpu in current_usage.gpu_usage['gpus']:
                st.write(f"**{gpu['name']}**: {gpu['gpu_utilization']}% ä½¿ç”¨ç‡, "
                        f"{gpu['memory_percent']:.1f}% æ˜¾å­˜, {gpu['temperature_c']}Â°C")

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.chat_history:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage) and message.content:
        with st.chat_message("assistant"):
            st.markdown(message.content)

# è·å–ç”¨æˆ·è¾“å…¥
initial_user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„æŒ‡ä»¤...")

if initial_user_input:
    # ç«‹å³æ˜¾ç¤ºå¹¶è®°å½•ç”¨æˆ·çš„åˆæ¬¡è¾“å…¥
    with st.chat_message("user"):
        st.markdown(initial_user_input)
    st.session_state.chat_history.append(HumanMessage(content=initial_user_input))

    # ä½¿ç”¨å¢å¼ºçš„Agentå¤„ç†ç”¨æˆ·è¾“å…¥
    with st.spinner("ğŸ§  æ™ºèƒ½åˆ†æä¸­..."):
        try:
            # æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
            enhanced_context = {
                "chat_history_length": len(st.session_state.chat_history),
                "session_id": st.session_state.session_id,
                "ml_tools_available": {
                    "auto_analysis": auto_analysis,
                    "smart_preprocessing": smart_preprocessing,
                    "auto_model_selection": auto_model_selection,
                    "advanced_viz": advanced_viz
                },
                "resource_monitoring": st.session_state.resource_monitoring
            }
            
            # ä½¿ç”¨å¢å¼ºçš„Agentå¤„ç†è¾“å…¥
            enhanced_response = st.session_state.enhanced_agent.process_user_input(
                user_input=initial_user_input,
                context=enhanced_context
            )
            
            # è®°å½•å¯¹è¯åˆ°å†å²
            conversation_entry = HistoryHelpers.create_conversation_entry(
                session_id=st.session_state.session_id,
                user_input=initial_user_input,
                agent_response=enhanced_response["response"]["content"]
            )
            history_manager.add_entry(conversation_entry)
            
            # æ˜¾ç¤ºå¢å¼ºçš„å“åº”
            with st.chat_message("assistant"):
                st.markdown(enhanced_response["response"]["content"])
                
                # æ˜¾ç¤ºæ„å›¾åˆ†æï¼ˆå¯æŠ˜å ï¼‰
                if enhanced_response.get("intent_analysis"):
                    with st.expander("ğŸ¯ æ„å›¾åˆ†æ"):
                        st.json(enhanced_response["intent_analysis"])
                
                # æ˜¾ç¤ºä»»åŠ¡è®¡åˆ’ï¼ˆå¦‚æœæœ‰ï¼‰
                if enhanced_response.get("task_plan"):
                    with st.expander("ğŸ“‹ ä»»åŠ¡è®¡åˆ’"):
                        task_plan = enhanced_response["task_plan"]
                        st.write(f"**ä»»åŠ¡ID:** {task_plan['task_id']}")
                        st.write(f"**ç±»å‹:** {task_plan['task_type']}")
                        st.write(f"**ä¼˜å…ˆçº§:** {task_plan['priority']}")
                        st.write(f"**é¢„ä¼°æ—¶é•¿:** {task_plan['estimated_duration']}åˆ†é’Ÿ")
                        
                        if task_plan['subtasks']:
                            st.write("**å­ä»»åŠ¡:**")
                            for subtask in task_plan['subtasks']:
                                st.write(f"- {subtask['description']}")
                
                # æ˜¾ç¤ºç›¸å…³è®°å¿†ï¼ˆå¦‚æœæœ‰ï¼‰
                if enhanced_response.get("relevant_memories"):
                    with st.expander("ğŸ§  ç›¸å…³è®°å¿†"):
                        for memory in enhanced_response["relevant_memories"]:
                            st.write(f"**{memory['timestamp'][:19]}:** {memory['content']}")
                            if memory['tags']:
                                st.caption(f"æ ‡ç­¾: {', '.join(memory['tags'])}")
            
            # å°†å¢å¼ºå“åº”æ·»åŠ åˆ°å†å²
            st.session_state.chat_history.append(AIMessage(content=enhanced_response["response"]["content"]))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦MLå·¥å…·å¢å¼ºå¤„ç†
            should_use_ml_tools = any(keyword in initial_user_input.lower() for keyword in 
                ["åˆ†æ", "æ•°æ®", "æ¨¡å‹", "è®­ç»ƒ", "é¢„æµ‹", "å¯è§†åŒ–", "ç‰¹å¾"])
            
            if should_use_ml_tools:
                with st.spinner("ğŸ§  ä½¿ç”¨å¢å¼ºMLå·¥å…·å¤„ç†..."):
                    try:
                        # è¿™é‡Œå¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥è°ƒç”¨ç›¸åº”çš„MLå·¥å…·
                        ml_context = {
                            "auto_analysis_enabled": auto_analysis,
                            "smart_preprocessing_enabled": smart_preprocessing,
                            "auto_model_selection_enabled": auto_model_selection,
                            "advanced_viz_enabled": advanced_viz
                        }
                        
                        # æ˜¾ç¤ºMLå·¥å…·å»ºè®®
                        with st.chat_message("assistant"):
                            st.markdown("### ğŸ§  MLå·¥å…·å»ºè®®")
                            if auto_analysis and "åˆ†æ" in initial_user_input:
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨è‡ªåŠ¨æ•°æ®åˆ†æå·¥å…·æ¥å¿«é€Ÿäº†è§£æ•°æ®ç‰¹å¾")
                            if smart_preprocessing and ("é¢„å¤„ç†" in initial_user_input or "æ¸…æ´—" in initial_user_input):
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨æ™ºèƒ½é¢„å¤„ç†å·¥å…·æ¥è‡ªåŠ¨å¤„ç†æ•°æ®è´¨é‡é—®é¢˜")
                            if auto_model_selection and ("æ¨¡å‹" in initial_user_input or "è®­ç»ƒ" in initial_user_input):
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨è‡ªåŠ¨æ¨¡å‹é€‰æ‹©å·¥å…·æ¥æ‰¾åˆ°æœ€é€‚åˆçš„ç®—æ³•")
                            if advanced_viz and "å¯è§†åŒ–" in initial_user_input:
                                st.info("ğŸ’¡ å»ºè®®ä½¿ç”¨é«˜çº§å¯è§†åŒ–å·¥å…·æ¥åˆ›å»ºäº¤äº’å¼å›¾è¡¨")
                        
                    except Exception as ml_e:
                        logger.warning(f"ML tools enhancement failed: {str(ml_e)}")
            
            # å¦‚æœéœ€è¦æ‰§è¡Œä»£ç ï¼Œè°ƒç”¨åŸæœ‰çš„agent_executor
            if "ä»£ç " in initial_user_input or "æ‰§è¡Œ" in initial_user_input or "è¿è¡Œ" in initial_user_input:
                with st.spinner("ğŸ”§ æ‰§è¡Œä»£ç ä¸­..."):
                    try:
                        # è°ƒç”¨åŸæœ‰çš„agent_executoræ‰§è¡Œå…·ä½“ä»»åŠ¡
                        execution_response = agent_executor.invoke({
                            "input": initial_user_input,
                            "chat_history": st.session_state.chat_history[:-2]
                        })
                        
                        execution_content = execution_response["output"]
                        
                        # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
                        with st.chat_message("assistant"):
                            st.markdown("### ğŸ”§ æ‰§è¡Œç»“æœ")
                            st.markdown(execution_content)
                        
                        # è®°å½•æ‰§è¡Œç»“æœ
                        st.session_state.chat_history.append(AIMessage(content=f"æ‰§è¡Œç»“æœ: {execution_content}"))
                        
                        # è®°å½•æ‰§è¡Œåˆ°å†å²
                        execution_entry = HistoryHelpers.create_execution_entry(
                            session_id=st.session_state.session_id,
                            code=initial_user_input,
                            result=execution_content,
                            execution_time=2.0,  # ç®€åŒ–å¤„ç†
                            success=True
                        )
                        history_manager.add_entry(execution_entry)
                        
                    except Exception as exec_e:
                        error_message = f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(exec_e)}"
                        with st.chat_message("assistant"):
                            st.error(error_message)
                        
                        # è®°å½•é”™è¯¯åˆ°å†å²
                        error_entry = HistoryHelpers.create_error_entry(
                            session_id=st.session_state.session_id,
                            error_message=str(exec_e),
                            error_type="execution_error",
                            context={"user_input": initial_user_input}
                        )
                        history_manager.add_entry(error_entry)

        except Exception as e:
            # å¦‚æœinvokeåœ¨ä»»ä½•æ­¥éª¤å¤±è´¥äº†ï¼ˆåŒ…æ‹¬æˆ‘ä»¬å…³å¿ƒçš„ValidationErrorï¼‰
            print(f"--- [Agent Error Caught] æ•è·åˆ°å¼‚å¸¸: {e} ---")
            
            # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ï¼Œå‡†å¤‡ä½œä¸ºæ–°çš„è¾“å…¥åé¦ˆç»™Agent
            error_feedback_prompt = f"""
            æˆ‘åœ¨å°è¯•æ‰§è¡Œä½ ä¸Šä¸€æ­¥çš„è®¡åˆ’æ—¶é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ã€‚è¯·åˆ†æä¸‹é¢çš„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶ä¿®æ­£ä½ ä¹‹å‰çš„å·¥å…·è°ƒç”¨æˆ–è®¡åˆ’ã€‚

            **é”™è¯¯ä¿¡æ¯:**
            ```
            {str(e)}
            ```

            è¯·æ ¹æ®è¿™ä¸ªé”™è¯¯ï¼Œé‡æ–°ç”Ÿæˆä¸€ä¸ªæ­£ç¡®çš„å·¥å…·è°ƒç”¨ã€‚
            """
            
            # å°†è¿™ä¸ªé”™è¯¯åé¦ˆä½œä¸ºæ–°çš„è¾“å…¥ï¼Œå†æ¬¡è°ƒç”¨Agent
            with st.spinner("æ£€æµ‹åˆ°é”™è¯¯ï¼Œå°è¯•è‡ªæˆ‘ä¿®æ­£..."):
                try:
                    # æˆ‘ä»¬å°†é”™è¯¯åé¦ˆä¹Ÿä½œä¸ºHumanMessageæ·»åŠ åˆ°å†å²ä¸­ï¼Œè®©ä¸Šä¸‹æ–‡æ›´æ¸…æ™°
                    st.session_state.chat_history.append(HumanMessage(content=error_feedback_prompt))

                    # å†æ¬¡è°ƒç”¨ï¼Œä½†è¿™æ¬¡çš„inputæ˜¯æˆ‘ä»¬çš„é”™è¯¯åé¦ˆ
                    # å†å²è®°å½•ç°åœ¨åŒ…å«äº†åŸå§‹è¾“å…¥å’Œæˆ‘ä»¬çš„é”™è¯¯åé¦ˆ
                    corrected_response = agent_executor.invoke({
                        "input": error_feedback_prompt, 
                        "chat_history": st.session_state.chat_history[:-1] # ä¼ é€’åŒ…å«åŸå§‹è¾“å…¥ä½†ä¸åŒ…å«æˆ‘ä»¬åˆšå‘çš„é”™è¯¯åé¦ˆçš„å†å²
                    })

                    ai_response_content = corrected_response["output"]

                    # æ˜¾ç¤ºä¿®æ­£åçš„æœ€ç»ˆå›å¤
                    with st.chat_message("assistant"):
                        st.markdown(ai_response_content)
                    
                    # å°†æœ€ç»ˆçš„æˆåŠŸå›å¤æ·»åŠ åˆ°å†å²
                    st.session_state.chat_history.append(AIMessage(content=ai_response_content))

                except Exception as final_e:
                    # å¦‚æœåœ¨ä¿®æ­£åå†æ¬¡è°ƒç”¨ä»ç„¶å¤±è´¥
                    final_error_message = f"æŠ±æ­‰ï¼Œåœ¨å°è¯•è‡ªæˆ‘ä¿®æ­£åï¼Œæˆ‘ä»ç„¶é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼š\n\n```\n{final_e}\n```"
                    with st.chat_message("assistant"):
                        st.error(final_error_message)
                    st.session_state.chat_history.append(AIMessage(content=final_error_message))