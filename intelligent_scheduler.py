# intelligent_scheduler.py
"""
æ™ºèƒ½MLå·¥å…·è°ƒåº¦ç³»ç»Ÿ
åŸºäºgemini-cliçš„TypeScriptå·¥å…·è°ƒåº¦æ¶æ„
æä¾›æ™ºèƒ½ä¾èµ–åˆ†æã€èµ„æºç®¡ç†å’Œæ‰§è¡Œä¼˜åŒ–
"""

import asyncio
import time
from enum import Enum
from dataclasses import dataclass, field
from typing import Dict, List, Set, Any, Optional, Callable, Union
import networkx as nx
from datetime import datetime, timedelta
import json
import uuid
import threading
import queue
import psutil
from pathlib import Path

class ToolStatus(Enum):
    """å·¥å…·æ‰§è¡ŒçŠ¶æ€"""
    PENDING = "pending"
    VALIDATING = "validating"
    SCHEDULED = "scheduled"
    WAITING_APPROVAL = "waiting_approval"
    EXECUTING = "executing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    TIMEOUT = "timeout"

class ResourceType(Enum):
    """èµ„æºç±»å‹"""
    CPU = "cpu"
    MEMORY = "memory"
    GPU = "gpu"
    DISK = "disk"
    NETWORK = "network"

@dataclass
class ResourceRequirement:
    """èµ„æºéœ€æ±‚å®šä¹‰"""
    type: ResourceType
    amount: float
    unit: str  # cores, GB, %
    exclusive: bool = False  # æ˜¯å¦ç‹¬å 
    
@dataclass
class MLToolCall:
    """MLå·¥å…·è°ƒç”¨å®šä¹‰"""
    id: str
    tool_name: str
    parameters: Dict[str, Any]
    status: ToolStatus = ToolStatus.PENDING
    
    # ä¾èµ–å…³ç³»
    explicit_dependencies: Set[str] = field(default_factory=set)
    implicit_dependencies: Set[str] = field(default_factory=set)
    
    # æ‰§è¡Œå±æ€§
    priority: int = 0  # ä¼˜å…ˆçº§ (è¶Šé«˜è¶Šä¼˜å…ˆ)
    estimated_duration: float = 60.0  # é¢„ä¼°æ‰§è¡Œæ—¶é—´(ç§’)
    timeout: float = 3600.0  # è¶…æ—¶æ—¶é—´(ç§’)
    
    # èµ„æºéœ€æ±‚
    resource_requirements: List[ResourceRequirement] = field(default_factory=list)
    
    # æ‰§è¡Œä¿¡æ¯
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    actual_duration: Optional[float] = None
    
    # ç»“æœå’Œé”™è¯¯
    result: Optional[Any] = None
    error: Optional[str] = None
    
    # æ‰¹å‡†ç›¸å…³
    requires_approval: bool = False
    approval_message: Optional[str] = None
    approved: bool = False
    
    # å›è°ƒå‡½æ•°
    progress_callback: Optional[Callable] = None
    completion_callback: Optional[Callable] = None

@dataclass
class ResourcePool:
    """èµ„æºæ± ç®¡ç†"""
    cpu_cores: int
    memory_gb: float
    gpu_count: int
    disk_gb: float
    
    # å½“å‰ä½¿ç”¨æƒ…å†µ
    cpu_used: float = 0.0
    memory_used: float = 0.0
    gpu_used: int = 0
    disk_used: float = 0.0
    
    def get_available_resources(self) -> Dict[ResourceType, float]:
        """è·å–å¯ç”¨èµ„æº"""
        return {
            ResourceType.CPU: self.cpu_cores - self.cpu_used,
            ResourceType.MEMORY: self.memory_gb - self.memory_used,
            ResourceType.GPU: self.gpu_count - self.gpu_used,
            ResourceType.DISK: self.disk_gb - self.disk_used
        }
    
    def can_allocate(self, requirements: List[ResourceRequirement]) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†é…èµ„æº"""
        available = self.get_available_resources()
        
        for req in requirements:
            if available[req.type] < req.amount:
                return False
        return True
    
    def allocate(self, requirements: List[ResourceRequirement]) -> bool:
        """åˆ†é…èµ„æº"""
        if not self.can_allocate(requirements):
            return False
        
        for req in requirements:
            if req.type == ResourceType.CPU:
                self.cpu_used += req.amount
            elif req.type == ResourceType.MEMORY:
                self.memory_used += req.amount
            elif req.type == ResourceType.GPU:
                self.gpu_used += int(req.amount)
            elif req.type == ResourceType.DISK:
                self.disk_used += req.amount
        
        return True
    
    def release(self, requirements: List[ResourceRequirement]):
        """é‡Šæ”¾èµ„æº"""
        for req in requirements:
            if req.type == ResourceType.CPU:
                self.cpu_used = max(0, self.cpu_used - req.amount)
            elif req.type == ResourceType.MEMORY:
                self.memory_used = max(0, self.memory_used - req.amount)
            elif req.type == ResourceType.GPU:
                self.gpu_used = max(0, self.gpu_used - int(req.amount))
            elif req.type == ResourceType.DISK:
                self.disk_used = max(0, self.disk_used - req.amount)

class IntelligentMLToolScheduler:
    """æ™ºèƒ½MLå·¥å…·è°ƒåº¦å™¨"""
    
    def __init__(self, max_concurrent_tools: int = 4):
        self.max_concurrent_tools = max_concurrent_tools
        
        # å·¥å…·ç®¡ç†
        self.tools: Dict[str, MLToolCall] = {}
        self.dependency_graph = nx.DiGraph()
        self.execution_queue = queue.PriorityQueue()
        
        # èµ„æºç®¡ç†
        self.resource_pool = self._initialize_resource_pool()
        
        # æ‰§è¡Œç®¡ç†
        self.active_executions: Dict[str, asyncio.Task] = {}
        self.completed_tools: List[str] = []
        self.failed_tools: List[str] = []
        
        # äº‹ä»¶å›è°ƒ
        self.event_callbacks: Dict[str, List[Callable]] = {
            'tool_scheduled': [],
            'tool_started': [],
            'tool_completed': [],
            'tool_failed': [],
            'batch_completed': []
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_scheduled': 0,
            'total_completed': 0,
            'total_failed': 0,
            'avg_execution_time': 0.0,
            'resource_utilization': {}
        }
        
    def _initialize_resource_pool(self) -> ResourcePool:
        """åˆå§‹åŒ–èµ„æºæ± """
        return ResourcePool(
            cpu_cores=psutil.cpu_count(),
            memory_gb=psutil.virtual_memory().total / (1024**3),
            gpu_count=0,  # éœ€è¦GPUæ£€æµ‹
            disk_gb=psutil.disk_usage('/').free / (1024**3)
        )
    
    def register_event_callback(self, event: str, callback: Callable):
        """æ³¨å†Œäº‹ä»¶å›è°ƒ"""
        if event in self.event_callbacks:
            self.event_callbacks[event].append(callback)
    
    async def schedule_tools(self, tool_calls: List[MLToolCall]) -> List[str]:
        """æ™ºèƒ½è°ƒåº¦MLå·¥å…·æ‰§è¡Œ"""
        print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½è°ƒåº¦ {len(tool_calls)} ä¸ªå·¥å…·")
        
        # 1. æ³¨å†Œæ‰€æœ‰å·¥å…·
        for tool in tool_calls:
            self.tools[tool.id] = tool
            self.stats['total_scheduled'] += 1
        
        # 2. æ„å»ºä¾èµ–å›¾
        await self._build_dependency_graph(tool_calls)
        
        # 3. æ£€æµ‹å¾ªç¯ä¾èµ–
        if not nx.is_directed_acyclic_graph(self.dependency_graph):
            cycles = list(nx.simple_cycles(self.dependency_graph))
            raise ValueError(f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {cycles}")
        
        # 4. æ‹“æ‰‘æ’åºè·å¾—åŸºç¡€æ‰§è¡Œé¡ºåº
        base_order = list(nx.topological_sort(self.dependency_graph))
        
        # 5. æ™ºèƒ½ä¼˜åŒ–æ‰§è¡Œé¡ºåº
        optimized_order = await self._optimize_execution_order(base_order)
        
        # 6. åˆ›å»ºå¹¶è¡Œæ‰§è¡Œç»„
        parallel_groups = await self._create_parallel_groups(optimized_order)
        
        # 7. æ‰§è¡Œè°ƒåº¦
        scheduled_ids = await self._execute_parallel_groups(parallel_groups)
        
        print(f"âœ… è°ƒåº¦å®Œæˆï¼ŒæˆåŠŸè°ƒåº¦ {len(scheduled_ids)} ä¸ªå·¥å…·")
        return scheduled_ids
    
    async def _build_dependency_graph(self, tool_calls: List[MLToolCall]):
        """æ„å»ºæ™ºèƒ½ä¾èµ–å›¾"""
        print("ğŸ”— æ„å»ºå·¥å…·ä¾èµ–å›¾...")
        
        # æ¸…ç©ºç°æœ‰å›¾
        self.dependency_graph.clear()
        
        # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
        for tool in tool_calls:
            self.dependency_graph.add_node(tool.id, tool=tool)
        
        # æ·»åŠ æ˜¾å¼ä¾èµ–
        for tool in tool_calls:
            for dep_id in tool.explicit_dependencies:
                if dep_id in self.tools:
                    self.dependency_graph.add_edge(dep_id, tool.id)
        
        # æ¨æ–­éšå¼ä¾èµ–
        for tool in tool_calls:
            implicit_deps = await self._infer_implicit_dependencies(tool, tool_calls)
            for dep_id in implicit_deps:
                self.dependency_graph.add_edge(dep_id, tool.id)
                tool.implicit_dependencies.add(dep_id)
        
        print(f"ğŸ“Š ä¾èµ–å›¾æ„å»ºå®Œæˆ: {len(self.dependency_graph.nodes)} èŠ‚ç‚¹, {len(self.dependency_graph.edges)} è¾¹")
    
    async def _infer_implicit_dependencies(self, tool: MLToolCall, all_tools: List[MLToolCall]) -> Set[str]:
        """æ™ºèƒ½æ¨æ–­å·¥å…·é—´çš„éšå¼ä¾èµ–"""
        implicit_deps = set()
        
        # æ•°æ®æµä¾èµ–åˆ†æ
        data_flow_deps = self._analyze_data_flow_dependencies(tool, all_tools)
        implicit_deps.update(data_flow_deps)
        
        # èµ„æºå†²çªåˆ†æ
        resource_conflict_deps = self._analyze_resource_conflicts(tool, all_tools)
        implicit_deps.update(resource_conflict_deps)
        
        # æ–‡ä»¶ç³»ç»Ÿä¾èµ–
        file_system_deps = self._analyze_file_system_dependencies(tool, all_tools)
        implicit_deps.update(file_system_deps)
        
        # æ¨¡å‹ç”Ÿå‘½å‘¨æœŸä¾èµ–
        model_lifecycle_deps = self._analyze_model_lifecycle_dependencies(tool, all_tools)
        implicit_deps.update(model_lifecycle_deps)
        
        return implicit_deps
    
    def _analyze_data_flow_dependencies(self, tool: MLToolCall, all_tools: List[MLToolCall]) -> Set[str]:
        """åˆ†ææ•°æ®æµä¾èµ–"""
        deps = set()
        
        # å¸¸è§çš„æ•°æ®æµæ¨¡å¼
        data_flow_patterns = {
            'train_model': ['preprocess_data', 'feature_engineering', 'data_validation'],
            'evaluate_model': ['train_model'],
            'deploy_model': ['train_model', 'evaluate_model'],
            'generate_report': ['evaluate_model', 'analyze_results'],
            'visualize_results': ['evaluate_model', 'analyze_data'],
            'feature_engineering': ['preprocess_data', 'load_data'],
            'hyperparameter_tuning': ['preprocess_data', 'feature_engineering']
        }
        
        if tool.tool_name in data_flow_patterns:
            required_predecessors = data_flow_patterns[tool.tool_name]
            for other in all_tools:
                if other.tool_name in required_predecessors:
                    # æ£€æŸ¥æ˜¯å¦æ“ä½œç›¸åŒçš„æ•°æ®é›†
                    if self._shares_data_pipeline(tool, other):
                        deps.add(other.id)
        
        return deps
    
    def _analyze_resource_conflicts(self, tool: MLToolCall, all_tools: List[MLToolCall]) -> Set[str]:
        """åˆ†æèµ„æºå†²çª"""
        deps = set()
        
        for other in all_tools:
            if other.id == tool.id:
                continue
                
            # æ£€æŸ¥èµ„æºå†²çª
            if self._has_resource_conflict(tool, other):
                # æŒ‰ä¼˜å…ˆçº§ç¡®å®šä¾èµ–å…³ç³»
                if other.priority > tool.priority:
                    deps.add(other.id)
                elif other.priority == tool.priority:
                    # ä¼˜å…ˆçº§ç›¸åŒæ—¶ï¼ŒæŒ‰é¢„ä¼°æ‰§è¡Œæ—¶é—´æ’åº
                    if other.estimated_duration > tool.estimated_duration:
                        deps.add(other.id)
        
        return deps
    
    def _analyze_file_system_dependencies(self, tool: MLToolCall, all_tools: List[MLToolCall]) -> Set[str]:
        """åˆ†ææ–‡ä»¶ç³»ç»Ÿä¾èµ–"""
        deps = set()
        
        tool_input_files = self._extract_input_files(tool)
        
        for other in all_tools:
            if other.id == tool.id:
                continue
                
            other_output_files = self._extract_output_files(other)
            
            # å¦‚æœå½“å‰å·¥å…·çš„è¾“å…¥ä¾èµ–å…¶ä»–å·¥å…·çš„è¾“å‡º
            if tool_input_files & other_output_files:
                deps.add(other.id)
        
        return deps
    
    def _analyze_model_lifecycle_dependencies(self, tool: MLToolCall, all_tools: List[MLToolCall]) -> Set[str]:
        """åˆ†ææ¨¡å‹ç”Ÿå‘½å‘¨æœŸä¾èµ–"""
        deps = set()
        
        model_lifecycle_order = [
            'data_validation', 'preprocess_data', 'feature_engineering',
            'train_model', 'validate_model', 'evaluate_model', 
            'hyperparameter_tuning', 'deploy_model', 'monitor_model'
        ]
        
        try:
            tool_stage = model_lifecycle_order.index(tool.tool_name)
            
            for other in all_tools:
                if other.id == tool.id:
                    continue
                    
                try:
                    other_stage = model_lifecycle_order.index(other.tool_name)
                    # å¦‚æœå…¶ä»–å·¥å…·åœ¨ç”Ÿå‘½å‘¨æœŸä¸­æ›´æ—©ï¼Œä¸”æ“ä½œç›¸åŒæ¨¡å‹
                    if other_stage < tool_stage and self._shares_model_artifacts(tool, other):
                        deps.add(other.id)
                except ValueError:
                    # å…¶ä»–å·¥å…·ä¸åœ¨æ ‡å‡†ç”Ÿå‘½å‘¨æœŸä¸­
                    pass
        except ValueError:
            # å½“å‰å·¥å…·ä¸åœ¨æ ‡å‡†ç”Ÿå‘½å‘¨æœŸä¸­
            pass
        
        return deps
    
    def _shares_data_pipeline(self, tool1: MLToolCall, tool2: MLToolCall) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå·¥å…·æ˜¯å¦å…±äº«æ•°æ®ç®¡é“"""
        # æ£€æŸ¥å‚æ•°ä¸­çš„æ•°æ®é›†è·¯å¾„
        dataset_keys = ['dataset_path', 'data_path', 'input_data', 'data_file']
        
        tool1_datasets = set()
        tool2_datasets = set()
        
        for key in dataset_keys:
            if key in tool1.parameters:
                tool1_datasets.add(str(tool1.parameters[key]))
            if key in tool2.parameters:
                tool2_datasets.add(str(tool2.parameters[key]))
        
        return bool(tool1_datasets & tool2_datasets)
    
    def _has_resource_conflict(self, tool1: MLToolCall, tool2: MLToolCall) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå·¥å…·æ˜¯å¦æœ‰èµ„æºå†²çª"""
        for req1 in tool1.resource_requirements:
            for req2 in tool2.resource_requirements:
                if req1.type == req2.type:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç‹¬å éœ€æ±‚æˆ–èµ„æºä½¿ç”¨è¿‡å¤š
                    if req1.exclusive or req2.exclusive:
                        return True
                    
                    # æ£€æŸ¥æ€»èµ„æºéœ€æ±‚æ˜¯å¦è¶…è¿‡å¯ç”¨èµ„æº
                    available = self.resource_pool.get_available_resources()[req1.type]
                    if (req1.amount + req2.amount) > available:
                        return True
        
        return False
    
    def _extract_input_files(self, tool: MLToolCall) -> Set[str]:
        """æå–å·¥å…·çš„è¾“å…¥æ–‡ä»¶"""
        input_files = set()
        input_keys = ['input_file', 'data_file', 'model_file', 'config_file']
        
        for key in input_keys:
            if key in tool.parameters:
                input_files.add(str(tool.parameters[key]))
        
        return input_files
    
    def _extract_output_files(self, tool: MLToolCall) -> Set[str]:
        """æå–å·¥å…·çš„è¾“å‡ºæ–‡ä»¶"""
        output_files = set()
        output_keys = ['output_file', 'model_path', 'result_path', 'report_path']
        
        for key in output_keys:
            if key in tool.parameters:
                output_files.add(str(tool.parameters[key]))
        
        return output_files
    
    def _shares_model_artifacts(self, tool1: MLToolCall, tool2: MLToolCall) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå·¥å…·æ˜¯å¦å…±äº«æ¨¡å‹æ–‡ä»¶"""
        model_keys = ['model_path', 'model_file', 'checkpoint_path']
        
        tool1_models = set()
        tool2_models = set()
        
        for key in model_keys:
            if key in tool1.parameters:
                tool1_models.add(str(tool1.parameters[key]))
            if key in tool2.parameters:
                tool2_models.add(str(tool2.parameters[key]))
        
        return bool(tool1_models & tool2_models)
    
    async def _optimize_execution_order(self, base_order: List[str]) -> List[str]:
        """æ™ºèƒ½ä¼˜åŒ–æ‰§è¡Œé¡ºåº"""
        print("ğŸ§  ä¼˜åŒ–æ‰§è¡Œé¡ºåº...")
        
        # åˆ›å»ºä¼˜åŒ–åçš„é¡ºåº
        optimized = []
        remaining = set(base_order)
        
        while remaining:
            # æ‰¾åˆ°å¯ä»¥æ‰§è¡Œçš„å·¥å…·ï¼ˆæ‰€æœ‰ä¾èµ–éƒ½å·²å®Œæˆï¼‰
            ready_tools = []
            for tool_id in remaining:
                tool = self.tools[tool_id]
                all_deps = tool.explicit_dependencies | tool.implicit_dependencies
                
                # æ£€æŸ¥æ‰€æœ‰ä¾èµ–æ˜¯å¦å·²ç»åœ¨ä¼˜åŒ–åºåˆ—ä¸­
                if all_deps.issubset(set(optimized)):
                    ready_tools.append(tool_id)
            
            if not ready_tools:
                # å¦‚æœæ²¡æœ‰å‡†å¤‡å¥½çš„å·¥å…·ï¼Œå¯èƒ½æœ‰å¾ªç¯ä¾èµ–
                break
            
            # æŒ‰ä¼˜å…ˆçº§å’Œèµ„æºéœ€æ±‚æ’åº
            ready_tools.sort(key=lambda tid: (
                -self.tools[tid].priority,  # ä¼˜å…ˆçº§é«˜çš„åœ¨å‰
                self.tools[tid].estimated_duration,  # æ‰§è¡Œæ—¶é—´çŸ­çš„åœ¨å‰
                -len(self.tools[tid].resource_requirements)  # èµ„æºéœ€æ±‚å°‘çš„åœ¨å‰
            ))
            
            # é€‰æ‹©æœ€ä¼˜çš„å·¥å…·
            selected = ready_tools[0]
            optimized.append(selected)
            remaining.remove(selected)
        
        # å°†å‰©ä½™çš„å·¥å…·æŒ‰åŸé¡ºåºæ·»åŠ ï¼ˆå¤„ç†å¾ªç¯ä¾èµ–æƒ…å†µï¼‰
        for tool_id in base_order:
            if tool_id in remaining:
                optimized.append(tool_id)
        
        print(f"âœ… æ‰§è¡Œé¡ºåºä¼˜åŒ–å®Œæˆ")
        return optimized
    
    async def _create_parallel_groups(self, execution_order: List[str]) -> List[List[str]]:
        """åˆ›å»ºå¹¶è¡Œæ‰§è¡Œç»„"""
        print("ğŸ”„ åˆ›å»ºå¹¶è¡Œæ‰§è¡Œç»„...")
        
        groups = []
        remaining = execution_order.copy()
        completed = set()
        
        while remaining:
            current_group = []
            current_resources = ResourcePool(
                cpu_cores=self.resource_pool.cpu_cores,
                memory_gb=self.resource_pool.memory_gb,
                gpu_count=self.resource_pool.gpu_count,
                disk_gb=self.resource_pool.disk_gb
            )
            
            # æ‰¾åˆ°å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„å·¥å…·
            i = 0
            while i < len(remaining) and len(current_group) < self.max_concurrent_tools:
                tool_id = remaining[i]
                tool = self.tools[tool_id]
                
                # æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³
                all_deps = tool.explicit_dependencies | tool.implicit_dependencies
                if not all_deps.issubset(completed):
                    i += 1
                    continue
                
                # æ£€æŸ¥èµ„æºæ˜¯å¦å¯ç”¨
                if current_resources.can_allocate(tool.resource_requirements):
                    current_group.append(tool_id)
                    current_resources.allocate(tool.resource_requirements)
                    remaining.pop(i)
                else:
                    i += 1
            
            if current_group:
                groups.append(current_group)
                completed.update(current_group)
            else:
                # å¦‚æœæ²¡æœ‰å·¥å…·å¯ä»¥æ‰§è¡Œï¼Œå¼ºåˆ¶é€‰æ‹©ç¬¬ä¸€ä¸ª
                if remaining:
                    groups.append([remaining.pop(0)])
                    completed.add(groups[-1][0])
        
        print(f"ğŸ“Š åˆ›å»ºäº† {len(groups)} ä¸ªå¹¶è¡Œæ‰§è¡Œç»„")
        return groups
    
    async def _execute_parallel_groups(self, groups: List[List[str]]) -> List[str]:
        """æ‰§è¡Œå¹¶è¡Œç»„"""
        scheduled_ids = []
        
        for i, group in enumerate(groups):
            print(f"ğŸš€ æ‰§è¡Œç¬¬ {i+1}/{len(groups)} ç»„: {len(group)} ä¸ªå·¥å…·")
            
            # æ£€æŸ¥æ‰¹å‡†éœ€æ±‚
            approval_needed = []
            for tool_id in group:
                tool = self.tools[tool_id]
                if tool.requires_approval and not tool.approved:
                    approval_needed.append(tool_id)
            
            # å¤„ç†æ‰¹å‡†
            if approval_needed:
                await self._request_approvals(approval_needed)
            
            # å¹¶è¡Œæ‰§è¡Œç»„å†…å·¥å…·
            group_results = await self._execute_group(group)
            scheduled_ids.extend(group_results)
        
        return scheduled_ids
    
    async def _request_approvals(self, tool_ids: List[str]):
        """è¯·æ±‚å·¥å…·æ‰§è¡Œæ‰¹å‡†"""
        print(f"âš ï¸ éœ€è¦æ‰¹å‡† {len(tool_ids)} ä¸ªå·¥å…·çš„æ‰§è¡Œ")
        
        for tool_id in tool_ids:
            tool = self.tools[tool_id]
            print(f"ğŸ” å·¥å…·: {tool.tool_name}")
            print(f"ğŸ“ è¯´æ˜: {tool.approval_message or 'éœ€è¦ç”¨æˆ·ç¡®è®¤'}")
            
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯å¼‚æ­¥çš„ç”¨æˆ·ç•Œé¢äº¤äº’
            # ç°åœ¨æš‚æ—¶è‡ªåŠ¨æ‰¹å‡†
            tool.approved = True
            print(f"âœ… å·¥å…· {tool.tool_name} å·²æ‰¹å‡†")
    
    async def _execute_group(self, group: List[str]) -> List[str]:
        """æ‰§è¡Œå·¥å…·ç»„"""
        tasks = []
        
        # åˆ†é…èµ„æº
        for tool_id in group:
            tool = self.tools[tool_id]
            if not self.resource_pool.allocate(tool.resource_requirements):
                print(f"âš ï¸ æ— æ³•ä¸ºå·¥å…· {tool.tool_name} åˆ†é…èµ„æº")
                continue
            
            # åˆ›å»ºæ‰§è¡Œä»»åŠ¡
            task = asyncio.create_task(self._execute_single_tool(tool))
            tasks.append((tool_id, task))
            
            # è§¦å‘äº‹ä»¶
            await self._trigger_event('tool_scheduled', tool)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = []
        for tool_id, task in tasks:
            try:
                result = await task
                results.append(tool_id)
                self.completed_tools.append(tool_id)
                self.stats['total_completed'] += 1
                
                # è§¦å‘å®Œæˆäº‹ä»¶
                await self._trigger_event('tool_completed', self.tools[tool_id])
                
            except Exception as e:
                print(f"âŒ å·¥å…· {tool_id} æ‰§è¡Œå¤±è´¥: {str(e)}")
                self.tools[tool_id].status = ToolStatus.FAILED
                self.tools[tool_id].error = str(e)
                self.failed_tools.append(tool_id)
                self.stats['total_failed'] += 1
                
                # è§¦å‘å¤±è´¥äº‹ä»¶
                await self._trigger_event('tool_failed', self.tools[tool_id])
            
            finally:
                # é‡Šæ”¾èµ„æº
                tool = self.tools[tool_id]
                self.resource_pool.release(tool.resource_requirements)
        
        return results
    
    async def _execute_single_tool(self, tool: MLToolCall) -> Any:
        """æ‰§è¡Œå•ä¸ªå·¥å…·"""
        tool.status = ToolStatus.EXECUTING
        tool.start_time = datetime.now()
        
        # è§¦å‘å¼€å§‹äº‹ä»¶
        await self._trigger_event('tool_started', tool)
        
        try:
            # æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œï¼ˆå®é™…åº”ç”¨ä¸­è¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„å·¥å…·ï¼‰
            if tool.progress_callback:
                # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
                for progress in [0.2, 0.4, 0.6, 0.8, 1.0]:
                    await asyncio.sleep(tool.estimated_duration * 0.2)
                    await tool.progress_callback(tool.id, progress)
            else:
                await asyncio.sleep(tool.estimated_duration)
            
            # æ¨¡æ‹ŸæˆåŠŸç»“æœ
            tool.result = {"status": "success", "data": f"Result from {tool.tool_name}"}
            tool.status = ToolStatus.COMPLETED
            
        except asyncio.TimeoutError:
            tool.status = ToolStatus.TIMEOUT
            tool.error = "æ‰§è¡Œè¶…æ—¶"
            raise
        except Exception as e:
            tool.status = ToolStatus.FAILED
            tool.error = str(e)
            raise
        finally:
            tool.end_time = datetime.now()
            if tool.start_time:
                tool.actual_duration = (tool.end_time - tool.start_time).total_seconds()
            
            if tool.completion_callback:
                await tool.completion_callback(tool.id, tool.result)
        
        return tool.result
    
    async def _trigger_event(self, event: str, tool: MLToolCall):
        """è§¦å‘äº‹ä»¶å›è°ƒ"""
        if event in self.event_callbacks:
            for callback in self.event_callbacks[event]:
                try:
                    if asyncio.iscoroutinefunction(callback):
                        await callback(tool)
                    else:
                        callback(tool)
                except Exception as e:
                    print(f"âš ï¸ äº‹ä»¶å›è°ƒå¤±è´¥ {event}: {str(e)}")
    
    def get_execution_status(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡ŒçŠ¶æ€"""
        status_counts = {}
        for status in ToolStatus:
            status_counts[status.value] = len([
                t for t in self.tools.values() if t.status == status
            ])
        
        return {
            'total_tools': len(self.tools),
            'status_breakdown': status_counts,
            'resource_utilization': {
                'cpu': f"{self.resource_pool.cpu_used}/{self.resource_pool.cpu_cores}",
                'memory': f"{self.resource_pool.memory_used:.1f}/{self.resource_pool.memory_gb:.1f} GB",
                'gpu': f"{self.resource_pool.gpu_used}/{self.resource_pool.gpu_count}"
            },
            'stats': self.stats
        }
    
    def export_execution_report(self, output_path: str = None) -> str:
        """å¯¼å‡ºæ‰§è¡ŒæŠ¥å‘Š"""
        if output_path is None:
            output_path = f"ml_execution_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'execution_summary': self.get_execution_status(),
            'tool_details': {
                tool_id: {
                    'tool_name': tool.tool_name,
                    'status': tool.status.value,
                    'duration': tool.actual_duration,
                    'start_time': tool.start_time.isoformat() if tool.start_time else None,
                    'end_time': tool.end_time.isoformat() if tool.end_time else None,
                    'error': tool.error,
                    'dependencies': list(tool.explicit_dependencies | tool.implicit_dependencies)
                }
                for tool_id, tool in self.tools.items()
            },
            'dependency_graph': {
                'nodes': list(self.dependency_graph.nodes()),
                'edges': list(self.dependency_graph.edges())
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“„ æ‰§è¡ŒæŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_path}")
        return output_path

# æµ‹è¯•å‡½æ•°
async def test_intelligent_scheduler():
    """æµ‹è¯•æ™ºèƒ½è°ƒåº¦å™¨"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½è°ƒåº¦å™¨æµ‹è¯•...")
    
    scheduler = IntelligentMLToolScheduler(max_concurrent_tools=3)
    
    # åˆ›å»ºæµ‹è¯•å·¥å…·
    tools = []
    
    # æ•°æ®é¢„å¤„ç†å·¥å…·
    preprocess_tool = MLToolCall(
        id="preprocess_1",
        tool_name="preprocess_data",
        parameters={"dataset_path": "data/raw.csv", "output_path": "data/processed.csv"},
        estimated_duration=10,
        priority=3,
        resource_requirements=[
            ResourceRequirement(ResourceType.CPU, 2, "cores"),
            ResourceRequirement(ResourceType.MEMORY, 4, "GB")
        ]
    )
    tools.append(preprocess_tool)
    
    # ç‰¹å¾å·¥ç¨‹å·¥å…·
    feature_tool = MLToolCall(
        id="feature_1",
        tool_name="feature_engineering",
        parameters={"input_data": "data/processed.csv", "output_path": "data/features.csv"},
        estimated_duration=15,
        priority=2,
        explicit_dependencies={"preprocess_1"},
        resource_requirements=[
            ResourceRequirement(ResourceType.CPU, 1, "cores"),
            ResourceRequirement(ResourceType.MEMORY, 2, "GB")
        ]
    )
    tools.append(feature_tool)
    
    # æ¨¡å‹è®­ç»ƒå·¥å…·
    train_tool = MLToolCall(
        id="train_1",
        tool_name="train_model",
        parameters={"data_path": "data/features.csv", "model_path": "models/model.pkl"},
        estimated_duration=30,
        priority=1,
        requires_approval=True,
        approval_message="è®­ç»ƒæ¨¡å‹ä¼šæ¶ˆè€—å¤§é‡è®¡ç®—èµ„æº",
        resource_requirements=[
            ResourceRequirement(ResourceType.CPU, 4, "cores"),
            ResourceRequirement(ResourceType.MEMORY, 8, "GB")
        ]
    )
    tools.append(train_tool)
    
    # æ¨¡å‹è¯„ä¼°å·¥å…·
    eval_tool = MLToolCall(
        id="eval_1",
        tool_name="evaluate_model",
        parameters={"model_path": "models/model.pkl", "test_data": "data/test.csv"},
        estimated_duration=5,
        priority=2,
        resource_requirements=[
            ResourceRequirement(ResourceType.CPU, 1, "cores"),
            ResourceRequirement(ResourceType.MEMORY, 2, "GB")
        ]
    )
    tools.append(eval_tool)
    
    # æ³¨å†Œäº‹ä»¶å›è°ƒ
    async def on_tool_completed(tool: MLToolCall):
        print(f"âœ… å·¥å…·å®Œæˆ: {tool.tool_name} (è€—æ—¶: {tool.actual_duration:.1f}s)")
    
    scheduler.register_event_callback('tool_completed', on_tool_completed)
    
    # æ‰§è¡Œè°ƒåº¦
    try:
        scheduled_ids = await scheduler.schedule_tools(tools)
        print(f"ğŸ‰ è°ƒåº¦å®Œæˆ: {len(scheduled_ids)} ä¸ªå·¥å…·æˆåŠŸè°ƒåº¦")
        
        # æ‰“å°æ‰§è¡ŒçŠ¶æ€
        status = scheduler.get_execution_status()
        print(f"ğŸ“Š æ‰§è¡ŒçŠ¶æ€: {status}")
        
        # å¯¼å‡ºæŠ¥å‘Š
        report_path = scheduler.export_execution_report()
        print(f"ğŸ“‹ æ‰§è¡ŒæŠ¥å‘Š: {report_path}")
        
    except Exception as e:
        print(f"âŒ è°ƒåº¦å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    asyncio.run(test_intelligent_scheduler())