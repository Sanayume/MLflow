"""
Intelligent Prompt Generation System
æ™ºèƒ½æç¤ºç”Ÿæˆç³»ç»Ÿ - å¢å¼ºç‰ˆæœ¬
åŸºäºgemini-cliçš„æ™ºèƒ½æç¤ºæ¶æ„ï¼Œç»“åˆç”¨æˆ·æ„å›¾åˆ†æå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥
æä¾›åŠ¨æ€æç¤ºç”Ÿæˆã€æ¨¡æ¿ç®¡ç†å’Œæ™ºèƒ½æ¨è
"""

import asyncio
import json
import re
import hashlib
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple, Union
from enum import Enum
import sqlite3
import pickle
import logging

logger = logging.getLogger(__name__)

# æ–°å¢æ™ºèƒ½æ„å›¾åˆ†æç±»å‹
class IntentType(Enum):
    """ç”¨æˆ·æ„å›¾ç±»å‹"""
    DATA_EXPLORATION = "data_exploration"
    MODEL_TRAINING = "model_training"
    MODEL_EVALUATION = "model_evaluation"
    FEATURE_ENGINEERING = "feature_engineering"
    VISUALIZATION = "visualization"
    DEBUGGING = "debugging"
    OPTIMIZATION = "optimization"
    DEPLOYMENT = "deployment"
    GENERAL_QUESTION = "general_question"

class PromptComplexity(Enum):
    """æç¤ºå¤æ‚åº¦"""
    SIMPLE = "simple"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"
    EXPERT = "expert"

@dataclass
class UserIntent:
    """ç”¨æˆ·æ„å›¾åˆ†æç»“æœ"""
    primary_intent: IntentType
    confidence: float
    secondary_intents: List[Tuple[IntentType, float]] = field(default_factory=list)
    extracted_entities: Dict[str, List[str]] = field(default_factory=dict)
    complexity_level: PromptComplexity = PromptComplexity.INTERMEDIATE
    context_keywords: Set[str] = field(default_factory=set)

@dataclass
class PromptTemplateData:
    """æ™ºèƒ½æç¤ºæ¨¡æ¿æ•°æ®"""
    id: str
    name: str
    description: str
    template_text: str
    intent_types: List[IntentType]
    complexity_level: PromptComplexity
    variables: List[str]  # æ¨¡æ¿å˜é‡
    tags: Set[str] = field(default_factory=set)
    usage_count: int = 0
    effectiveness_score: float = 0.5
    created_at: datetime = field(default_factory=datetime.now)
    last_used: Optional[datetime] = None

@dataclass
class PromptGenerationContext:
    """æç¤ºç”Ÿæˆä¸Šä¸‹æ–‡"""
    user_input: str
    ml_context: Dict[str, Any]  # æ¥è‡ªç¯å¢ƒåˆ†æå™¨çš„ä¸Šä¸‹æ–‡
    conversation_history: List[str] = field(default_factory=list)
    current_task: Optional[str] = None
    available_tools: List[str] = field(default_factory=list)
    data_context: Dict[str, Any] = field(default_factory=dict)
    model_context: Dict[str, Any] = field(default_factory=dict)

@dataclass
class GeneratedPrompt:
    """ç”Ÿæˆçš„æç¤º"""
    id: str
    original_input: str
    generated_text: str
    intent: UserIntent
    template_used: Optional[str]
    confidence: float
    reasoning: str
    suggestions: List[str] = field(default_factory=list)
    context_used: Dict[str, Any] = field(default_factory=dict)
    generated_at: datetime = field(default_factory=datetime.now)

class IntelligentMLPromptGenerator:
    """æ™ºèƒ½MLæç¤ºç”Ÿæˆå™¨ - å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self, project_root: str = ".", prompt_db_path: str = None):
        self.project_root = Path(project_root).resolve()
        self.prompt_db_path = prompt_db_path or self.project_root / ".mlagent" / "prompts.db"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.prompt_db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å†…å­˜ä¸­çš„æ•°æ®ç»“æ„
        self.templates: Dict[str, PromptTemplateData] = {}
        self.generated_prompts: Dict[str, GeneratedPrompt] = {}
        
        # ç¼“å­˜
        self.generation_cache = {}
        self.cache_ttl = 1800  # 30åˆ†é’Ÿ
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
        
        # åŠ è½½é¢„å®šä¹‰æ¨¡æ¿
        asyncio.create_task(self._load_predefined_templates())
        
        # åŠ è½½ç°æœ‰æ•°æ®
        asyncio.create_task(self._load_data())
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        self.conn = sqlite3.connect(str(self.prompt_db_path), check_same_thread=False)
        
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS prompt_templates (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT NOT NULL,
                template_text TEXT NOT NULL,
                intent_types TEXT NOT NULL,
                complexity_level TEXT NOT NULL,
                variables TEXT NOT NULL,
                tags TEXT,
                usage_count INTEGER DEFAULT 0,
                effectiveness_score REAL DEFAULT 0.5,
                created_at TEXT,
                last_used TEXT
            );
            
            CREATE TABLE IF NOT EXISTS generated_prompts (
                id TEXT PRIMARY KEY,
                original_input TEXT NOT NULL,
                generated_text TEXT NOT NULL,
                intent_data TEXT NOT NULL,
                template_used TEXT,
                confidence REAL,
                reasoning TEXT,
                suggestions TEXT,
                context_used TEXT,
                generated_at TEXT
            );
            
            CREATE TABLE IF NOT EXISTS user_feedback (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                prompt_id TEXT NOT NULL,
                rating INTEGER CHECK(rating >= 1 AND rating <= 5),
                feedback_text TEXT,
                timestamp TEXT,
                FOREIGN KEY (prompt_id) REFERENCES generated_prompts (id)
            );
            
            CREATE INDEX IF NOT EXISTS idx_template_intent ON prompt_templates(intent_types);
            CREATE INDEX IF NOT EXISTS idx_template_complexity ON prompt_templates(complexity_level);
            CREATE INDEX IF NOT EXISTS idx_prompt_timestamp ON generated_prompts(generated_at);
        """)
        
        self.conn.commit()
    
    async def _load_predefined_templates(self):
        """åŠ è½½é¢„å®šä¹‰çš„æç¤ºæ¨¡æ¿"""
        print("ğŸ“ åŠ è½½é¢„å®šä¹‰æç¤ºæ¨¡æ¿...")
        
        predefined_templates = [
            {
                "name": "æ•°æ®æ¢ç´¢åˆ†æ",
                "description": "ç”¨äºæ•°æ®é›†åˆæ­¥æ¢ç´¢å’Œç»Ÿè®¡åˆ†æ",
                "template_text": "è¯·å¸®æˆ‘åˆ†ææ•°æ®é›† {dataset_name}ã€‚æˆ‘æƒ³äº†è§£ï¼š\n1. æ•°æ®çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n2. æ•°æ®è´¨é‡ï¼ˆç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ï¼‰\n3. ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§\n4. æ•°æ®åˆ†å¸ƒç‰¹å¾\nè¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå¯è§†åŒ–å»ºè®®ã€‚",
                "intent_types": [IntentType.DATA_EXPLORATION],
                "complexity_level": PromptComplexity.INTERMEDIATE,
                "variables": ["dataset_name"],
                "tags": {"exploration", "statistics", "visualization"}
            },
            {
                "name": "æ¨¡å‹è®­ç»ƒæŒ‡å¯¼",
                "description": "ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„æŒ‡å¯¼",
                "template_text": "æˆ‘æƒ³è®­ç»ƒä¸€ä¸ª {model_type} æ¨¡å‹æ¥è§£å†³ {problem_type} é—®é¢˜ã€‚\næ•°æ®é›†ï¼š{dataset_info}\nç›®æ ‡ï¼š{target_description}\nè¯·å¸®æˆ‘ï¼š\n1. é€‰æ‹©åˆé€‚çš„ç®—æ³•å’Œå‚æ•°\n2. è®¾è®¡è®­ç»ƒæµç¨‹\n3. å®šä¹‰è¯„ä¼°æŒ‡æ ‡\n4. æä¾›ä»£ç å®ç°å»ºè®®",
                "intent_types": [IntentType.MODEL_TRAINING],
                "complexity_level": PromptComplexity.ADVANCED,
                "variables": ["model_type", "problem_type", "dataset_info", "target_description"],
                "tags": {"training", "algorithm", "parameters"}
            },
            {
                "name": "ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–",
                "description": "ç”¨äºç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†",
                "template_text": "é’ˆå¯¹ {data_type} æ•°æ®ï¼Œæˆ‘éœ€è¦è¿›è¡Œç‰¹å¾å·¥ç¨‹ã€‚\nå½“å‰ç‰¹å¾ï¼š{current_features}\nç›®æ ‡ï¼š{objective}\nè¯·å»ºè®®ï¼š\n1. ç‰¹å¾é€‰æ‹©ç­–ç•¥\n2. ç‰¹å¾å˜æ¢æ–¹æ³•\n3. æ–°ç‰¹å¾åˆ›å»ºæ€è·¯\n4. ç‰¹å¾é‡è¦æ€§è¯„ä¼°æ–¹æ³•",
                "intent_types": [IntentType.FEATURE_ENGINEERING],
                "complexity_level": PromptComplexity.INTERMEDIATE,
                "variables": ["data_type", "current_features", "objective"],
                "tags": {"feature_engineering", "preprocessing", "selection"}
            },
            {
                "name": "æ¨¡å‹è¯„ä¼°è¯Šæ–­",
                "description": "ç”¨äºæ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œè¯Šæ–­",
                "template_text": "æˆ‘çš„ {model_name} æ¨¡å‹åœ¨ {dataset} ä¸Šçš„è¡¨ç°æ˜¯ï¼š\n{performance_metrics}\nè¯·å¸®æˆ‘ï¼š\n1. åˆ†ææ¨¡å‹æ€§èƒ½\n2. è¯Šæ–­å¯èƒ½çš„é—®é¢˜\n3. æå‡ºæ”¹è¿›å»ºè®®\n4. æ¨èè¿›ä¸€æ­¥çš„è¯„ä¼°æ–¹æ³•",
                "intent_types": [IntentType.MODEL_EVALUATION],
                "complexity_level": PromptComplexity.INTERMEDIATE,
                "variables": ["model_name", "dataset", "performance_metrics"],
                "tags": {"evaluation", "metrics", "diagnosis"}
            },
            {
                "name": "ä»£ç è°ƒè¯•å¸®åŠ©",
                "description": "ç”¨äºMLä»£ç çš„è°ƒè¯•å’Œé”™è¯¯è§£å†³",
                "template_text": "æˆ‘åœ¨è¿è¡ŒMLä»£ç æ—¶é‡åˆ°äº†é—®é¢˜ï¼š\né”™è¯¯ä¿¡æ¯ï¼š{error_message}\nä»£ç ç‰‡æ®µï¼š{code_snippet}\nç¯å¢ƒä¿¡æ¯ï¼š{environment_info}\nè¯·å¸®æˆ‘ï¼š\n1. åˆ†æé”™è¯¯åŸå› \n2. æä¾›è§£å†³æ–¹æ¡ˆ\n3. ç»™å‡ºä¿®å¤åçš„ä»£ç \n4. é¢„é˜²ç±»ä¼¼é—®é¢˜çš„å»ºè®®",
                "intent_types": [IntentType.DEBUGGING],
                "complexity_level": PromptComplexity.SIMPLE,
                "variables": ["error_message", "code_snippet", "environment_info"],
                "tags": {"debugging", "error", "troubleshooting"}
            },
            {
                "name": "å¯è§†åŒ–è®¾è®¡",
                "description": "ç”¨äºæ•°æ®å¯è§†åŒ–å’Œç»“æœå±•ç¤º",
                "template_text": "æˆ‘éœ€è¦ä¸º {data_description} åˆ›å»ºå¯è§†åŒ–ã€‚\nç›®çš„ï¼š{visualization_purpose}\nç›®æ ‡å—ä¼—ï¼š{target_audience}\nè¯·æ¨èï¼š\n1. åˆé€‚çš„å›¾è¡¨ç±»å‹\n2. å¯è§†åŒ–åº“å’Œå·¥å…·\n3. è®¾è®¡åŸåˆ™å’Œæœ€ä½³å®è·µ\n4. å…·ä½“çš„å®ç°ä»£ç ",
                "intent_types": [IntentType.VISUALIZATION],
                "complexity_level": PromptComplexity.INTERMEDIATE,
                "variables": ["data_description", "visualization_purpose", "target_audience"],
                "tags": {"visualization", "charts", "presentation"}
            }
        ]
        
        for template_data in predefined_templates:
            template_id = hashlib.md5(template_data["name"].encode()).hexdigest()[:12]
            
            template = PromptTemplateData(
                id=template_id,
                name=template_data["name"],
                description=template_data["description"],
                template_text=template_data["template_text"],
                intent_types=template_data["intent_types"],
                complexity_level=template_data["complexity_level"],
                variables=template_data["variables"],
                tags=template_data["tags"]
            )
            
            self.templates[template_id] = template
            await self._save_template(template)
        
        print(f"âœ… åŠ è½½äº† {len(predefined_templates)} ä¸ªé¢„å®šä¹‰æ¨¡æ¿")
    
    async def _load_data(self):
        """ä»æ•°æ®åº“åŠ è½½æ•°æ®"""
        print("ğŸ’¾ åŠ è½½ç°æœ‰æç¤ºæ•°æ®...")
        
        cursor = self.conn.cursor()
        
        # åŠ è½½æ¨¡æ¿
        cursor.execute("SELECT * FROM prompt_templates")
        for row in cursor.fetchall():
            template = self._row_to_template(row)
            self.templates[template.id] = template
        
        # åŠ è½½ç”Ÿæˆçš„æç¤º
        cursor.execute("SELECT * FROM generated_prompts")
        for row in cursor.fetchall():
            prompt = self._row_to_generated_prompt(row)
            self.generated_prompts[prompt.id] = prompt
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.templates)} ä¸ªæ¨¡æ¿, {len(self.generated_prompts)} ä¸ªç”Ÿæˆçš„æç¤º")
    
    def _row_to_template(self, row) -> PromptTemplateData:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºæ¨¡æ¿å¯¹è±¡"""
        return PromptTemplateData(
            id=row[0],
            name=row[1],
            description=row[2],
            template_text=row[3],
            intent_types=[IntentType(t) for t in json.loads(row[4])],
            complexity_level=PromptComplexity(row[5]),
            variables=json.loads(row[6]),
            tags=set(json.loads(row[7]) if row[7] else []),
            usage_count=row[8],
            effectiveness_score=row[9],
            created_at=datetime.fromisoformat(row[10]),
            last_used=datetime.fromisoformat(row[11]) if row[11] else None
        )
    
    def _row_to_generated_prompt(self, row) -> GeneratedPrompt:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºç”Ÿæˆæç¤ºå¯¹è±¡"""
        intent_data = json.loads(row[3])
        intent = UserIntent(
            primary_intent=IntentType(intent_data['primary_intent']),
            confidence=intent_data['confidence'],
            secondary_intents=[(IntentType(t), s) for t, s in intent_data.get('secondary_intents', [])],
            extracted_entities=intent_data.get('extracted_entities', {}),
            complexity_level=PromptComplexity(intent_data.get('complexity_level', 'intermediate')),
            context_keywords=set(intent_data.get('context_keywords', []))
        )
        
        return GeneratedPrompt(
            id=row[0],
            original_input=row[1],
            generated_text=row[2],
            intent=intent,
            template_used=row[4],
            confidence=row[5],
            reasoning=row[6],
            suggestions=json.loads(row[7]) if row[7] else [],
            context_used=json.loads(row[8]) if row[8] else {},
            generated_at=datetime.fromisoformat(row[9])
        )
    
    async def generate_prompt(self, 
                            user_input: str,
                            context: PromptGenerationContext = None) -> GeneratedPrompt:
        """ä¸»è¦å…¥å£ï¼šæ™ºèƒ½ç”Ÿæˆæç¤º"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(f"{user_input}_{context}".encode()).hexdigest()
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.generation_cache:
            cached_result, timestamp = self.generation_cache[cache_key]
            if (datetime.now().timestamp() - timestamp) < self.cache_ttl:
                print("ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„æç¤ºç”Ÿæˆç»“æœ")
                return cached_result
        
        print(f"ğŸ§  å¼€å§‹æ™ºèƒ½æç¤ºç”Ÿæˆ: {user_input[:50]}...")
        
        try:
            # 1. åˆ†æç”¨æˆ·æ„å›¾
            intent = await self._analyze_user_intent(user_input, context)
            
            # 2. é€‰æ‹©æœ€ä½³æ¨¡æ¿
            best_template = await self._select_best_template(intent, context)
            
            # 3. ç”Ÿæˆæç¤ºæ–‡æœ¬
            generated_text, reasoning = await self._generate_prompt_text(
                user_input, intent, best_template, context
            )
            
            # 4. ç”Ÿæˆæ™ºèƒ½å»ºè®®
            suggestions = await self._generate_suggestions(intent, context)
            
            # 5. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_generation_confidence(intent, best_template, context)
            
            # 6. åˆ›å»ºç”Ÿæˆçš„æç¤ºå¯¹è±¡
            prompt_id = hashlib.md5(f"{user_input}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
            
            generated_prompt = GeneratedPrompt(
                id=prompt_id,
                original_input=user_input,
                generated_text=generated_text,
                intent=intent,
                template_used=best_template.id if best_template else None,
                confidence=confidence,
                reasoning=reasoning,
                suggestions=suggestions,
                context_used=asdict(context) if context else {}
            )
            
            # 7. ä¿å­˜åˆ°æ•°æ®åº“
            self.generated_prompts[prompt_id] = generated_prompt
            await self._save_generated_prompt(generated_prompt)
            
            # 8. æ›´æ–°æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
            if best_template:
                await self._update_template_usage(best_template.id)
            
            # 9. ç¼“å­˜ç»“æœ
            self.generation_cache[cache_key] = (generated_prompt, datetime.now().timestamp())
            
            print(f"âœ… æç¤ºç”Ÿæˆå®Œæˆï¼Œç½®ä¿¡åº¦: {confidence:.2f}")
            return generated_prompt
            
        except Exception as e:
            print(f"âŒ æç¤ºç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def _analyze_user_intent(self, 
                                 user_input: str, 
                                 context: PromptGenerationContext = None) -> UserIntent:
        """åˆ†æç”¨æˆ·æ„å›¾"""
        
        # å…³é”®è¯æ¨¡å¼åŒ¹é…
        intent_patterns = {
            IntentType.DATA_EXPLORATION: [
                r'æ¢ç´¢|åˆ†æ|æŸ¥çœ‹|æ£€æŸ¥|ç»Ÿè®¡|æè¿°|æ¦‚è§ˆ',
                r'æ•°æ®é›†|æ•°æ®|data|dataset|explore|analyze',
                r'åˆ†å¸ƒ|ç›¸å…³æ€§|ç¼ºå¤±å€¼|å¼‚å¸¸å€¼|è´¨é‡'
            ],
            IntentType.MODEL_TRAINING: [
                r'è®­ç»ƒ|å»ºæ¨¡|æ‹Ÿåˆ|å­¦ä¹ |train|fit|model',
                r'ç®—æ³•|åˆ†ç±»|å›å½’|èšç±»|æ·±åº¦å­¦ä¹ |ç¥ç»ç½‘ç»œ',
                r'å‚æ•°|è¶…å‚æ•°|ä¼˜åŒ–|è°ƒä¼˜'
            ],
            IntentType.MODEL_EVALUATION: [
                r'è¯„ä¼°|æµ‹è¯•|éªŒè¯|æ€§èƒ½|å‡†ç¡®ç‡|evaluate',
                r'æŒ‡æ ‡|metrics|score|accuracy|precision|recall',
                r'äº¤å‰éªŒè¯|éªŒè¯é›†|æµ‹è¯•é›†'
            ],
            IntentType.FEATURE_ENGINEERING: [
                r'ç‰¹å¾|feature|engineering|å˜æ¢|é€‰æ‹©',
                r'é¢„å¤„ç†|å½’ä¸€åŒ–|æ ‡å‡†åŒ–|ç¼–ç |é™ç»´',
                r'ç‰¹å¾é‡è¦æ€§|ç‰¹å¾é€‰æ‹©|ç‰¹å¾æå–'
            ],
            IntentType.VISUALIZATION: [
                r'å¯è§†åŒ–|å›¾è¡¨|ç”»å›¾|plot|chart|visualize',
                r'æŸ±çŠ¶å›¾|æ•£ç‚¹å›¾|çƒ­åŠ›å›¾|æŠ˜çº¿å›¾|ç›´æ–¹å›¾',
                r'matplotlib|seaborn|plotly'
            ],
            IntentType.DEBUGGING: [
                r'é”™è¯¯|æŠ¥é”™|debug|fix|è§£å†³|é—®é¢˜',
                r'å¼‚å¸¸|exception|error|bug|è°ƒè¯•',
                r'ä¸å·¥ä½œ|å¤±è´¥|æ— æ³•è¿è¡Œ'
            ],
            IntentType.OPTIMIZATION: [
                r'ä¼˜åŒ–|æå‡|æ”¹è¿›|accelerate|optimize',
                r'æ€§èƒ½|é€Ÿåº¦|å†…å­˜|æ•ˆç‡|performance',
                r'å¹¶è¡Œ|åˆ†å¸ƒå¼|GPU|åŠ é€Ÿ'
            ],
            IntentType.DEPLOYMENT: [
                r'éƒ¨ç½²|deploy|ç”Ÿäº§|production|æœåŠ¡',
                r'API|web|åº”ç”¨|application|ä¸Šçº¿',
                r'docker|kubernetes|äº‘|cloud'
            ]
        }
        
        # è®¡ç®—æ¯ä¸ªæ„å›¾çš„å¾—åˆ†
        intent_scores = {}
        text_lower = user_input.lower()
        
        for intent_type, patterns in intent_patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, text_lower))
                score += matches
            
            if score > 0:
                intent_scores[intent_type] = score / len(patterns)
        
        # ç¡®å®šä¸»è¦æ„å›¾
        if intent_scores:
            primary_intent = max(intent_scores, key=intent_scores.get)
            primary_confidence = intent_scores[primary_intent]
            
            # å½’ä¸€åŒ–ç½®ä¿¡åº¦
            total_score = sum(intent_scores.values())
            primary_confidence = primary_confidence / total_score if total_score > 0 else 0.1
            
            # ç¡®å®šæ¬¡è¦æ„å›¾
            secondary_intents = [
                (intent, score / total_score) 
                for intent, score in intent_scores.items() 
                if intent != primary_intent and score > 0
            ]
            secondary_intents.sort(key=lambda x: x[1], reverse=True)
            
        else:
            primary_intent = IntentType.GENERAL_QUESTION
            primary_confidence = 0.1
            secondary_intents = []
        
        # æå–å®ä½“
        entities = self._extract_entities(user_input)
        
        # ç¡®å®šå¤æ‚åº¦
        complexity = self._determine_complexity(user_input, context)
        
        # æå–ä¸Šä¸‹æ–‡å…³é”®è¯
        context_keywords = self._extract_context_keywords(user_input, context)
        
        return UserIntent(
            primary_intent=primary_intent,
            confidence=primary_confidence,
            secondary_intents=secondary_intents[:3],  # æœ€å¤š3ä¸ªæ¬¡è¦æ„å›¾
            extracted_entities=entities,
            complexity_level=complexity,
            context_keywords=context_keywords
        )
    
    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–å®ä½“ä¿¡æ¯"""
        entities = {
            'datasets': [],
            'models': [],
            'algorithms': [],
            'metrics': [],
            'libraries': [],
            'file_paths': []
        }
        
        # æ•°æ®é›†æ¨¡å¼
        dataset_patterns = r'(\w+\.csv|\w+\.json|\w+\.parquet|\w+\.xlsx|\w+_data|\w+_dataset)'
        entities['datasets'] = re.findall(dataset_patterns, text.lower())
        
        # æ¨¡å‹ç®—æ³•
        algorithm_keywords = [
            'random forest', 'svm', 'linear regression', 'logistic regression',
            'neural network', 'cnn', 'rnn', 'lstm', 'transformer',
            'xgboost', 'lightgbm', 'catboost', 'decision tree'
        ]
        for keyword in algorithm_keywords:
            if keyword in text.lower():
                entities['algorithms'].append(keyword)
        
        # è¯„ä¼°æŒ‡æ ‡
        metric_keywords = [
            'accuracy', 'precision', 'recall', 'f1', 'auc', 'roc',
            'mse', 'mae', 'rmse', 'r2', 'loss'
        ]
        for keyword in metric_keywords:
            if keyword in text.lower():
                entities['metrics'].append(keyword)
        
        # åº“å
        library_keywords = [
            'pandas', 'numpy', 'sklearn', 'tensorflow', 'pytorch',
            'matplotlib', 'seaborn', 'plotly', 'xgboost'
        ]
        for keyword in library_keywords:
            if keyword in text.lower():
                entities['libraries'].append(keyword)
        
        # æ–‡ä»¶è·¯å¾„
        file_path_pattern = r'([./][\w/.-]+\.\w+)'
        entities['file_paths'] = re.findall(file_path_pattern, text)
        
        return entities
    
    def _determine_complexity(self, 
                            user_input: str, 
                            context: PromptGenerationContext = None) -> PromptComplexity:
        """ç¡®å®šæç¤ºå¤æ‚åº¦"""
        
        complexity_indicators = {
            PromptComplexity.SIMPLE: [
                r'ç®€å•|åŸºç¡€|basic|simple|å…¥é—¨',
                r'æ€ä¹ˆ|å¦‚ä½•|what|how',
                r'å¼€å§‹|start|begin'
            ],
            PromptComplexity.ADVANCED: [
                r'é«˜çº§|advanced|å¤æ‚|complex',
                r'ä¼˜åŒ–|optimization|æ€§èƒ½|performance',
                r'åˆ†å¸ƒå¼|distributed|å¹¶è¡Œ|parallel',
                r'æ·±åº¦|deep|ç¥ç»ç½‘ç»œ|neural'
            ],
            PromptComplexity.EXPERT: [
                r'ä¸“å®¶|expert|ç ”ç©¶|research',
                r'è®ºæ–‡|paper|ç®—æ³•å®ç°|implementation',
                r'è‡ªå®šä¹‰|custom|åº•å±‚|low-level'
            ]
        }
        
        text_lower = user_input.lower()
        scores = {}
        
        for complexity, patterns in complexity_indicators.items():
            score = sum(len(re.findall(pattern, text_lower)) for pattern in patterns)
            if score > 0:
                scores[complexity] = score
        
        # è€ƒè™‘è¾“å…¥é•¿åº¦
        if len(user_input) > 200:
            scores[PromptComplexity.ADVANCED] = scores.get(PromptComplexity.ADVANCED, 0) + 1
        elif len(user_input) < 50:
            scores[PromptComplexity.SIMPLE] = scores.get(PromptComplexity.SIMPLE, 0) + 1
        
        # è€ƒè™‘ä¸Šä¸‹æ–‡
        if context and context.ml_context:
            if len(context.ml_context.get('existing_models', [])) > 3:
                scores[PromptComplexity.ADVANCED] = scores.get(PromptComplexity.ADVANCED, 0) + 1
        
        if scores:
            return max(scores, key=scores.get)
        else:
            return PromptComplexity.INTERMEDIATE
    
    def _extract_context_keywords(self, 
                                user_input: str, 
                                context: PromptGenerationContext = None) -> Set[str]:
        """æå–ä¸Šä¸‹æ–‡å…³é”®è¯"""
        keywords = set()
        
        # ä»ç”¨æˆ·è¾“å…¥æå–
        words = re.findall(r'\b\w+\b', user_input.lower())
        ml_related_words = {
            'model', 'data', 'train', 'test', 'feature', 'accuracy',
            'prediction', 'classification', 'regression', 'clustering'
        }
        keywords.update(word for word in words if word in ml_related_words)
        
        # ä»ä¸Šä¸‹æ–‡æå–
        if context:
            if context.current_task:
                keywords.add(context.current_task.lower())
            
            keywords.update(context.available_tools)
            
            if context.data_context:
                keywords.update(str(v).lower() for v in context.data_context.values() if isinstance(v, str))
        
        return keywords
    
    async def _select_best_template(self, 
                                  intent: UserIntent, 
                                  context: PromptGenerationContext = None) -> Optional[PromptTemplateData]:
        """é€‰æ‹©æœ€ä½³æ¨¡æ¿"""
        
        if not self.templates:
            return None
        
        # ç­›é€‰åŒ¹é…æ„å›¾çš„æ¨¡æ¿
        matching_templates = [
            template for template in self.templates.values()
            if intent.primary_intent in template.intent_types
        ]
        
        if not matching_templates:
            # å¦‚æœæ²¡æœ‰ç›´æ¥åŒ¹é…çš„ï¼Œå¯»æ‰¾æ¬¡è¦æ„å›¾åŒ¹é…
            for secondary_intent, _ in intent.secondary_intents:
                matching_templates = [
                    template for template in self.templates.values()
                    if secondary_intent in template.intent_types
                ]
                if matching_templates:
                    break
        
        if not matching_templates:
            return None
        
        # æŒ‰å¤šä¸ªå› ç´ è¯„åˆ†
        scored_templates = []
        for template in matching_templates:
            score = 0
            
            # æ„å›¾åŒ¹é…åº¦
            if intent.primary_intent in template.intent_types:
                score += 3
            
            # å¤æ‚åº¦åŒ¹é…åº¦
            if template.complexity_level == intent.complexity_level:
                score += 2
            elif abs(list(PromptComplexity).index(template.complexity_level) - list(PromptComplexity).index(intent.complexity_level)) <= 1:
                score += 1
            
            # ä½¿ç”¨é¢‘ç‡å’Œæ•ˆæœ
            score += template.usage_count * 0.1
            score += template.effectiveness_score * 2
            
            # æ ‡ç­¾åŒ¹é…åº¦
            if template.tags & intent.context_keywords:
                score += len(template.tags & intent.context_keywords)
            
            scored_templates.append((template, score))
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ¨¡æ¿
        scored_templates.sort(key=lambda x: x[1], reverse=True)
        return scored_templates[0][0] if scored_templates else None
    
    async def _generate_prompt_text(self, 
                                  user_input: str,
                                  intent: UserIntent,
                                  template: Optional[PromptTemplateData],
                                  context: PromptGenerationContext = None) -> Tuple[str, str]:
        """ç”Ÿæˆæç¤ºæ–‡æœ¬"""
        
        if template:
            # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ
            prompt_text, reasoning = await self._generate_from_template(
                user_input, intent, template, context
            )
        else:
            # ç›´æ¥åŸºäºæ„å›¾ç”Ÿæˆ
            prompt_text, reasoning = await self._generate_from_intent(
                user_input, intent, context
            )
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context and context.ml_context:
            prompt_text = await self._enhance_with_context(prompt_text, context)
        
        return prompt_text, reasoning
    
    async def _generate_from_template(self, 
                                    user_input: str,
                                    intent: UserIntent,
                                    template: PromptTemplateData,
                                    context: PromptGenerationContext = None) -> Tuple[str, str]:
        """ä»æ¨¡æ¿ç”Ÿæˆæç¤º"""
        
        # æå–æ¨¡æ¿å˜é‡çš„å€¼
        variable_values = await self._extract_template_variables(
            user_input, intent, template, context
        )
        
        # æ›¿æ¢æ¨¡æ¿å˜é‡
        prompt_text = template.template_text
        for var, value in variable_values.items():
            placeholder = "{" + var + "}"
            prompt_text = prompt_text.replace(placeholder, str(value))
        
        # å¦‚æœè¿˜æœ‰æœªæ›¿æ¢çš„å˜é‡ï¼Œç”¨å ä½ç¬¦æ›¿æ¢
        remaining_vars = re.findall(r'\{(\w+)\}', prompt_text)
        for var in remaining_vars:
            placeholder = "{" + var + "}"
            prompt_text = prompt_text.replace(placeholder, f"[è¯·å¡«å†™{var}]")
        
        reasoning = f"ä½¿ç”¨æ¨¡æ¿ '{template.name}' ç”Ÿæˆæç¤ºï¼ŒåŒ¹é…æ„å›¾ {intent.primary_intent.value}"
        
        return prompt_text, reasoning
    
    async def _generate_from_intent(self, 
                                  user_input: str,
                                  intent: UserIntent,
                                  context: PromptGenerationContext = None) -> Tuple[str, str]:
        """åŸºäºæ„å›¾ç›´æ¥ç”Ÿæˆæç¤º"""
        
        intent_templates = {
            IntentType.DATA_EXPLORATION: "è¯·å¸®æˆ‘åˆ†æå’Œæ¢ç´¢æ•°æ®ã€‚æˆ‘æƒ³äº†è§£æ•°æ®çš„åŸºæœ¬ç‰¹å¾ã€è´¨é‡å’Œåˆ†å¸ƒæƒ…å†µã€‚",
            IntentType.MODEL_TRAINING: "è¯·æŒ‡å¯¼æˆ‘è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚åŒ…æ‹¬ç®—æ³•é€‰æ‹©ã€å‚æ•°è®¾ç½®å’Œè®­ç»ƒæµç¨‹ã€‚",
            IntentType.MODEL_EVALUATION: "è¯·å¸®æˆ‘è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚åˆ†æè¯„ä¼°æŒ‡æ ‡ï¼Œè¯Šæ–­é—®é¢˜ï¼Œæå‡ºæ”¹è¿›å»ºè®®ã€‚",
            IntentType.FEATURE_ENGINEERING: "è¯·ååŠ©æˆ‘è¿›è¡Œç‰¹å¾å·¥ç¨‹ã€‚åŒ…æ‹¬ç‰¹å¾é€‰æ‹©ã€å˜æ¢å’Œåˆ›å»ºã€‚",
            IntentType.VISUALIZATION: "è¯·å¸®æˆ‘åˆ›å»ºæ•°æ®å¯è§†åŒ–ã€‚é€‰æ‹©åˆé€‚çš„å›¾è¡¨ç±»å‹å’Œè®¾è®¡æ–¹æ¡ˆã€‚",
            IntentType.DEBUGGING: "è¯·å¸®æˆ‘è§£å†³ä»£ç é—®é¢˜ã€‚åˆ†æé”™è¯¯åŸå› å¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚",
            IntentType.OPTIMIZATION: "è¯·å¸®æˆ‘ä¼˜åŒ–MLç³»ç»Ÿæ€§èƒ½ã€‚æå‡é€Ÿåº¦ã€æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚",
            IntentType.DEPLOYMENT: "è¯·æŒ‡å¯¼æˆ‘éƒ¨ç½²MLæ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒã€‚åŒ…æ‹¬æœåŠ¡åŒ–å’Œç›‘æ§ã€‚",
            IntentType.GENERAL_QUESTION: "è¯·å›ç­”æˆ‘çš„MLç›¸å…³é—®é¢˜ã€‚æä¾›è¯¦ç»†å’Œå®ç”¨çš„å»ºè®®ã€‚"
        }
        
        base_prompt = intent_templates.get(intent.primary_intent, intent_templates[IntentType.GENERAL_QUESTION])
        
        # æ·»åŠ ç”¨æˆ·è¾“å…¥çš„å…·ä½“å†…å®¹
        prompt_text = f"{base_prompt}\n\nç”¨æˆ·éœ€æ±‚ï¼š{user_input}"
        
        # æ ¹æ®æå–çš„å®ä½“æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡
        if intent.extracted_entities:
            context_info = []
            for entity_type, entities in intent.extracted_entities.items():
                if entities:
                    context_info.append(f"{entity_type}: {', '.join(entities)}")
            
            if context_info:
                prompt_text += f"\n\nç›¸å…³ä¿¡æ¯ï¼š\n" + "\n".join(context_info)
        
        reasoning = f"åŸºäºæ„å›¾ {intent.primary_intent.value} ç›´æ¥ç”Ÿæˆæç¤ºï¼Œç½®ä¿¡åº¦ {intent.confidence:.2f}"
        
        return prompt_text, reasoning
    
    async def _extract_template_variables(self, 
                                        user_input: str,
                                        intent: UserIntent,
                                        template: PromptTemplateData,
                                        context: PromptGenerationContext = None) -> Dict[str, str]:
        """æå–æ¨¡æ¿å˜é‡çš„å€¼"""
        
        variable_values = {}
        
        for var in template.variables:
            value = None
            
            # ä»å®ä½“ä¸­æå–
            if var in ['dataset_name', 'dataset']:
                if intent.extracted_entities.get('datasets'):
                    value = intent.extracted_entities['datasets'][0]
                elif context and context.data_context.get('current_dataset'):
                    value = context.data_context['current_dataset']
            
            elif var in ['model_type', 'model_name']:
                if intent.extracted_entities.get('algorithms'):
                    value = intent.extracted_entities['algorithms'][0]
                elif intent.extracted_entities.get('models'):
                    value = intent.extracted_entities['models'][0]
            
            elif var in ['error_message']:
                # å°è¯•ä»ç”¨æˆ·è¾“å…¥ä¸­æå–é”™è¯¯ä¿¡æ¯
                error_pattern = r'error[:\s]*([^.!?\n]+)'
                match = re.search(error_pattern, user_input, re.IGNORECASE)
                if match:
                    value = match.group(1).strip()
            
            # ä»ä¸Šä¸‹æ–‡ä¸­æå–
            if not value and context:
                if var == 'current_task' and context.current_task:
                    value = context.current_task
                elif var in context.data_context:
                    value = str(context.data_context[var])
                elif var in context.model_context:
                    value = str(context.model_context[var])
            
            # ä½¿ç”¨é»˜è®¤å€¼æˆ–å ä½ç¬¦
            if not value:
                value = f"[{var}]"
            
            variable_values[var] = value
        
        return variable_values
    
    async def _enhance_with_context(self, 
                                  prompt_text: str, 
                                  context: PromptGenerationContext) -> str:
        """ä½¿ç”¨ä¸Šä¸‹æ–‡å¢å¼ºæç¤º"""
        
        context_additions = []
        
        # æ·»åŠ ç¯å¢ƒä¿¡æ¯
        if context.ml_context:
            ml_ctx = context.ml_context
            
            if ml_ctx.get('available_datasets'):
                datasets = ml_ctx['available_datasets']
                if len(datasets) > 0:
                    context_additions.append(f"å¯ç”¨æ•°æ®é›†: {len(datasets)} ä¸ª")
            
            if ml_ctx.get('existing_models'):
                models = ml_ctx['existing_models']
                if len(models) > 0:
                    context_additions.append(f"ç°æœ‰æ¨¡å‹: {len(models)} ä¸ª")
            
            if ml_ctx.get('ml_frameworks'):
                frameworks = ml_ctx['ml_frameworks']
                if frameworks:
                    context_additions.append(f"MLæ¡†æ¶: {', '.join(frameworks[:3])}")
        
        # æ·»åŠ å·¥å…·ä¿¡æ¯
        if context.available_tools:
            context_additions.append(f"å¯ç”¨å·¥å…·: {', '.join(context.available_tools[:5])}")
        
        # æ·»åŠ å¯¹è¯å†å²
        if context.conversation_history:
            recent_history = context.conversation_history[-2:]  # æœ€è¿‘2æ¡
            if recent_history:
                context_additions.append(f"æœ€è¿‘å¯¹è¯: {'; '.join(recent_history)}")
        
        if context_additions:
            context_text = "\n\nç¯å¢ƒä¸Šä¸‹æ–‡:\n" + "\n".join(f"- {addition}" for addition in context_additions)
            prompt_text += context_text
        
        return prompt_text
    
    async def _generate_suggestions(self, 
                                  intent: UserIntent, 
                                  context: PromptGenerationContext = None) -> List[str]:
        """ç”Ÿæˆæ™ºèƒ½å»ºè®®"""
        
        suggestions = []
        
        # åŸºäºæ„å›¾çš„å»ºè®®
        intent_suggestions = {
            IntentType.DATA_EXPLORATION: [
                "å»ºè®®å…ˆæŸ¥çœ‹æ•°æ®çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯",
                "æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œç‰¹åˆ«æ˜¯ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼",
                "åˆ›å»ºæ•°æ®å¯è§†åŒ–æ¥ç†è§£åˆ†å¸ƒç‰¹å¾"
            ],
            IntentType.MODEL_TRAINING: [
                "å»ºè®®å…ˆè¿›è¡Œæ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹",
                "ä½¿ç”¨äº¤å‰éªŒè¯æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½",
                "å°è¯•å¤šç§ç®—æ³•å¹¶æ¯”è¾ƒç»“æœ"
            ],
            IntentType.MODEL_EVALUATION: [
                "ä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°æ¨¡å‹",
                "åˆ†ææ··æ·†çŸ©é˜µæ¥ç†è§£åˆ†ç±»é”™è¯¯",
                "æ£€æŸ¥æ¨¡å‹åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šçš„è¡¨ç°"
            ],
            IntentType.FEATURE_ENGINEERING: [
                "åˆ†æç‰¹å¾é‡è¦æ€§æ¥æŒ‡å¯¼ç‰¹å¾é€‰æ‹©",
                "è€ƒè™‘åˆ›å»ºäº¤äº’ç‰¹å¾å’Œå¤šé¡¹å¼ç‰¹å¾",
                "ä½¿ç”¨é¢†åŸŸçŸ¥è¯†æ¥è®¾è®¡æœ‰æ„ä¹‰çš„ç‰¹å¾"
            ],
            IntentType.VISUALIZATION: [
                "é€‰æ‹©é€‚åˆæ•°æ®ç±»å‹å’Œç›®çš„çš„å›¾è¡¨",
                "ç¡®ä¿å¯è§†åŒ–æ¸…æ™°æ˜“æ‡‚",
                "è€ƒè™‘äº¤äº’å¼å¯è§†åŒ–æ¥æ¢ç´¢æ•°æ®"
            ],
            IntentType.DEBUGGING: [
                "ä»”ç»†é˜…è¯»é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª",
                "æ£€æŸ¥æ•°æ®ç±»å‹å’Œå½¢çŠ¶æ˜¯å¦åŒ¹é…",
                "ä½¿ç”¨è°ƒè¯•å·¥å…·é€æ­¥æ£€æŸ¥ä»£ç "
            ]
        }
        
        base_suggestions = intent_suggestions.get(intent.primary_intent, [])
        suggestions.extend(base_suggestions[:3])  # æœ€å¤š3ä¸ªåŸºç¡€å»ºè®®
        
        # åŸºäºä¸Šä¸‹æ–‡çš„å»ºè®®
        if context:
            if context.ml_context:
                ml_ctx = context.ml_context
                
                # æ•°æ®ç›¸å…³å»ºè®®
                if ml_ctx.get('data_volume_gb', 0) > 5:
                    suggestions.append("æ•°æ®é‡è¾ƒå¤§ï¼Œè€ƒè™‘ä½¿ç”¨é‡‡æ ·æˆ–åˆ†å¸ƒå¼å¤„ç†")
                
                # GPUå»ºè®®
                if not ml_ctx.get('gpu_available') and 'tensorflow' in ml_ctx.get('ml_frameworks', []):
                    suggestions.append("è€ƒè™‘ä½¿ç”¨GPUåŠ é€Ÿæ·±åº¦å­¦ä¹ è®­ç»ƒ")
                
                # æ¨¡å‹å»ºè®®
                if len(ml_ctx.get('existing_models', [])) > 0:
                    suggestions.append("å¯ä»¥å‚è€ƒç°æœ‰æ¨¡å‹çš„æ¶æ„å’Œå‚æ•°")
        
        return suggestions[:5]  # æœ€å¤šè¿”å›5ä¸ªå»ºè®®
    
    def _calculate_generation_confidence(self, 
                                       intent: UserIntent,
                                       template: Optional[PromptTemplateData],
                                       context: PromptGenerationContext = None) -> float:
        """è®¡ç®—ç”Ÿæˆç½®ä¿¡åº¦"""
        
        confidence = intent.confidence  # åŸºç¡€ç½®ä¿¡åº¦æ¥è‡ªæ„å›¾åˆ†æ
        
        # æ¨¡æ¿åŒ¹é…åŠ åˆ†
        if template:
            confidence += 0.2
            
            # æ¨¡æ¿æ•ˆæœåŠ åˆ†
            confidence += template.effectiveness_score * 0.1
            
            # å˜é‡å¡«å……å®Œæ•´æ€§
            if template.variables:
                filled_vars = sum(1 for var in template.variables if f"[{var}]" not in template.template_text)
                completeness = filled_vars / len(template.variables)
                confidence += completeness * 0.1
        
        # ä¸Šä¸‹æ–‡ä¸°å¯Œåº¦åŠ åˆ†
        if context:
            if context.ml_context:
                confidence += 0.1
            if context.conversation_history:
                confidence += 0.05
            if context.available_tools:
                confidence += 0.05
        
        # å®ä½“æå–è´¨é‡åŠ åˆ†
        if intent.extracted_entities:
            entity_count = sum(len(entities) for entities in intent.extracted_entities.values())
            confidence += min(0.1, entity_count * 0.02)
        
        return min(1.0, confidence)
    
    async def _save_template(self, template: PromptTemplateData):
        """ä¿å­˜æ¨¡æ¿åˆ°æ•°æ®åº“"""
        cursor = self.conn.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO prompt_templates 
            (id, name, description, template_text, intent_types, complexity_level, 
             variables, tags, usage_count, effectiveness_score, created_at, last_used)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            template.id,
            template.name,
            template.description,
            template.template_text,
            json.dumps([intent.value for intent in template.intent_types]),
            template.complexity_level.value,
            json.dumps(template.variables),
            json.dumps(list(template.tags)),
            template.usage_count,
            template.effectiveness_score,
            template.created_at.isoformat(),
            template.last_used.isoformat() if template.last_used else None
        ))
        
        self.conn.commit()
    
    async def _save_generated_prompt(self, prompt: GeneratedPrompt):
        """ä¿å­˜ç”Ÿæˆçš„æç¤ºåˆ°æ•°æ®åº“"""
        cursor = self.conn.cursor()
        
        intent_data = {
            'primary_intent': prompt.intent.primary_intent.value,
            'confidence': prompt.intent.confidence,
            'secondary_intents': [(intent.value, score) for intent, score in prompt.intent.secondary_intents],
            'extracted_entities': prompt.intent.extracted_entities,
            'complexity_level': prompt.intent.complexity_level.value,
            'context_keywords': list(prompt.intent.context_keywords)
        }
        
        cursor.execute("""
            INSERT OR REPLACE INTO generated_prompts 
            (id, original_input, generated_text, intent_data, template_used, 
             confidence, reasoning, suggestions, context_used, generated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            prompt.id,
            prompt.original_input,
            prompt.generated_text,
            json.dumps(intent_data),
            prompt.template_used,
            prompt.confidence,
            prompt.reasoning,
            json.dumps(prompt.suggestions),
            json.dumps(prompt.context_used),
            prompt.generated_at.isoformat()
        ))
        
        self.conn.commit()
    
    async def _update_template_usage(self, template_id: str):
        """æ›´æ–°æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡"""
        if template_id in self.templates:
            template = self.templates[template_id]
            template.usage_count += 1
            template.last_used = datetime.now()
            await self._save_template(template)
    
    async def provide_feedback(self, prompt_id: str, rating: int, feedback_text: str = None):
        """æä¾›ç”¨æˆ·åé¦ˆ"""
        cursor = self.conn.cursor()
        
        cursor.execute("""
            INSERT INTO user_feedback (prompt_id, rating, feedback_text, timestamp)
            VALUES (?, ?, ?, ?)
        """, (prompt_id, rating, feedback_text, datetime.now().isoformat()))
        
        self.conn.commit()
        
        # æ›´æ–°æ¨¡æ¿æ•ˆæœè¯„åˆ†
        if prompt_id in self.generated_prompts:
            generated_prompt = self.generated_prompts[prompt_id]
            if generated_prompt.template_used:
                await self._update_template_effectiveness(generated_prompt.template_used, rating)
        
        print(f"âœ… åé¦ˆå·²è®°å½•: è¯„åˆ† {rating}/5")
    
    async def _update_template_effectiveness(self, template_id: str, rating: int):
        """æ›´æ–°æ¨¡æ¿æ•ˆæœè¯„åˆ†"""
        if template_id in self.templates:
            template = self.templates[template_id]
            
            # ç®€å•çš„ç§»åŠ¨å¹³å‡
            normalized_rating = rating / 5.0  # è½¬æ¢ä¸º0-1èŒƒå›´
            template.effectiveness_score = (template.effectiveness_score * 0.8 + normalized_rating * 0.2)
            
            await self._save_template(template)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æç¤ºç”Ÿæˆç»Ÿè®¡"""
        if not self.generated_prompts:
            return {'total_prompts': 0}
        
        # æ„å›¾åˆ†å¸ƒ
        intent_counts = {}
        for prompt in self.generated_prompts.values():
            intent = prompt.intent.primary_intent.value
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
        
        # å¤æ‚åº¦åˆ†å¸ƒ
        complexity_counts = {}
        for prompt in self.generated_prompts.values():
            complexity = prompt.intent.complexity_level.value
            complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
        
        # å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(p.confidence for p in self.generated_prompts.values()) / len(self.generated_prompts)
        
        # æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
        template_usage = {}
        for prompt in self.generated_prompts.values():
            if prompt.template_used:
                template_name = self.templates[prompt.template_used].name
                template_usage[template_name] = template_usage.get(template_name, 0) + 1
        
        return {
            'total_prompts': len(self.generated_prompts),
            'total_templates': len(self.templates),
            'intent_distribution': intent_counts,
            'complexity_distribution': complexity_counts,
            'average_confidence': avg_confidence,
            'template_usage': template_usage,
            'cache_hit_rate': len(self.generation_cache) / max(1, len(self.generated_prompts))
        }
    
    def export_data(self, output_path: str = None) -> str:
        """å¯¼å‡ºæç¤ºæ•°æ®"""
        if output_path is None:
            output_path = f"ml_prompts_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        export_data = {
            'templates': {
                tid: asdict(template) for tid, template in self.templates.items()
            },
            'generated_prompts': {
                pid: asdict(prompt) for pid, prompt in self.generated_prompts.items()
            },
            'statistics': self.get_statistics(),
            'export_timestamp': datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“„ æç¤ºæ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")
        return output_path

# æµ‹è¯•å‡½æ•°
async def test_intelligent_prompt_generator():
    """æµ‹è¯•æ™ºèƒ½æç¤ºç”Ÿæˆå™¨"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½æç¤ºç”Ÿæˆå™¨æµ‹è¯•...")
    
    # åˆ›å»ºæç¤ºç”Ÿæˆå™¨
    generator = IntelligentMLPromptGenerator(".")
    
    # åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
    context = PromptGenerationContext(
        user_input="",
        ml_context={
            'available_datasets': [{'name': 'sales_data.csv', 'type': 'structured_text'}],
            'existing_models': [{'name': 'model.pkl', 'framework': 'scikit-learn'}],
            'ml_frameworks': ['scikit-learn', 'pandas', 'matplotlib']
        },
        current_task="æ•°æ®åˆ†æ",
        available_tools=['preprocess_data', 'train_model', 'visualize_data'],
        data_context={'current_dataset': 'sales_data.csv'}
    )
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æç¤ºç”Ÿæˆ
    test_inputs = [
        "æˆ‘æƒ³åˆ†æsales_data.csvæ•°æ®é›†ï¼Œäº†è§£é”€å”®è¶‹åŠ¿",
        "å¸®æˆ‘è®­ç»ƒä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹æ¥é¢„æµ‹é”€å”®é¢",
        "æˆ‘çš„æ¨¡å‹å‡†ç¡®ç‡åªæœ‰60%ï¼Œæ€ä¹ˆæå‡ï¼Ÿ",
        "åˆ›å»ºä¸€ä¸ªé”€å”®æ•°æ®çš„å¯è§†åŒ–å›¾è¡¨",
        "ä»£ç æŠ¥é”™ï¼šValueError: X has 10 features but model expects 8"
    ]
    
    for i, user_input in enumerate(test_inputs, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {user_input}")
        
        context.user_input = user_input
        generated_prompt = await generator.generate_prompt(user_input, context)
        
        print(f"ğŸ¯ æ„å›¾: {generated_prompt.intent.primary_intent.value}")
        print(f"ğŸ² ç½®ä¿¡åº¦: {generated_prompt.confidence:.2f}")
        print(f"ğŸ“‹ ç”Ÿæˆçš„æç¤º:")
        print(generated_prompt.generated_text)
        print(f"ğŸ’¡ å»ºè®®: {'; '.join(generated_prompt.suggestions[:2])}")
        print("-" * 50)
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = generator.get_statistics()
    print(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡: {stats}")
    
    # å¯¼å‡ºæ•°æ®
    export_path = generator.export_data()
    print(f"ğŸ“‹ æ•°æ®å¯¼å‡º: {export_path}")

# å…¼å®¹åŸæœ‰æ¥å£ï¼Œä¿æŒå‘åå…¼å®¹

class PromptTemplate(Enum):
    """æç¤ºæ¨¡æ¿ç±»å‹"""
    DATA_ANALYSIS = "data_analysis"
    MODEL_TRAINING = "model_training"
    VISUALIZATION = "visualization"
    DEBUGGING = "debugging"
    OPTIMIZATION = "optimization"
    EXPLANATION = "explanation"

class ContextType(Enum):
    """ä¸Šä¸‹æ–‡ç±»å‹"""
    TECHNICAL = "technical"
    BUSINESS = "business"
    EDUCATIONAL = "educational"
    RESEARCH = "research"

@dataclass
class IntelligentPrompt:
    """æ™ºèƒ½æç¤ºæ•°æ®ç»“æ„"""
    template_type: PromptTemplate
    context_type: ContextType
    base_prompt: str
    dynamic_sections: Dict[str, str]
    examples: List[str]
    best_practices: List[str]
    common_pitfalls: List[str]
    
    def generate(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆå®Œæ•´çš„æ™ºèƒ½æç¤º"""
        sections = [
            self._generate_system_identity(),
            self._generate_context_section(context),
            self._generate_capability_section(),
            self._generate_memory_section(context),
            self._generate_task_analysis_section(user_input, context),
            self._generate_best_practices_section(),
            self._generate_response_guidelines(),
            self._generate_user_input_section(user_input)
        ]
        
        return "\n\n".join(filter(None, sections))
    
    def _generate_system_identity(self) -> str:
        """ç”Ÿæˆç³»ç»Ÿèº«ä»½éƒ¨åˆ†"""
        return """# AutoML Workflow Agent - å¢å¼ºå‹AIåŠ©æ‰‹

ä½ æ˜¯ä¸€ä¸ªå…·å¤‡é«˜çº§æœºå™¨å­¦ä¹ çŸ¥è¯†å’Œå®è·µç»éªŒçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºï¼š
- ğŸ§  æ™ºèƒ½åˆ†æç”¨æˆ·éœ€æ±‚å¹¶åˆ¶å®šæœ€ä¼˜è§£å†³æ–¹æ¡ˆ
- ğŸ”§ åœ¨å®‰å…¨çš„Dockerç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚çš„MLå·¥ä½œæµ
- ğŸ“Š æä¾›æ•°æ®é©±åŠ¨çš„æ´å¯Ÿå’Œå»ºè®®
- ğŸš€ ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡
- ğŸ“š ä¼ æˆæœ€ä½³å®è·µå’Œè¡Œä¸šæ ‡å‡†

## æ ¸å¿ƒèƒ½åŠ›
- **æ™ºèƒ½æ¨ç†**: åŸºäºä¸Šä¸‹æ–‡å’Œå†å²ç»éªŒè¿›è¡Œæ·±åº¦åˆ†æ
- **è®°å¿†ç³»ç»Ÿ**: å­¦ä¹ å’Œè®°ä½ç”¨æˆ·åå¥½ä¸é¡¹ç›®ç‰¹ç‚¹
- **ä»»åŠ¡è§„åˆ’**: è‡ªåŠ¨åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºå¯æ‰§è¡Œæ­¥éª¤
- **é”™è¯¯é¢„é˜²**: ä¸»åŠ¨è¯†åˆ«æ½œåœ¨é—®é¢˜å¹¶æä¾›è§£å†³æ–¹æ¡ˆ"""
    
    def _generate_context_section(self, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡éƒ¨åˆ†"""
        if not context:
            return ""
        
        sections = ["## å½“å‰ä¸Šä¸‹æ–‡"]
        
        if context.get("session_info"):
            sections.append(f"**ä¼šè¯ä¿¡æ¯**: {context['session_info']}")
        
        if context.get("data_info"):
            sections.append(f"**æ•°æ®ä¿¡æ¯**: {context['data_info']}")
        
        if context.get("previous_tasks"):
            sections.append("**å†å²ä»»åŠ¡**:")
            for task in context["previous_tasks"]:
                sections.append(f"- {task}")
        
        if context.get("user_preferences"):
            sections.append("**ç”¨æˆ·åå¥½**:")
            for pref, value in context["user_preferences"].items():
                sections.append(f"- {pref}: {value}")
        
        return "\n".join(sections)
    
    def _generate_capability_section(self) -> str:
        """ç”Ÿæˆèƒ½åŠ›è¯´æ˜éƒ¨åˆ†"""
        return f"""## å¯ç”¨å·¥å…·å’Œèƒ½åŠ›

### ğŸ”§ ä»£ç æ‰§è¡Œç¯å¢ƒ
- **å®‰å…¨æ²™ç®±**: Dockeréš”ç¦»ç¯å¢ƒï¼Œæ”¯æŒGPUåŠ é€Ÿ
- **MLåº“æ”¯æŒ**: scikit-learn, pandas, numpy, matplotlib, seaborn
- **æ·±åº¦å­¦ä¹ **: PyTorch, TensorFlow (CPU/GPU)
- **æ•°æ®å¤„ç†**: å¤§è§„æ¨¡æ•°æ®é›†å¤„ç†å’Œåˆ†æ

### ğŸ“Š æ•°æ®åˆ†æ
- **æ¢ç´¢æ€§åˆ†æ**: è‡ªåŠ¨ç”Ÿæˆæ•°æ®åˆ†å¸ƒã€ç›¸å…³æ€§åˆ†æ
- **è´¨é‡è¯„ä¼°**: ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€æ•°æ®åæ–œæ£€æµ‹
- **å¯è§†åŒ–**: äº¤äº’å¼å›¾è¡¨å’Œç»Ÿè®¡å›¾å½¢

### ğŸ¤– æœºå™¨å­¦ä¹ 
- **è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹**: ç‰¹å¾é€‰æ‹©ã€åˆ›å»ºã€è½¬æ¢
- **æ¨¡å‹é€‰æ‹©**: åŸºäºæ•°æ®ç‰¹ç‚¹æ¨èæœ€é€‚åˆçš„ç®—æ³•
- **è¶…å‚æ•°ä¼˜åŒ–**: è‡ªåŠ¨è°ƒå‚å’Œäº¤å‰éªŒè¯
- **æ¨¡å‹è¯„ä¼°**: å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡å’Œè§£é‡Šæ€§åˆ†æ

### ğŸ’¾ æ•°æ®ç®¡ç†
- **å†å²è®°å½•**: å®Œæ•´çš„å®éªŒè¿½è¸ªå’Œç‰ˆæœ¬æ§åˆ¶
- **ç»“æœä¿å­˜**: ç»“æ„åŒ–å­˜å‚¨MLæˆæœå’Œæ´å¯Ÿ
- **æ•°æ®è¡€ç¼˜**: è·Ÿè¸ªæ•°æ®å¤„ç†å’Œè½¬æ¢è¿‡ç¨‹"""
    
    def _generate_memory_section(self, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆè®°å¿†éƒ¨åˆ†"""
        if not context or not context.get("relevant_memories"):
            return ""
        
        sections = ["## ğŸ“š ç›¸å…³è®°å¿†å’Œç»éªŒ"]
        
        for memory in context["relevant_memories"]:
            importance_stars = "â­" * min(5, int(memory.get("importance", 0) * 5))
            sections.append(f"{importance_stars} {memory.get('content', '')}")
        
        return "\n".join(sections)
    
    def _generate_task_analysis_section(self, user_input: str, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆä»»åŠ¡åˆ†æéƒ¨åˆ†"""
        sections = ["## ğŸ¯ ä»»åŠ¡åˆ†ææ¡†æ¶"]
        
        # æ·»åŠ åŸºäºæ¨¡æ¿ç±»å‹çš„åˆ†ææ¡†æ¶
        if self.template_type == PromptTemplate.DATA_ANALYSIS:
            sections.extend([
                "### æ•°æ®åˆ†ææ£€æŸ¥æ¸…å•",
                "1. **æ•°æ®ç†è§£**: å½¢çŠ¶ã€ç±»å‹ã€åˆ†å¸ƒã€è´¨é‡",
                "2. **ä¸šåŠ¡ç†è§£**: ç›®æ ‡ã€çº¦æŸã€æˆåŠŸæŒ‡æ ‡",
                "3. **æ¢ç´¢ç­–ç•¥**: å•å˜é‡ã€åŒå˜é‡ã€å¤šå˜é‡åˆ†æ",
                "4. **æ´å¯Ÿæå–**: æ¨¡å¼ã€å¼‚å¸¸ã€å…³ç³»ã€è¶‹åŠ¿"
            ])
        elif self.template_type == PromptTemplate.MODEL_TRAINING:
            sections.extend([
                "### å»ºæ¨¡æµç¨‹æ¡†æ¶",
                "1. **é—®é¢˜å®šä¹‰**: ç›‘ç£/æ— ç›‘ç£ã€åˆ†ç±»/å›å½’/èšç±»",
                "2. **æ•°æ®å‡†å¤‡**: æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€åˆ’åˆ†",
                "3. **æ¨¡å‹é€‰æ‹©**: ç®—æ³•æ¯”è¾ƒã€å¤æ‚åº¦æƒè¡¡",
                "4. **è¯„ä¼°ä¼˜åŒ–**: æŒ‡æ ‡é€‰æ‹©ã€è°ƒå‚ã€éªŒè¯"
            ])
        elif self.template_type == PromptTemplate.VISUALIZATION:
            sections.extend([
                "### å¯è§†åŒ–è®¾è®¡åŸåˆ™",
                "1. **ç›®æ ‡æ˜ç¡®**: æ¢ç´¢æ€§ vs è§£é‡Šæ€§å¯è§†åŒ–",
                "2. **å›¾è¡¨é€‰æ‹©**: åŸºäºæ•°æ®ç±»å‹å’Œå…³ç³»",
                "3. **è®¾è®¡ç¾å­¦**: è‰²å½©ã€å¸ƒå±€ã€æ ‡æ³¨",
                "4. **äº¤äº’æ€§**: åŠ¨æ€ç­›é€‰ã€ç¼©æ”¾ã€è¯¦æƒ…"
            ])
        
        return "\n".join(sections)
    
    def _generate_best_practices_section(self) -> str:
        """ç”Ÿæˆæœ€ä½³å®è·µéƒ¨åˆ†"""
        general_practices = [
            "ğŸ” **æ•°æ®ä¼˜å…ˆ**: å§‹ç»ˆä»ç†è§£æ•°æ®å¼€å§‹",
            "ğŸ“ **æ¸è¿›å¼å¼€å‘**: ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–",
            "ğŸ”’ **å®‰å…¨ç¬¬ä¸€**: æ‰€æœ‰æ“ä½œåœ¨æ²™ç®±ç¯å¢ƒä¸­è¿›è¡Œ",
            "ğŸ“Š **å¯è§£é‡Šæ€§**: ç¡®ä¿ç»“æœå¯ä»¥å‘ä¸šåŠ¡æ–¹è§£é‡Š",
            "ğŸ§ª **å®éªŒè¿½è¸ª**: è®°å½•æ‰€æœ‰å°è¯•å’Œç»“æœ",
            "â™»ï¸ **ä»£ç å¤ç”¨**: å°†æœ‰æ•ˆæ–¹æ³•ä¿å­˜ä¸ºæ¨¡æ¿"
        ]
        
        template_specific = []
        if self.template_type == PromptTemplate.DATA_ANALYSIS:
            template_specific = [
                "ğŸ“‹ **å®Œæ•´æ€§æ£€æŸ¥**: éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§",
                "ğŸ¯ **ç›®æ ‡å¯¼å‘**: åˆ†æè¦ä¸ä¸šåŠ¡ç›®æ ‡å¯¹é½",
                "ğŸ“ˆ **ç»Ÿè®¡æ˜¾è‘—æ€§**: é¿å…å¶ç„¶æ¨¡å¼çš„è¿‡åº¦è§£è¯»"
            ]
        elif self.template_type == PromptTemplate.MODEL_TRAINING:
            template_specific = [
                "âš–ï¸ **åŸºçº¿å¯¹æ¯”**: æ€»æ˜¯å»ºç«‹ç®€å•åŸºçº¿æ¨¡å‹",
                "ğŸ”„ **äº¤å‰éªŒè¯**: ä½¿ç”¨åˆé€‚çš„éªŒè¯ç­–ç•¥",
                "ğŸš« **é¿å…è¿‡æ‹Ÿåˆ**: ç›‘æ§è®­ç»ƒå’ŒéªŒè¯æ€§èƒ½å·®å¼‚"
            ]
        
        all_practices = general_practices + template_specific
        return "## ğŸ’¡ æœ€ä½³å®è·µæŒ‡å—\n\n" + "\n".join(all_practices)
    
    def _generate_response_guidelines(self) -> str:
        """ç”Ÿæˆå“åº”æŒ‡å¯¼åŸåˆ™"""
        return """## ğŸ“ å“åº”æŒ‡å¯¼åŸåˆ™

### ğŸ¨ å“åº”ç»“æ„
1. **ç®€æ˜æ‘˜è¦**: é¦–å…ˆæä¾›1-2å¥è¯çš„æ ¸å¿ƒå›ç­”
2. **è¯¦ç»†åˆ†æ**: æ·±å…¥è§£é‡Šæ–¹æ³•ã€åŸç†å’Œè€ƒè™‘å› ç´ 
3. **å…·ä½“æ­¥éª¤**: æä¾›å¯æ‰§è¡Œçš„æ“ä½œæ­¥éª¤
4. **ä»£ç ç¤ºä¾‹**: ç»™å‡ºå®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç 
5. **æœŸæœ›ç»“æœ**: è¯´æ˜é¢„æœŸè¾“å‡ºå’Œå¦‚ä½•è§£é‡Š

### ğŸ¯ è´¨é‡æ ‡å‡†
- **å‡†ç¡®æ€§**: ç¡®ä¿æŠ€æœ¯å†…å®¹æ­£ç¡®æ— è¯¯
- **å®Œæ•´æ€§**: è¦†ç›–é—®é¢˜çš„æ‰€æœ‰é‡è¦æ–¹é¢
- **å®ç”¨æ€§**: æä¾›ç«‹å³å¯ç”¨çš„è§£å†³æ–¹æ¡ˆ
- **æ•™è‚²æ€§**: è§£é‡ŠèƒŒåçš„åŸç†å’Œæœ€ä½³å®è·µ
- **å®‰å…¨æ€§**: æ‰€æœ‰æ“ä½œç¬¦åˆå®‰å…¨è§„èŒƒ

### ğŸš€ å¢å€¼æœåŠ¡
- **ä¸»åŠ¨å»ºè®®**: æä¾›ç›¸å…³çš„æ”¹è¿›å»ºè®®
- **é£é™©æé†’**: æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- **èµ„æºæ¨è**: æ¨èç›¸å…³å·¥å…·ã€åº“æˆ–å­¦ä¹ èµ„æº
- **åç»­è§„åˆ’**: å»ºè®®ä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨æ–¹å‘"""
    
    def _generate_user_input_section(self, user_input: str) -> str:
        """ç”Ÿæˆç”¨æˆ·è¾“å…¥éƒ¨åˆ†"""
        return f"""## ğŸ’¬ ç”¨æˆ·è¯·æ±‚

{user_input}

---

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šä¸”å®ç”¨çš„å“åº”ã€‚è®°ä½è¦ï¼š
1. å……åˆ†åˆ©ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œç»éªŒ
2. è€ƒè™‘ä¸Šä¸‹æ–‡å’Œå†å²ä¿¡æ¯
3. æä¾›å…·ä½“å¯æ‰§è¡Œçš„è§£å†³æ–¹æ¡ˆ
4. ä¸»åŠ¨é¢„é˜²å¯èƒ½çš„é—®é¢˜
5. ç¡®ä¿å“åº”çš„æ•™è‚²ä»·å€¼å’Œå®ç”¨æ€§"""

class IntelligentPromptGenerator:
    """æ™ºèƒ½æç¤ºç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.templates = self._load_templates()
        self.context_enhancers = self._load_context_enhancers()
        
    def _load_templates(self) -> Dict[PromptTemplate, IntelligentPrompt]:
        """åŠ è½½æç¤ºæ¨¡æ¿"""
        templates = {}
        
        # æ•°æ®åˆ†ææ¨¡æ¿
        templates[PromptTemplate.DATA_ANALYSIS] = IntelligentPrompt(
            template_type=PromptTemplate.DATA_ANALYSIS,
            context_type=ContextType.TECHNICAL,
            base_prompt="æ•°æ®åˆ†æä¸“å®¶æç¤º",
            dynamic_sections={},
            examples=[
                "df.describe() è·å–æè¿°æ€§ç»Ÿè®¡",
                "df.info() æŸ¥çœ‹æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼",
                "sns.pairplot(df) åˆ›å»ºé…å¯¹å›¾"
            ],
            best_practices=[
                "å§‹ç»ˆä»æ•°æ®è´¨é‡è¯„ä¼°å¼€å§‹",
                "ä½¿ç”¨å¤šç§å¯è§†åŒ–æ–¹æ³•æ¢ç´¢æ•°æ®",
                "éªŒè¯å‡è®¾å’Œå‘ç°"
            ],
            common_pitfalls=[
                "å¿½ç•¥ç¼ºå¤±å€¼çš„å¤„ç†",
                "è¿‡åº¦è§£è¯»ç›¸å…³æ€§",
                "å¿½ç•¥æ•°æ®åˆ†å¸ƒçš„åæ–œ"
            ]
        )
        
        # æ¨¡å‹è®­ç»ƒæ¨¡æ¿
        templates[PromptTemplate.MODEL_TRAINING] = IntelligentPrompt(
            template_type=PromptTemplate.MODEL_TRAINING,
            context_type=ContextType.TECHNICAL,
            base_prompt="æœºå™¨å­¦ä¹ å»ºæ¨¡ä¸“å®¶æç¤º",
            dynamic_sections={},
            examples=[
                "from sklearn.model_selection import train_test_split",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.metrics import classification_report"
            ],
            best_practices=[
                "å»ºç«‹åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒ",
                "ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹",
                "ç›‘æ§è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ"
            ],
            common_pitfalls=[
                "æ•°æ®æ³„éœ²é—®é¢˜",
                "ä¸å¹³è¡¡æ•°æ®é›†å¤„ç†ä¸å½“",
                "è¶…å‚æ•°è°ƒä¼˜è¿‡åº¦"
            ]
        )
        
        # å¯è§†åŒ–æ¨¡æ¿
        templates[PromptTemplate.VISUALIZATION] = IntelligentPrompt(
            template_type=PromptTemplate.VISUALIZATION,
            context_type=ContextType.TECHNICAL,
            base_prompt="æ•°æ®å¯è§†åŒ–ä¸“å®¶æç¤º",
            dynamic_sections={},
            examples=[
                "import matplotlib.pyplot as plt",
                "import seaborn as sns",
                "plt.figure(figsize=(12, 8))"
            ],
            best_practices=[
                "é€‰æ‹©åˆé€‚çš„å›¾è¡¨ç±»å‹",
                "æ³¨æ„é¢œè‰²å’Œæ ‡æ³¨",
                "ç¡®ä¿å¯è¯»æ€§å’Œç¾è§‚æ€§"
            ],
            common_pitfalls=[
                "å›¾è¡¨è¿‡äºå¤æ‚",
                "é¢œè‰²é€‰æ‹©ä¸å½“",
                "ç¼ºå°‘å¿…è¦çš„æ ‡æ³¨"
            ]
        )
        
        return templates
    
    def _load_context_enhancers(self) -> Dict[str, Any]:
        """åŠ è½½ä¸Šä¸‹æ–‡å¢å¼ºå™¨"""
        return {
            "data_quality_checklist": [
                "æ£€æŸ¥æ•°æ®å½¢çŠ¶å’ŒåŸºæœ¬ä¿¡æ¯",
                "è¯†åˆ«ç¼ºå¤±å€¼æ¨¡å¼",
                "æ£€æµ‹å¼‚å¸¸å€¼å’Œç¦»ç¾¤ç‚¹",
                "éªŒè¯æ•°æ®ç±»å‹ä¸€è‡´æ€§",
                "è¯„ä¼°æ•°æ®åˆ†å¸ƒç‰¹å¾"
            ],
            "ml_workflow_steps": [
                "é—®é¢˜å®šä¹‰å’Œç›®æ ‡è®¾å®š",
                "æ•°æ®æ”¶é›†å’Œç†è§£",
                "æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹",
                "æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒ",
                "æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–",
                "æ¨¡å‹éƒ¨ç½²å’Œç›‘æ§"
            ],
            "common_algorithms": {
                "åˆ†ç±»": ["RandomForest", "XGBoost", "SVM", "LogisticRegression"],
                "å›å½’": ["LinearRegression", "RandomForestRegressor", "XGBoostRegressor"],
                "èšç±»": ["KMeans", "DBSCAN", "HierarchicalClustering"],
                "é™ç»´": ["PCA", "t-SNE", "UMAP"]
            }
        }
    
    def infer_template_type(self, user_input: str, context: Dict[str, Any] = None) -> PromptTemplate:
        """æ¨æ–­æç¤ºæ¨¡æ¿ç±»å‹"""
        user_input_lower = user_input.lower()
        
        # æ•°æ®åˆ†æå…³é”®è¯
        analysis_keywords = ["åˆ†æ", "æ¢ç´¢", "ç»Ÿè®¡", "åˆ†å¸ƒ", "ç›¸å…³æ€§", "describe", "info", "explore"]
        if any(keyword in user_input_lower for keyword in analysis_keywords):
            return PromptTemplate.DATA_ANALYSIS
        
        # æ¨¡å‹è®­ç»ƒå…³é”®è¯
        training_keywords = ["è®­ç»ƒ", "æ¨¡å‹", "ç®—æ³•", "é¢„æµ‹", "åˆ†ç±»", "å›å½’", "train", "model", "predict"]
        if any(keyword in user_input_lower for keyword in training_keywords):
            return PromptTemplate.MODEL_TRAINING
        
        # å¯è§†åŒ–å…³é”®è¯
        viz_keywords = ["å›¾", "å¯è§†åŒ–", "ç”»", "chart", "plot", "visualization", "graph"]
        if any(keyword in user_input_lower for keyword in viz_keywords):
            return PromptTemplate.VISUALIZATION
        
        # è°ƒè¯•å…³é”®è¯
        debug_keywords = ["é”™è¯¯", "é—®é¢˜", "è°ƒè¯•", "bug", "error", "debug", "fix"]
        if any(keyword in user_input_lower for keyword in debug_keywords):
            return PromptTemplate.DEBUGGING
        
        # ä¼˜åŒ–å…³é”®è¯
        opt_keywords = ["ä¼˜åŒ–", "æ”¹è¿›", "æå‡", "optimize", "improve", "enhance"]
        if any(keyword in user_input_lower for keyword in opt_keywords):
            return PromptTemplate.OPTIMIZATION
        
        # è§£é‡Šå…³é”®è¯
        explain_keywords = ["è§£é‡Š", "è¯´æ˜", "åŸç†", "explain", "how", "why", "what"]
        if any(keyword in user_input_lower for keyword in explain_keywords):
            return PromptTemplate.EXPLANATION
        
        # é»˜è®¤è¿”å›æ•°æ®åˆ†æ
        return PromptTemplate.DATA_ANALYSIS
    
    def generate_intelligent_prompt(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆæ™ºèƒ½æç¤º"""
        try:
            # æ¨æ–­æ¨¡æ¿ç±»å‹
            template_type = self.infer_template_type(user_input, context)
            
            # è·å–å¯¹åº”æ¨¡æ¿
            template = self.templates.get(template_type)
            if not template:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šæ¨¡æ¿ï¼Œä½¿ç”¨æ•°æ®åˆ†ææ¨¡æ¿ä½œä¸ºé»˜è®¤
                template = self.templates[PromptTemplate.DATA_ANALYSIS]
            
            # å¢å¼ºä¸Šä¸‹æ–‡
            enhanced_context = self._enhance_context(context, template_type, user_input)
            
            # ç”Ÿæˆæ™ºèƒ½æç¤º
            intelligent_prompt = template.generate(user_input, enhanced_context)
            
            logger.info(f"Generated intelligent prompt using template: {template_type.value}")
            return intelligent_prompt
            
        except Exception as e:
            logger.error(f"Error generating intelligent prompt: {str(e)}", exc_info=True)
            # è¿”å›åŸºç¡€æç¤ºä½œä¸ºå¤‡ä»½
            return self._generate_fallback_prompt(user_input, context)
    
    def _enhance_context(self, context: Dict[str, Any], template_type: PromptTemplate, user_input: str) -> Dict[str, Any]:
        """å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        enhanced = context.copy() if context else {}
        
        # æ·»åŠ æ¨¡æ¿ç‰¹å®šçš„ä¸Šä¸‹æ–‡å¢å¼º
        if template_type == PromptTemplate.DATA_ANALYSIS:
            enhanced["data_quality_checklist"] = self.context_enhancers["data_quality_checklist"]
        elif template_type == PromptTemplate.MODEL_TRAINING:
            enhanced["ml_workflow"] = self.context_enhancers["ml_workflow_steps"]
            enhanced["algorithm_suggestions"] = self.context_enhancers["common_algorithms"]
        
        # æ·»åŠ æ—¶é—´ä¸Šä¸‹æ–‡
        enhanced["current_time"] = datetime.datetime.now().isoformat()
        
        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ†æ
        enhanced["input_analysis"] = {
            "length": len(user_input),
            "complexity": "high" if len(user_input) > 100 else "medium" if len(user_input) > 50 else "low",
            "contains_code": "```" in user_input or "def " in user_input or "import " in user_input
        }
        
        return enhanced
    
    def _generate_fallback_prompt(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆå¤‡ç”¨æç¤º"""
        return f"""# AutoML Workflow Agent

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨å­¦ä¹ åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ç”¨æˆ·è¯·æ±‚æä¾›ä¸“ä¸šã€è¯¦ç»†çš„å¸®åŠ©ï¼š

## ç”¨æˆ·è¯·æ±‚
{user_input}

## å“åº”è¦æ±‚
1. æä¾›å‡†ç¡®ã€å®ç”¨çš„è§£å†³æ–¹æ¡ˆ
2. åŒ…å«å…·ä½“çš„ä»£ç ç¤ºä¾‹
3. è§£é‡Šå…³é”®æ¦‚å¿µå’Œæœ€ä½³å®è·µ
4. è€ƒè™‘å®‰å…¨æ€§å’Œæ•ˆç‡
5. ä¸»åŠ¨æä¾›ç›¸å…³å»ºè®®

è¯·ç°åœ¨å¼€å§‹å“åº”ç”¨æˆ·çš„è¯·æ±‚ã€‚"""

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    generator = IntelligentPromptGenerator()
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„ç”¨æˆ·è¾“å…¥
    test_inputs = [
        "å¸®æˆ‘åˆ†æè¿™ä¸ªæ•°æ®é›†çš„è´¨é‡",
        "è®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹æ¥é¢„æµ‹å®¢æˆ·æµå¤±",
        "åˆ›å»ºä¸€ä¸ªå¯è§†åŒ–å±•ç¤ºé”€å”®è¶‹åŠ¿",
        "æˆ‘çš„æ¨¡å‹å‡ºç°äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³ï¼Ÿ"
    ]
    
    for user_input in test_inputs:
        print(f"\n{'='*50}")
        print(f"ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"{'='*50}")
        
        prompt = generator.generate_intelligent_prompt(
            user_input=user_input,
            context={
                "session_info": "æµ‹è¯•ä¼šè¯",
                "relevant_memories": [
                    {"content": "ç”¨æˆ·åå¥½ä½¿ç”¨scikit-learn", "importance": 0.8},
                    {"content": "ä¹‹å‰æˆåŠŸå¤„ç†è¿‡å®¢æˆ·æ•°æ®", "importance": 0.6}
                ]
            }
        )
        
        print(prompt[:500] + "...")  # æ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦